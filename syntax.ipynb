{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            text_id                                          full_text  \\\n",
      "0     0016926B079C  I think that students would benefit from learn...   \n",
      "1     0022683E9EA5  When a problem is a change you have to let it ...   \n",
      "2     00299B378633  Dear, Principal\\n\\nIf u change the school poli...   \n",
      "3     003885A45F42  The best time in life is when you become yours...   \n",
      "4     0049B1DF5CCC  Small act of kindness can impact in other peop...   \n",
      "...            ...                                                ...   \n",
      "3906  FFD29828A873  I believe using cellphones in class for educat...   \n",
      "3907  FFD9A83B0849  Working alone, students do not have to argue w...   \n",
      "3908  FFDC4011AC9C  \"A problem is a chance for you to do your best...   \n",
      "3909  FFE16D704B16  Many people disagree with Albert Schweitzer's ...   \n",
      "3910  FFED00D6E0BD  Do you think that failure is the main thing fo...   \n",
      "\n",
      "      cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
      "0          3.5     3.5         3.0          3.0      4.0          3.0  \n",
      "1          2.5     2.5         3.0          2.0      2.0          2.5  \n",
      "2          3.0     3.5         3.0          3.0      3.0          2.5  \n",
      "3          4.5     4.5         4.5          4.5      4.0          5.0  \n",
      "4          2.5     3.0         3.0          3.0      2.5          2.5  \n",
      "...        ...     ...         ...          ...      ...          ...  \n",
      "3906       2.5     3.0         3.0          3.5      2.5          2.5  \n",
      "3907       4.0     4.0         4.0          4.0      3.5          3.0  \n",
      "3908       2.5     3.0         3.0          3.0      3.5          3.0  \n",
      "3909       4.0     4.5         4.5          4.0      4.5          4.5  \n",
      "3910       3.5     2.5         3.5          3.0      3.0          3.5  \n",
      "\n",
      "[3911 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', header=0, sep=',', quotechar='\"', on_bad_lines='skip') #on_bad_lines='skip'\n",
    "print(df.head)\n",
    "# df_test = pd.read_csv('test.csv', header=0, sep=',', quotechar='\"', on_bad_lines='skip') #on_bad_lines='skip'\n",
    "# print(df_test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_id', 'full_text', 'syntax']\n",
      "<bound method NDFrame.head of 0       7.0\n",
      "1       5.0\n",
      "2       7.0\n",
      "3       9.0\n",
      "4       6.0\n",
      "       ... \n",
      "3906    6.0\n",
      "3907    8.0\n",
      "3908    6.0\n",
      "3909    9.0\n",
      "3910    5.0\n",
      "Name: syntax, Length: 3911, dtype: float64>\n",
      "(3911, 3)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['cohesion','vocabulary','phraseology','grammar','conventions'])\n",
    "# df_test = df_test.drop(columns=['cohesion','vocabulary','phraseology','grammar','conventions'])\n",
    "\n",
    "df['syntax'] = df['syntax'] * 2\n",
    "\n",
    "print(list(df.columns))\n",
    "print(df['syntax'].head)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3911.000000\n",
      "mean        6.056507\n",
      "std         1.288798\n",
      "min         2.000000\n",
      "25%         5.000000\n",
      "50%         6.000000\n",
      "75%         7.000000\n",
      "max        10.000000\n",
      "Name: syntax, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['syntax'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lydia\\AppData\\Local\\Temp\\ipykernel_20384\\415936517.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['full_text'] = df['full_text'].str.replace(' +', ' ') #remove duplicate space - problem: might miss\n",
      "C:\\Users\\lydia\\AppData\\Local\\Temp\\ipykernel_20384\\415936517.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['full_text'] = df['full_text'].str.replace(r'[^a-zA-Z\\d\\s:]', '') #removed special characters\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "df['full_text'] = df['full_text'].str.lower()\n",
    "df['full_text'] = df['full_text'].str.lstrip()\n",
    "df['full_text'] = df['full_text'].str.rstrip()\n",
    "df['full_text'] = df['full_text'].str.replace(' +', ' ') #remove duplicate space - problem: might miss\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "df['full_text'] = df['full_text'].str.replace(r'[^a-zA-Z\\d\\s:]', '') #removed special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         text_id                                          full_text  syntax\n",
      "0   0016926B079C  think students would benefit learning homebeca...     7.0\n",
      "1   0022683E9EA5  problem change let best matter happening chang...     5.0\n",
      "2   00299B378633  dear principal change school policy grade b av...     7.0\n",
      "3   003885A45F42  best time life become agree greatest accomplis...     9.0\n",
      "4   0049B1DF5CCC  small act kindness impact people change people...     6.0\n",
      "5   004AC288D833  dear principal school community center reasons...     8.0\n",
      "6   005661280443  imagine could prove people good problem solver...     8.0\n",
      "7   008DDDDD8E8D  think good idea estudnets commit career young ...     5.0\n",
      "8   009BCCC61C2A  positive attitude key success agree anything l...     6.0\n",
      "9   009F4E9310CB  asking one person advice help take strong choi...     6.0\n",
      "10  00B21F9B726F  think good idea students commit career young a...     7.0\n",
      "11  00BCADB373EF  positive attitude key success many people posi...     6.0\n",
      "12  00D281524375  technology allows people many things order thi...     5.0\n",
      "13  00ED2563D0B1  philosopher physician humanitarian albert schw...     6.0\n",
      "14  011AAA636F11  ever solved math problem less 30 seconds math ...     7.0\n",
      "15  01350DF42AED  people decide good posture live thing better e...     4.0\n",
      "16  01405C3C569D  positive attitude key successful person life m...     4.0\n",
      "17  01501F95B8B2  10pm curfews bad idea teens lives would affect...     8.0\n",
      "18  017802562E71  march 12 20019 technology allows people comple...     4.0\n",
      "19  01794F5F1423  schools lunch menu every week imagine changes ...     7.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather TFIDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=2, shuffle=True, stratify=df['syntax'].values.astype(int))\n",
    "\n",
    "train_labels = train['syntax'].values.astype(int).tolist()\n",
    "test_labels = test['syntax'].values.astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector = TfidfVectorizer(analyzer = 'word') #, ngram_range = (2,2)\n",
    "tfidf_train = vector.fit_transform(train['full_text'])\n",
    "tfidf_features = vector.get_feature_names_out()\n",
    "tfidf_test = vector.transform(test['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Running Linear Models on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "         1.5       0.00      0.00      0.00         6\n",
      "         2.0       0.22      0.13      0.17        82\n",
      "         2.5       0.27      0.23      0.24       168\n",
      "         3.0       0.33      0.36      0.35       250\n",
      "         3.5       0.24      0.32      0.27       174\n",
      "         4.0       0.18      0.13      0.15        78\n",
      "         4.5       0.00      0.00      0.00        20\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.26       783\n",
      "   macro avg       0.14      0.13      0.13       783\n",
      "weighted avg       0.26      0.26      0.26       783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "perceptron_classifier = Perceptron(random_state=0) #max_iter=200\n",
    "\n",
    "perceptron_classifier.fit(tfidf_train, train_labels)\n",
    "y_pred = perceptron_classifier.predict(tfidf_test)\n",
    "\n",
    "target_names = df['syntax'].unique() / 2\n",
    "target_names = np.sort(target_names)\n",
    "target_names_list = [str(i) for i in target_names]\n",
    "\n",
    "# print(target_names_list, \"target_names\", len(target_names_list))\n",
    "print(classification_report(test_labels, y_pred, target_names=target_names_list))\n",
    "# print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "         1.5       0.00      0.00      0.00         6\n",
      "         2.0       0.17      0.09      0.11        82\n",
      "         2.5       0.27      0.26      0.26       168\n",
      "         3.0       0.33      0.38      0.35       250\n",
      "         3.5       0.24      0.32      0.27       174\n",
      "         4.0       0.16      0.12      0.13        78\n",
      "         4.5       0.00      0.00      0.00        20\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.27       783\n",
      "   macro avg       0.13      0.13      0.13       783\n",
      "weighted avg       0.25      0.27      0.25       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_classifier = make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=0, tol=1e-5, max_iter=4000))\n",
    "svm_classifier.fit(tfidf_train, train_labels)\n",
    "y_pred = svm_classifier.predict(tfidf_test)\n",
    "print(classification_report(test_labels, y_pred, target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "         1.5       0.00      0.00      0.00         6\n",
      "         2.0       0.50      0.04      0.07        82\n",
      "         2.5       0.28      0.21      0.24       168\n",
      "         3.0       0.34      0.65      0.44       250\n",
      "         3.5       0.30      0.29      0.30       174\n",
      "         4.0       0.00      0.00      0.00        78\n",
      "         4.5       0.00      0.00      0.00        20\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.32       783\n",
      "   macro avg       0.16      0.13      0.12       783\n",
      "weighted avg       0.29      0.32      0.27       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(random_state=0).fit(tfidf_train, train_labels)\n",
    "\n",
    "y_pred = logistic_regression_classifier.predict(tfidf_test)\n",
    "print(classification_report(test_labels, y_pred, target_names=target_names_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim\n",
    "\n",
    "model = api.load('word2vec-google-news-300') #load in the google model from the gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=2, shuffle=True)\n",
    "train_labels = train['syntax'].values.astype(int).tolist()\n",
    "test_labels = test['syntax'].values.astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "word2vec_train_labels = []\n",
    "\n",
    "# take each word in a review and average the word2vec vector \n",
    "for count, row in enumerate(train['full_text'].iteritems()):\n",
    "    tokenized_words = word_tokenize(row[-1])\n",
    "    words = []\n",
    "    for w in tokenized_words:\n",
    "        if w in model.key_to_index:\n",
    "            word_vec = model.get_vector(w)\n",
    "            words.append(word_vec)\n",
    "\n",
    "    average_vals = np.average(words, axis=0)\n",
    "\n",
    "    if average_vals.size == 300:\n",
    "        # add averaged vector to train_sentences for training\n",
    "        train_sentences.append(average_vals) \n",
    "        word2vec_train_labels.append(train_labels[count]) # get label -1 since model classifies 0-4 instead of 1-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train sentences:  3128\n",
      "size of train labels:  3128\n"
     ]
    }
   ],
   "source": [
    "print(\"size of train sentences: \", len(train_sentences))\n",
    "print(\"size of train labels: \", len(word2vec_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "word2vec_test_labels = []\n",
    "\n",
    "# take each word from test set reviews and average the word2vec vector\n",
    "for count, row in enumerate(test['full_text'].iteritems()):\n",
    "    tokenized_words = word_tokenize(row[-1])\n",
    "    words = []\n",
    "    for w in tokenized_words:\n",
    "        if w in model.key_to_index:\n",
    "            word_vec = model.get_vector(w)\n",
    "            words.append(word_vec)\n",
    "    \n",
    "    average_vals = np.average(words, axis=0)\n",
    "    if average_vals.size == 300:\n",
    "        test_sentences.append(average_vals)\n",
    "        word2vec_test_labels.append(test_labels[count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train sentences:  783\n",
      "size of train labels:  783\n",
      "[8, 6, 2, 7, 5, 5, 6, 6, 7, 8, 5, 4, 6, 5, 8, 8, 6, 8, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"size of train sentences: \", len(test_sentences))\n",
    "print(\"size of train labels: \", len(word2vec_test_labels))\n",
    "print(word2vec_test_labels[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the train and test lists for the fnn model\n",
    "word2vec_train_labels_adjusted = [x - 2 for x in word2vec_train_labels]\n",
    "word2vec_test_labels_adjusted = [x - 2 for x in word2vec_test_labels]\n",
    "train_list = list(zip(train_sentences, word2vec_train_labels_adjusted))\n",
    "test_list = list(zip(test_sentences, word2vec_test_labels_adjusted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron class accuracy results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0  0.00000000 0.00000000 0.00000000         6\n",
      "         1.5  0.00000000 0.00000000 0.00000000         7\n",
      "         2.0  0.00000000 0.00000000 0.00000000        93\n",
      "         2.5  0.27325581 0.56287425 0.36790607       167\n",
      "         3.0  0.33410673 0.61016949 0.43178411       236\n",
      "         3.5  0.00000000 0.00000000 0.00000000       171\n",
      "         4.0  0.00000000 0.00000000 0.00000000        81\n",
      "         4.5  0.25000000 0.05555556 0.09090909        18\n",
      "         5.0  0.00000000 0.00000000 0.00000000         4\n",
      "\n",
      "    accuracy                      0.30523627       783\n",
      "   macro avg  0.09526250 0.13651103 0.09895547       783\n",
      "weighted avg  0.16472913 0.30523627 0.21069952       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "perceptron_classifier.fit(train_sentences, word2vec_train_labels)\n",
    "y_pred = perceptron_classifier.predict(test_sentences)\n",
    "\n",
    "print(\"Perceptron class accuracy results\")\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9']\n",
    "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=4000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM class accuracy results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0  0.00000000 0.00000000 0.00000000         6\n",
      "         1.5  0.00000000 0.00000000 0.00000000         7\n",
      "         2.0  0.00000000 0.00000000 0.00000000        93\n",
      "         2.5  0.12500000 0.01796407 0.03141361       167\n",
      "         3.0  0.30820106 0.98728814 0.46975806       236\n",
      "         3.5  0.33333333 0.00584795 0.01149425       171\n",
      "         4.0  0.00000000 0.00000000 0.00000000        81\n",
      "         4.5  0.00000000 0.00000000 0.00000000        18\n",
      "         5.0  0.00000000 0.00000000 0.00000000         4\n",
      "\n",
      "    accuracy                      0.30268199       783\n",
      "   macro avg  0.08517049 0.11234446 0.05696288       783\n",
      "weighted avg  0.19235051 0.30268199 0.15079757       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = make_pipeline(StandardScaler(with_mean=False), SVC(kernel ='rbf', random_state=0, tol=1e-5, max_iter=4000))\n",
    "\n",
    "svm_classifier.fit(train_sentences, word2vec_train_labels)\n",
    "y_pred = svm_classifier.predict(test_sentences)\n",
    "\n",
    "print(\"SVM class accuracy results\")\n",
    "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0  0.00000000 0.00000000 0.00000000         6\n",
      "         1.5  0.00000000 0.00000000 0.00000000         7\n",
      "         2.0  0.66666667 0.02150538 0.04166667        93\n",
      "         2.5  0.26744186 0.13772455 0.18181818       167\n",
      "         3.0  0.33167496 0.84745763 0.47675805       236\n",
      "         3.5  0.28888889 0.15204678 0.19923372       171\n",
      "         4.0  1.00000000 0.01234568 0.02439024        81\n",
      "         4.5  0.00000000 0.00000000 0.00000000        18\n",
      "         5.0  0.00000000 0.00000000 0.00000000         4\n",
      "\n",
      "    accuracy                      0.32183908       783\n",
      "   macro avg  0.28385249 0.13012000 0.10265187       783\n",
      "weighted avg  0.40273063 0.32183908 0.23345863       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_classifier = LogisticRegression(random_state=0).fit(train_sentences, word2vec_train_labels)\n",
    "\n",
    "y_pred = logistic_regression_classifier.predict(test_sentences)\n",
    "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class balancing on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0  0.00000000 0.00000000 0.00000000         6\n",
      "         1.5  0.06578947 0.71428571 0.12048193         7\n",
      "         2.0  0.23026316 0.37634409 0.28571429        93\n",
      "         2.5  0.27722772 0.16766467 0.20895522       167\n",
      "         3.0  0.28767123 0.08898305 0.13592233       236\n",
      "         3.5  0.38095238 0.18713450 0.25098039       171\n",
      "         4.0  0.20652174 0.23456790 0.21965318        81\n",
      "         4.5  0.05504587 0.33333333 0.09448819        18\n",
      "         5.0  0.00000000 0.00000000 0.00000000         4\n",
      "\n",
      "    accuracy                      0.18646232       783\n",
      "   macro avg  0.16705240 0.23359036 0.14624395       783\n",
      "weighted avg  0.27959691 0.18646232 0.20025330       783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#  class_weight=\"balanced\"\n",
    "logistic_regression_classifier = LogisticRegression(random_state=0, class_weight=\"balanced\").fit(train_sentences, word2vec_train_labels)\n",
    "\n",
    "y_pred = logistic_regression_classifier.predict(test_sentences)\n",
    "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning on word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# initialize the NN\n",
    "model_nn = Net()\n",
    "print(model_nn)\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model_nn.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 8\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_sentences)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_size = 0.2\n",
    "# obtain training indices that will be used for validation\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_list, batch_size=batch_size,\n",
    "     sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_list, batch_size=batch_size,\n",
    "     sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_list, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0...... Step: 313/313....... Average Loss for Epoch: 17.06348803172858,  Average Loss Overall: 1.7074398190316642, Validation Loss: 0.40915515256659757\n",
      "Epoch 1...... Step: 313/313....... Average Loss for Epoch: 15.808424586305222,  Average Loss Overall: 1.5818532274659638, Validation Loss: 0.38381511453167555\n",
      "Epoch 2...... Step: 313/313....... Average Loss for Epoch: 14.931731071715918,  Average Loss Overall: 1.4941278214344893, Validation Loss: 0.366100372103474\n",
      "Epoch 3...... Step: 313/313....... Average Loss for Epoch: 14.336988067093749,  Average Loss Overall: 1.434615493925941, Validation Loss: 0.35447391341714296\n",
      "Epoch 4...... Step: 313/313....... Average Loss for Epoch: 13.952061482892631,  Average Loss Overall: 1.3960982238316475, Validation Loss: 0.3470439948236851\n",
      "Epoch 5...... Step: 313/313....... Average Loss for Epoch: 13.705163766400883,  Average Loss Overall: 1.3713926658834643, Validation Loss: 0.34226490705824264\n",
      "Epoch 6...... Step: 313/313....... Average Loss for Epoch: 13.543010867060945,  Average Loss Overall: 1.355167008117032, Validation Loss: 0.33938650032291023\n",
      "Epoch 7...... Step: 313/313....... Average Loss for Epoch: 13.434726718896494,  Average Loss Overall: 1.3443316697617016, Validation Loss: 0.33737113340126584\n",
      "Epoch 8...... Step: 313/313....... Average Loss for Epoch: 13.360547842689977,  Average Loss Overall: 1.3369090392461518, Validation Loss: 0.33604577808733793\n",
      "Epoch 9...... Step: 313/313....... Average Loss for Epoch: 13.312552001529609,  Average Loss Overall: 1.332106386342317, Validation Loss: 0.3353452447354031\n",
      "Epoch 10...... Step: 313/313....... Average Loss for Epoch: 13.276818834554653,  Average Loss Overall: 1.3285307849154753, Validation Loss: 0.33496082118709986\n",
      "Epoch 11...... Step: 313/313....... Average Loss for Epoch: 13.254365636898687,  Average Loss Overall: 1.32628402952343, Validation Loss: 0.3345012607629342\n",
      "Epoch 12...... Step: 313/313....... Average Loss for Epoch: 13.239476837289219,  Average Loss Overall: 1.3247941975931987, Validation Loss: 0.3343924375827355\n",
      "Epoch 13...... Step: 313/313....... Average Loss for Epoch: 13.22746011738579,  Average Loss Overall: 1.3235917572703813, Validation Loss: 0.33437912307127055\n",
      "Epoch 14...... Step: 313/313....... Average Loss for Epoch: 13.219898750225957,  Average Loss Overall: 1.3228351370910245, Validation Loss: 0.3340968257554657\n",
      "Epoch 15...... Step: 313/313....... Average Loss for Epoch: 13.212186753178557,  Average Loss Overall: 1.3220634442918442, Validation Loss: 0.3339338001159146\n",
      "Epoch 16...... Step: 313/313....... Average Loss for Epoch: 13.208013802671585,  Average Loss Overall: 1.3216458824284547, Validation Loss: 0.3340220231076945\n",
      "Epoch 17...... Step: 313/313....... Average Loss for Epoch: 13.204909716551295,  Average Loss Overall: 1.3213352753454461, Validation Loss: 0.3338801357752222\n",
      "Epoch 18...... Step: 313/313....... Average Loss for Epoch: 13.202881106172507,  Average Loss Overall: 1.3211322846010214, Validation Loss: 0.3337845949032118\n",
      "Epoch 19...... Step: 313/313....... Average Loss for Epoch: 13.199414535833244,  Average Loss Overall: 1.320785405919375, Validation Loss: 0.33381078847686346\n",
      "Epoch 20...... Step: 313/313....... Average Loss for Epoch: 13.196016905025933,  Average Loss Overall: 1.3204454255988225, Validation Loss: 0.3336851019078813\n",
      "Epoch 21...... Step: 313/313....... Average Loss for Epoch: 13.196175802629977,  Average Loss Overall: 1.3204613255189204, Validation Loss: 0.33376696545754553\n",
      "Epoch 22...... Step: 313/313....... Average Loss for Epoch: 13.191728046907784,  Average Loss Overall: 1.320016265563343, Validation Loss: 0.33368643649551266\n",
      "Epoch 23...... Step: 313/313....... Average Loss for Epoch: 13.192176727441172,  Average Loss Overall: 1.3200611623046954, Validation Loss: 0.3338640005615971\n",
      "Epoch 24...... Step: 313/313....... Average Loss for Epoch: 13.192037758354942,  Average Loss Overall: 1.320047256510581, Validation Loss: 0.3337901431657469\n",
      "Epoch 25...... Step: 313/313....... Average Loss for Epoch: 13.190965594194187,  Average Loss Overall: 1.3199399715418096, Validation Loss: 0.3338159058252564\n",
      "Epoch 26...... Step: 313/313....... Average Loss for Epoch: 13.188943979077445,  Average Loss Overall: 1.319737680770857, Validation Loss: 0.3339473007203978\n",
      "Epoch 27...... Step: 313/313....... Average Loss for Epoch: 13.188437735310758,  Average Loss Overall: 1.3196870240256608, Validation Loss: 0.3339608340617031\n",
      "Epoch 28...... Step: 313/313....... Average Loss for Epoch: 13.188724035272202,  Average Loss Overall: 1.3197156723274295, Validation Loss: 0.33358202779384527\n",
      "Epoch 29...... Step: 313/313....... Average Loss for Epoch: 13.189627870964928,  Average Loss Overall: 1.3198061136867079, Validation Loss: 0.3336307260843799\n",
      "Epoch 30...... Step: 313/313....... Average Loss for Epoch: 13.186716559215094,  Average Loss Overall: 1.3195147963664722, Validation Loss: 0.33354336777916344\n",
      "Epoch 31...... Step: 313/313....... Average Loss for Epoch: 13.185730228408838,  Average Loss Overall: 1.3194161002212168, Validation Loss: 0.3337714345482609\n",
      "Epoch 32...... Step: 313/313....... Average Loss for Epoch: 13.184414026455377,  Average Loss Overall: 1.3192843958697356, Validation Loss: 0.3336197651560654\n",
      "Epoch 33...... Step: 313/313....... Average Loss for Epoch: 13.184430055724928,  Average Loss Overall: 1.31928599982158, Validation Loss: 0.33350710178275245\n",
      "Epoch 34...... Step: 313/313....... Average Loss for Epoch: 13.179793016979108,  Average Loss Overall: 1.3188219994611448, Validation Loss: 0.33368588804893784\n",
      "Epoch 35...... Step: 313/313....... Average Loss for Epoch: 13.18441499993443,  Average Loss Overall: 1.319284493279884, Validation Loss: 0.33349042875535045\n",
      "Epoch 36...... Step: 313/313....... Average Loss for Epoch: 13.18485946967579,  Average Loss Overall: 1.3193289686728011, Validation Loss: 0.3337910953156479\n",
      "Epoch 37...... Step: 313/313....... Average Loss for Epoch: 13.181814386440923,  Average Loss Overall: 1.319024265650898, Validation Loss: 0.33336614823097466\n",
      "Epoch 38...... Step: 313/313....... Average Loss for Epoch: 13.185099641355082,  Average Loss Overall: 1.3193530011969758, Validation Loss: 0.3334303657569544\n",
      "Epoch 39...... Step: 313/313....... Average Loss for Epoch: 13.181801343878238,  Average Loss Overall: 1.319022960560706, Validation Loss: 0.33332705139504065\n",
      "Epoch 40...... Step: 313/313....... Average Loss for Epoch: 13.184484885142634,  Average Loss Overall: 1.319291486269068, Validation Loss: 0.33347952998507663\n",
      "Epoch 41...... Step: 313/313....... Average Loss for Epoch: 13.180072595136235,  Average Loss Overall: 1.3188499751526985, Validation Loss: 0.3337910648654489\n",
      "Epoch 42...... Step: 313/313....... Average Loss for Epoch: 13.18256864456323,  Average Loss Overall: 1.3190997396893513, Validation Loss: 0.33344295209326097\n",
      "Epoch 43...... Step: 313/313....... Average Loss for Epoch: 13.177555236572656,  Average Loss Overall: 1.318598078339911, Validation Loss: 0.3338026397520929\n",
      "Epoch 44...... Step: 313/313....... Average Loss for Epoch: 13.18137400142682,  Average Loss Overall: 1.318980198991878, Validation Loss: 0.3334984969909844\n",
      "Epoch 45...... Step: 313/313....... Average Loss for Epoch: 13.178551387101317,  Average Loss Overall: 1.318697757085266, Validation Loss: 0.3335002113105086\n",
      "Epoch 46...... Step: 313/313....... Average Loss for Epoch: 13.179361995035848,  Average Loss Overall: 1.318778869707871, Validation Loss: 0.3335121292287431\n",
      "Epoch 47...... Step: 313/313....... Average Loss for Epoch: 13.178632645561292,  Average Loss Overall: 1.3187058881268172, Validation Loss: 0.3335191194739793\n",
      "Epoch 48...... Step: 313/313....... Average Loss for Epoch: 13.179561774570722,  Average Loss Overall: 1.3187988604349858, Validation Loss: 0.33345092021290906\n",
      "Epoch 49...... Step: 313/313....... Average Loss for Epoch: 13.176602796243783,  Average Loss Overall: 1.3185027734093044, Validation Loss: 0.333213643954538\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0 #use validation to gage overfitting and best epoch length\n",
    "    counter = 0 # use to keep track of number of occurrences\n",
    "\n",
    "    # train the model \n",
    "    model_nn.train() # prep model for training\n",
    "    for idx, (data,target) in enumerate(train_loader):\n",
    "        counter += 1\n",
    "\n",
    "        # clear the gradients of all optimized variables       \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_nn(data)\n",
    "        # calculate the loss\n",
    "        # print(target.shape)\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        if counter%2000 == 0 or counter == len(train_loader):\n",
    "            #if a print encounter, run the validation set to determine error\n",
    "            for data, target in valid_loader:\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model_nn(data)\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, target)\n",
    "                # update running validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "            print(\"Epoch {}...... Step: {}/{}....... Average Loss for Epoch: {},  Average Loss Overall: {}, Validation Loss: {}\"\n",
    "                  .format(epoch, counter, len(train_loader), train_loss/counter, train_loss/len(train_loader.dataset), valid_loss))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build predict model for test set\n",
    "def predict(model_nn, dataloader, y_true):\n",
    "    prediction_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # print(batch)\n",
    "        data = torch.tensor(batch[0])   \n",
    "        outputs = model_nn(data)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        prediction_list.append(predicted.cpu())\n",
    "        y_true.append(batch[1])\n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "#build accuracy model for FNN\n",
    "def accuracy(model_nn, predictions, y_true, batch, test_size):\n",
    "    correct = 0\n",
    "    # print(len(predictions))\n",
    "    for i in range(0, len(predictions)):\n",
    "        curBatch = predictions[i]\n",
    "        # print(curBatch)\n",
    "        curLabels = y_true[i]\n",
    "        \n",
    "        for j, pred in enumerate(curBatch):\n",
    "            # print(pred, curLabels[j])           \n",
    "            if pred == curLabels[j]:\n",
    "                correct += 1\n",
    "        # print(correct)\n",
    "    \n",
    "    print(correct)\n",
    "    accuracy = correct / test_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size:  783\n",
      "size of test sentences:  783\n",
      "98\n",
      "torch.Size([8])\n",
      "98\n",
      "236\n",
      "0.30140485312899107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lydia\\AppData\\Local\\Temp\\ipykernel_20384\\223528509.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(batch[0])\n"
     ]
    }
   ],
   "source": [
    "test_size = len(test_sentences)\n",
    "\n",
    "print(\"test size: \", test_size)\n",
    "print(\"size of test sentences: \", len(test_sentences))\n",
    "\n",
    "y_true = []\n",
    "predictions = predict(model_nn,test_loader, y_true)\n",
    "\n",
    "print(len(predictions))\n",
    "print(predictions[0].shape)\n",
    "print(len(y_true))\n",
    "\n",
    "print(accuracy(model_nn, predictions, y_true, 20, test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try XLNet BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\lydia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\lydia\\Documents\\Class\\csci544\\english_language_learning_CS544\\data\\xlnet_base_cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\lydia\\Documents\\Class\\csci544\\english_language_learning_CS544\\data\\xlnet_base_cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@yingbiao/text-classification-with-xlnet-in-action-869029246f7e\n",
    "from transformers import XLNetForSequenceClassification\n",
    "# model_file_address = './data/xlnet-base-cased'\n",
    "model_file_address = \"C:\\\\Users\\\\lydia\\\\Documents\\\\Class\\\\csci544\\\\english_language_learning_CS544\\\\data\\\\xlnet_base_cased\"\n",
    "\n",
    "# Will load config and weight with from_pretrained()\n",
    "# Recommand download the model before using\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\"\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json\" \n",
    "model = XLNetForSequenceClassification.from_pretrained(model_file_address,num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultipleChoice: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultipleChoice were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import XLNetTokenizer, XLNetModel\n",
    "#  http://restanalytics.com/2021-05-04-Fine-Tuning-XLNet-For-Sequence-Classification/\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, XLNetModel, XLNetForMultipleChoice\n",
    "import torch\n",
    "\n",
    "xlnet_model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(xlnet_model_name, do_lower_case=True)\n",
    "\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetForMultipleChoice.from_pretrained('xlnet-base-cased', num_labels=len(target_names_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_lens = []\n",
    "token_list = []\n",
    "\n",
    "for txt in df['full_text'] :\n",
    "  tokens = tokenizer.encode(txt, max_length=512, add_special_tokens=False)\n",
    "  # max_length=512\n",
    "  token_lens.append(len(tokens))\n",
    "  token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=2, shuffle=True, stratify=df['syntax'].values.astype(int))\n",
    "\n",
    "train_labels = train['syntax'].values.astype(int).tolist()\n",
    "test_labels = test['syntax'].values.astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232, 466, 74, 1887, 1899, 192, 11730, 459, 133, 319, 907, 6256, 1162, 637, 2814, 3272, 480, 616, 1219, 3251, 192, 13646, 188, 297, 723, 1237, 4042, 216, 9509, 4232, 216, 12684, 338, 8933, 23, 232, 2779, 8322, 216, 338, 5868, 725, 115, 338, 197, 14095, 459, 560, 2814, 3134, 730, 1078, 192, 214, 3771, 3134, 466, 1044, 182, 17146, 297, 725, 182, 2284, 4042, 466, 5720, 195, 3535, 3706, 2400, 1864, 7273, 92, 280, 471, 297, 560, 2814, 214, 3706, 6192, 133, 931, 257, 216, 182, 6256, 133, 1237, 216, 192, 3251, 616, 1219, 1849, 2141, 25196, 176, 1486, 21700, 1075, 154, 281, 1326, 1132, 176, 714, 2804, 175, 4579, 162, 466, 1111, 3535, 466, 5199, 132, 5632, 1075]  is being processed\n",
      "tensor([  232,   466,    74,  1887,  1899,   192, 11730,   459,   133,   319,\n",
      "          907,  6256,  1162,   637,  2814,  3272,   480,   616,  1219,  3251,\n",
      "          192, 13646,   188,   297,   723,  1237,  4042,   216,  9509,  4232,\n",
      "          216, 12684,   338,  8933,    23,   232,  2779,  8322,   216,   338,\n",
      "         5868,   725,   115,   338,   197, 14095,   459,   560,  2814,  3134,\n",
      "          730,  1078,   192,   214,  3771,  3134,   466,  1044,   182, 17146,\n",
      "          297,   725,   182,  2284,  4042,   466,  5720,   195,  3535,  3706,\n",
      "         2400,  1864,  7273,    92,   280,   471,   297,   560,  2814,   214,\n",
      "         3706,  6192,   133,   931,   257,   216,   182,  6256,   133,  1237,\n",
      "          216,   192,  3251,   616,  1219,  1849,  2141, 25196,   176,  1486,\n",
      "        21700,  1075,   154,   281,  1326,  1132,   176,   714,  2804,   175,\n",
      "         4579,   162,   466,  1111,  3535,   466,  5199,   132,  5632,  1075,\n",
      "            4,     3])  is the tokenized encoding torch.Size([122])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lydia\\Documents\\Class\\csci544\\english_language_learning_CS544\\syntax.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lydia/Documents/Class/csci544/english_language_learning_CS544/syntax.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tokenized_encoding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokenizer\u001b[39m.\u001b[39mencode(token))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mflatten() \u001b[39m#add_special_tokens=False\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lydia/Documents/Class/csci544/english_language_learning_CS544/syntax.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(tokenized_encoding, \u001b[39m\"\u001b[39m\u001b[39m is the tokenized encoding\u001b[39m\u001b[39m\"\u001b[39m, tokenized_encoding\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lydia/Documents/Class/csci544/english_language_learning_CS544/syntax.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     outputs\u001b[39m.\u001b[39mappend(model(tokenized_encoding, labels\u001b[39m=\u001b[39;49mtarget_names_list))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lydia/Documents/Class/csci544/english_language_learning_CS544/syntax.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(outputs[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lydia/Documents/Class/csci544/english_language_learning_CS544/syntax.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# last_hidden_states = outputs[0]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lydia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1744\u001b[0m, in \u001b[0;36mXLNetForMultipleChoice.forward\u001b[1;34m(self, input_ids, token_type_ids, input_mask, attention_mask, mems, perm_mask, target_mapping, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39m    Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[39m    num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m \u001b[39m    `input_ids` above)\u001b[39;00m\n\u001b[0;32m   1741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1744\u001b[0m num_choices \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1746\u001b[0m flat_input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, input_ids\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m flat_token_type_ids \u001b[39m=\u001b[39m token_type_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, token_type_ids\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# input_ids = torch.tensor([tokenizer.encode(s , add_special_tokens=False) for s in token_list]).unsqueeze(0)\n",
    "\n",
    "# outputs = model(token_list)\n",
    "\n",
    "outputs = []\n",
    "for i, token in enumerate(token_list):\n",
    "    print(token, \" is being processed\")\n",
    "    tokenized_encoding = torch.tensor(tokenizer.encode(token)).unsqueeze(0).flatten() #add_special_tokens=False\n",
    "    print(tokenized_encoding, \" is the tokenized encoding\", tokenized_encoding.shape)\n",
    "    outputs.append(model(tokenized_encoding, labels=target_names_list))\n",
    "    print(outputs[i])\n",
    "# last_hidden_states = outputs[0]\n",
    "loss, classification_scores = outputs[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss, classification_scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-16.3427, -35.1695, -34.6468,  ..., -31.4001, -22.6753, -21.7447],\n",
      "         [-24.7520, -42.5470, -42.2781,  ..., -38.6128, -30.3361, -32.1718],\n",
      "         [-27.2489, -43.5741, -43.4678,  ..., -40.3299, -37.2766, -36.0232],\n",
      "         [-21.8894, -42.1254, -42.0676,  ..., -41.2019, -33.7251, -34.1798],\n",
      "         [-21.6637, -39.6066, -39.5360,  ..., -36.6144, -32.3140, -30.9366],\n",
      "         [-14.9458, -32.7998, -32.7879,  ..., -30.9330, -24.0895, -23.7962]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60fab8fe8636a998ca6cb0b1b1e73f8aa28fc993054ea5d496530e7813c285a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
