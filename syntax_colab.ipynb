{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZIBVbwxiMagq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "141UCMZqMagr",
        "outputId": "5e9bfa4b-eaed-43e9-f41e-4eeb42fcb407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of            text_id                                          full_text  \\\n",
            "0     0016926B079C  I think that students would benefit from learn...   \n",
            "1     0022683E9EA5  When a problem is a change you have to let it ...   \n",
            "2     00299B378633  Dear, Principal\\n\\nIf u change the school poli...   \n",
            "3     003885A45F42  The best time in life is when you become yours...   \n",
            "4     0049B1DF5CCC  Small act of kindness can impact in other peop...   \n",
            "...            ...                                                ...   \n",
            "3906  FFD29828A873  I believe using cellphones in class for educat...   \n",
            "3907  FFD9A83B0849  Working alone, students do not have to argue w...   \n",
            "3908  FFDC4011AC9C  \"A problem is a chance for you to do your best...   \n",
            "3909  FFE16D704B16  Many people disagree with Albert Schweitzer's ...   \n",
            "3910  FFED00D6E0BD  Do you think that failure is the main thing fo...   \n",
            "\n",
            "      cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
            "0          3.5     3.5         3.0          3.0      4.0          3.0  \n",
            "1          2.5     2.5         3.0          2.0      2.0          2.5  \n",
            "2          3.0     3.5         3.0          3.0      3.0          2.5  \n",
            "3          4.5     4.5         4.5          4.5      4.0          5.0  \n",
            "4          2.5     3.0         3.0          3.0      2.5          2.5  \n",
            "...        ...     ...         ...          ...      ...          ...  \n",
            "3906       2.5     3.0         3.0          3.5      2.5          2.5  \n",
            "3907       4.0     4.0         4.0          4.0      3.5          3.0  \n",
            "3908       2.5     3.0         3.0          3.0      3.5          3.0  \n",
            "3909       4.0     4.5         4.5          4.0      4.5          4.5  \n",
            "3910       3.5     2.5         3.5          3.0      3.0          3.5  \n",
            "\n",
            "[3911 rows x 8 columns]>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('train.csv', header=0, sep=',', quotechar='\"', on_bad_lines='skip') #on_bad_lines='skip'\n",
        "print(df.head)\n",
        "# df_test = pd.read_csv('test.csv', header=0, sep=',', quotechar='\"', on_bad_lines='skip') #on_bad_lines='skip'\n",
        "# print(df_test.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKtzAZyFMags",
        "outputId": "8b835ae4-d29c-4387-ab25-32ff5f7a1f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text_id', 'full_text', 'syntax']\n",
            "<bound method NDFrame.head of 0       7.0\n",
            "1       5.0\n",
            "2       7.0\n",
            "3       9.0\n",
            "4       6.0\n",
            "       ... \n",
            "3906    6.0\n",
            "3907    8.0\n",
            "3908    6.0\n",
            "3909    9.0\n",
            "3910    5.0\n",
            "Name: syntax, Length: 3911, dtype: float64>\n",
            "(3911, 3)\n"
          ]
        }
      ],
      "source": [
        "df = df.drop(columns=['cohesion','vocabulary','phraseology','grammar','conventions'])\n",
        "# df_test = df_test.drop(columns=['cohesion','vocabulary','phraseology','grammar','conventions'])\n",
        "\n",
        "df['syntax'] = df['syntax'] * 2\n",
        "\n",
        "print(list(df.columns))\n",
        "print(df['syntax'].head)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzx5lr7MMagt",
        "outputId": "ee79a89d-af89-4852-fde7-cd4595e3eb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    3911.000000\n",
            "mean        6.056507\n",
            "std         1.288798\n",
            "min         2.000000\n",
            "25%         5.000000\n",
            "50%         6.000000\n",
            "75%         7.000000\n",
            "max        10.000000\n",
            "Name: syntax, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(df['syntax'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzoceuvHMagt"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aquXhsI7Myou",
        "outputId": "482d23ef-1a96-4302-9bbd-6dad19cded74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ0nHxxJMagu",
        "outputId": "4ba9a692-e6d1-4340-f80d-5cd9088d659d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "import contractions\n",
        "\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "df['full_text'] = df['full_text'].str.lstrip()\n",
        "df['full_text'] = df['full_text'].str.rstrip()\n",
        "df['full_text'] = df['full_text'].str.replace(' +', ' ') #remove duplicate space - problem: might miss\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
        "\n",
        "df['full_text'] = df['full_text'].str.replace(r'[^a-zA-Z\\d\\s:]', '') #removed special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0ew4qOQ1Magv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61df4de-f186-42d0-bc72-e2b6fd742520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         text_id                                          full_text  syntax\n",
            "0   0016926B079C  think students would benefit learning homebeca...     7.0\n",
            "1   0022683E9EA5  problem change let best matter happening chang...     5.0\n",
            "2   00299B378633  dear principal change school policy grade b av...     7.0\n",
            "3   003885A45F42  best time life become agree greatest accomplis...     9.0\n",
            "4   0049B1DF5CCC  small act kindness impact people change people...     6.0\n",
            "5   004AC288D833  dear principal school community center reasons...     8.0\n",
            "6   005661280443  imagine could prove people good problem solver...     8.0\n",
            "7   008DDDDD8E8D  think good idea estudnets commit career young ...     5.0\n",
            "8   009BCCC61C2A  positive attitude key success agree anything l...     6.0\n",
            "9   009F4E9310CB  asking one person advice help take strong choi...     6.0\n",
            "10  00B21F9B726F  think good idea students commit career young a...     7.0\n",
            "11  00BCADB373EF  positive attitude key success many people posi...     6.0\n",
            "12  00D281524375  technology allows people many things order thi...     5.0\n",
            "13  00ED2563D0B1  philosopher physician humanitarian albert schw...     6.0\n",
            "14  011AAA636F11  ever solved math problem less 30 seconds math ...     7.0\n",
            "15  01350DF42AED  people decide good posture live thing better e...     4.0\n",
            "16  01405C3C569D  positive attitude key successful person life m...     4.0\n",
            "17  01501F95B8B2  10pm curfews bad idea teens lives would affect...     8.0\n",
            "18  017802562E71  march 12 20019 technology allows people comple...     4.0\n",
            "19  01794F5F1423  schools lunch menu every week imagine changes ...     7.0\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "print(df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_IgMy5eFMagw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=2, shuffle=True, stratify=df['syntax'].values.astype(int))\n",
        "\n",
        "train_labels = train['syntax'].values.astype(int).tolist()\n",
        "test_labels = test['syntax'].values.astype(int).tolist()\n",
        "\n",
        "target_names = df['syntax'].unique() / 2\n",
        "target_names = np.sort(target_names)\n",
        "target_names_list = [str(i) for i in target_names]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW2DYVw9Magw"
      },
      "source": [
        "# Gather TFIDF values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkOZYEDkMagx"
      },
      "outputs": [],
      "source": [
        "\n",
        "vector = TfidfVectorizer(analyzer = 'word') #, ngram_range = (2,2)\n",
        "tfidf_train = vector.fit_transform(train['full_text'])\n",
        "tfidf_features = vector.get_feature_names_out()\n",
        "tfidf_test = vector.transform(test['full_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVh3rSq-Magx"
      },
      "source": [
        "# Begin Running Linear Models on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2r08d2VMagx",
        "outputId": "3f3cba72-ed86-4f7b-d37a-a2d82e93a576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         6\n",
            "         2.0       0.22      0.13      0.17        82\n",
            "         2.5       0.27      0.23      0.24       168\n",
            "         3.0       0.33      0.36      0.35       250\n",
            "         3.5       0.24      0.32      0.27       174\n",
            "         4.0       0.18      0.13      0.15        78\n",
            "         4.5       0.00      0.00      0.00        20\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.26       783\n",
            "   macro avg       0.14      0.13      0.13       783\n",
            "weighted avg       0.26      0.26      0.26       783\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "perceptron_classifier = Perceptron(random_state=0) #max_iter=200\n",
        "\n",
        "perceptron_classifier.fit(tfidf_train, train_labels)\n",
        "y_pred = perceptron_classifier.predict(tfidf_test)\n",
        "\n",
        "target_names = df['syntax'].unique() / 2\n",
        "target_names = np.sort(target_names)\n",
        "target_names_list = [str(i) for i in target_names]\n",
        "\n",
        "# print(target_names_list, \"target_names\", len(target_names_list))\n",
        "print(classification_report(test_labels, y_pred, target_names=target_names_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmg2k3YwMagy",
        "outputId": "2a1fd5a4-ba0a-4954-d729-33a06e102ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         6\n",
            "         2.0       0.17      0.09      0.11        82\n",
            "         2.5       0.27      0.26      0.26       168\n",
            "         3.0       0.33      0.38      0.35       250\n",
            "         3.5       0.24      0.32      0.27       174\n",
            "         4.0       0.16      0.12      0.13        78\n",
            "         4.5       0.00      0.00      0.00        20\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.27       783\n",
            "   macro avg       0.13      0.13      0.13       783\n",
            "weighted avg       0.25      0.27      0.25       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "svm_classifier = make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=0, tol=1e-5, max_iter=4000))\n",
        "svm_classifier.fit(tfidf_train, train_labels)\n",
        "y_pred = svm_classifier.predict(tfidf_test)\n",
        "print(classification_report(test_labels, y_pred, target_names=target_names_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWzn3cSPMagy",
        "outputId": "0a7ce732-6b23-4529-e274-7e1cfce247a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         6\n",
            "         2.0       0.50      0.04      0.07        82\n",
            "         2.5       0.28      0.21      0.24       168\n",
            "         3.0       0.34      0.65      0.44       250\n",
            "         3.5       0.30      0.29      0.30       174\n",
            "         4.0       0.00      0.00      0.00        78\n",
            "         4.5       0.00      0.00      0.00        20\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.32       783\n",
            "   macro avg       0.16      0.13      0.12       783\n",
            "weighted avg       0.29      0.32      0.27       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression_classifier = LogisticRegression(random_state=0).fit(tfidf_train, train_labels)\n",
        "\n",
        "y_pred = logistic_regression_classifier.predict(tfidf_test)\n",
        "print(classification_report(test_labels, y_pred, target_names=target_names_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcBXE1kPMagz"
      },
      "source": [
        "# Try using word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "kjmChU03Magz",
        "outputId": "e5747e08-88f5-4463-c07d-d27bdcc3d32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[================----------------------------------] 32.7% 543.2/1662.8MB downloaded"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d0713cf0d82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load in the google model from the gensim library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 round(float(total_size) / (1024 * 1024), 1))\n\u001b[1;32m     97\u001b[0m         )\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         sys.stdout.write(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import gensim\n",
        "\n",
        "model = api.load('word2vec-google-news-300') #load in the google model from the gensim library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRRlx90_Magz"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=2, shuffle=True)\n",
        "train_labels = train['syntax'].values.astype(int).tolist()\n",
        "test_labels = test['syntax'].values.astype(int).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjzDbqLUMagz"
      },
      "outputs": [],
      "source": [
        "train_sentences = []\n",
        "word2vec_train_labels = []\n",
        "\n",
        "# take each word in a review and average the word2vec vector \n",
        "for count, row in enumerate(train['full_text'].iteritems()):\n",
        "    tokenized_words = word_tokenize(row[-1])\n",
        "    words = []\n",
        "    for w in tokenized_words:\n",
        "        if w in model.key_to_index:\n",
        "            word_vec = model.get_vector(w)\n",
        "            words.append(word_vec)\n",
        "\n",
        "    average_vals = np.average(words, axis=0)\n",
        "\n",
        "    if average_vals.size == 300:\n",
        "        # add averaged vector to train_sentences for training\n",
        "        train_sentences.append(average_vals) \n",
        "        word2vec_train_labels.append(train_labels[count]) # get label -1 since model classifies 0-4 instead of 1-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPQKzED2Magz"
      },
      "outputs": [],
      "source": [
        "print(\"size of train sentences: \", len(train_sentences))\n",
        "print(\"size of train labels: \", len(word2vec_train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWfoBbpAMag0"
      },
      "outputs": [],
      "source": [
        "test_sentences = []\n",
        "word2vec_test_labels = []\n",
        "\n",
        "# take each word from test set reviews and average the word2vec vector\n",
        "for count, row in enumerate(test['full_text'].iteritems()):\n",
        "    tokenized_words = word_tokenize(row[-1])\n",
        "    words = []\n",
        "    for w in tokenized_words:\n",
        "        if w in model.key_to_index:\n",
        "            word_vec = model.get_vector(w)\n",
        "            words.append(word_vec)\n",
        "    \n",
        "    average_vals = np.average(words, axis=0)\n",
        "    if average_vals.size == 300:\n",
        "        test_sentences.append(average_vals)\n",
        "        word2vec_test_labels.append(test_labels[count])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfq-G1oJMag0"
      },
      "outputs": [],
      "source": [
        "print(\"size of train sentences: \", len(test_sentences))\n",
        "print(\"size of train labels: \", len(word2vec_test_labels))\n",
        "print(word2vec_test_labels[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yEK_DDIMag0"
      },
      "outputs": [],
      "source": [
        "# build the train and test lists for the fnn model\n",
        "word2vec_train_labels_adjusted = [x - 2 for x in word2vec_train_labels]\n",
        "word2vec_test_labels_adjusted = [x - 2 for x in word2vec_test_labels]\n",
        "train_list = list(zip(train_sentences, word2vec_train_labels_adjusted))\n",
        "test_list = list(zip(test_sentences, word2vec_test_labels_adjusted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P8qvPiWMag0"
      },
      "outputs": [],
      "source": [
        "perceptron_classifier.fit(train_sentences, word2vec_train_labels)\n",
        "y_pred = perceptron_classifier.predict(test_sentences)\n",
        "\n",
        "print(\"Perceptron class accuracy results\")\n",
        "target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9']\n",
        "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvVZFDLUMag1"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_classifier = make_pipeline(StandardScaler(with_mean=False), SVC(kernel ='rbf', random_state=0, tol=1e-5, max_iter=4000))\n",
        "\n",
        "svm_classifier.fit(train_sentences, word2vec_train_labels)\n",
        "y_pred = svm_classifier.predict(test_sentences)\n",
        "\n",
        "print(\"SVM class accuracy results\")\n",
        "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROqkoSlaMag1"
      },
      "outputs": [],
      "source": [
        "logistic_regression_classifier = LogisticRegression(random_state=0).fit(train_sentences, word2vec_train_labels)\n",
        "\n",
        "y_pred = logistic_regression_classifier.predict(test_sentences)\n",
        "print(classification_report(word2vec_test_labels, y_pred, target_names=target_names_list, digits=8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttw1GRY5Mag1"
      },
      "source": [
        "# Deep learning on word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADG3Yz6qMag1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3tx-lQTMag2"
      },
      "outputs": [],
      "source": [
        "# define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(300, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "# initialize the NN\n",
        "model_nn = Net()\n",
        "print(model_nn)\n",
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "optimizer = torch.optim.SGD(model_nn.parameters(), lr=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5npPsImMag2"
      },
      "outputs": [],
      "source": [
        "# number of subprocesses to use for data loading\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 8\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_sentences)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "valid_size = 0.2\n",
        "# obtain training indices that will be used for validation\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_list, batch_size=batch_size,\n",
        "     sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_list, batch_size=batch_size,\n",
        "     sampler=valid_sampler, num_workers=num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_list, batch_size=batch_size, \n",
        "    num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGMpFV4ZMag2"
      },
      "outputs": [],
      "source": [
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0 #use validation to gage overfitting and best epoch length\n",
        "    counter = 0 # use to keep track of number of occurrences\n",
        "\n",
        "    # train the model \n",
        "    model_nn.train() # prep model for training\n",
        "    for idx, (data,target) in enumerate(train_loader):\n",
        "        counter += 1\n",
        "\n",
        "        # clear the gradients of all optimized variables       \n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_nn(data)\n",
        "        # calculate the loss\n",
        "        # print(target.shape)\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "        if counter%2000 == 0 or counter == len(train_loader):\n",
        "            #if a print encounter, run the validation set to determine error\n",
        "            for data, target in valid_loader:\n",
        "                # forward pass: compute predicted outputs by passing inputs to the model\n",
        "                output = model_nn(data)\n",
        "                # calculate the loss\n",
        "                loss = criterion(output, target)\n",
        "                # update running validation loss \n",
        "                valid_loss += loss.item()*data.size(0)\n",
        "            \n",
        "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "            print(\"Epoch {}...... Step: {}/{}....... Average Loss for Epoch: {},  Average Loss Overall: {}, Validation Loss: {}\"\n",
        "                  .format(epoch, counter, len(train_loader), train_loss/counter, train_loss/len(train_loader.dataset), valid_loss))\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-ruyKvGMag2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#build predict model for test set\n",
        "def predict(model_nn, dataloader, y_true):\n",
        "    prediction_list = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # print(batch)\n",
        "        data = torch.tensor(batch[0])   \n",
        "        outputs = model_nn(data)\n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        prediction_list.append(predicted.cpu())\n",
        "        y_true.append(batch[1])\n",
        "    return prediction_list\n",
        "\n",
        "\n",
        "#build accuracy model for FNN\n",
        "def accuracy(model_nn, predictions, y_true, batch, test_size):\n",
        "    correct = 0\n",
        "    # print(len(predictions))\n",
        "    for i in range(0, len(predictions)):\n",
        "        curBatch = predictions[i]\n",
        "        # print(curBatch)\n",
        "        curLabels = y_true[i]\n",
        "        \n",
        "        for j, pred in enumerate(curBatch):\n",
        "            # print(pred, curLabels[j])           \n",
        "            if pred == curLabels[j]:\n",
        "                correct += 1\n",
        "        # print(correct)\n",
        "    \n",
        "    print(correct)\n",
        "    accuracy = correct / test_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEIAPbV0Mag3"
      },
      "outputs": [],
      "source": [
        "test_size = len(test_sentences)\n",
        "\n",
        "print(\"test size: \", test_size)\n",
        "print(\"size of test sentences: \", len(test_sentences))\n",
        "\n",
        "y_true = []\n",
        "predictions = predict(model_nn,test_loader, y_true)\n",
        "\n",
        "print(len(predictions))\n",
        "print(predictions[0].shape)\n",
        "print(len(y_true))\n",
        "\n",
        "print(accuracy(model_nn, predictions, y_true, 20, test_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHfEFg-uMag3"
      },
      "source": [
        "# Try XLNet BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mydAS-ZaMag3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34656a2-536c-4ac6-c904-13a31f92600e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sentencepiece\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "HTBfPAb4Mag4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad67d49-5038-4428-e35e-386d7590c6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "60MXvJ7YMag4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdc2d78-b78e-47b0-9265-3dc3b9a84a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "['1.0', '1.5', '2.0', '2.5', '3.0', '3.5', '4.0', '4.5', '5.0']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# from transformers import XLNetTokenizer, XLNetModel\n",
        "# https://medium.com/@yingbiao/text-classification-with-xlnet-in-action-869029246f7e\n",
        "#  http://restanalytics.com/2021-05-04-Fine-Tuning-XLNet-For-Sequence-Classification/\n",
        "from transformers import XLNetTokenizer, XLNetForMultipleChoice, XLNetForSequenceClassification\n",
        "import torch\n",
        "\n",
        "xlnet_model_name = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(xlnet_model_name, do_lower_case=True)\n",
        "\n",
        "print(len(target_names_list))\n",
        "print(target_names_list)\n",
        "# model = XLNetForMultipleChoice.from_pretrained(xlnet_model_name, num_labels=len(target_names_list))\n",
        "model = XLNetForSequenceClassification.from_pretrained(xlnet_model_name, num_labels=len(target_names_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW\n",
        "from torch import nn\n",
        "class XLNetModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, dropout=0.1):\n",
        "        \n",
        "        super(XLNetModel, self).__init__()\n",
        "        self.xlnet =  AutoModel.from_pretrained('xlnet-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout,0)\n",
        "        self.linear = nn.Linear(768,256)\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "        self.linear_2 = nn.Linear(256, 128)\n",
        "        self.out = nn.Linear(128,1)\n",
        "        \n",
        "        \n",
        "    def forward(self,input_id,mask):\n",
        "        x = self.xlnet(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
        "        # print(x[0].shape)\n",
        "\n",
        "        x = self.dropout(x[0])\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.relu(x)\n",
        "        final_layer = self.out(x)\n",
        "        # print(final_layer.shape)\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "UPdfyx5VDzAN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLNetModel()\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj9GpHZDEUFU",
        "outputId": "44a9236e-92e4-4613-f0fc-9deb361ae3cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dm9UuL09Mag4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1da950-8aee-4dca-a38e-79d9de1633ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160,)\n",
            "[   17  6684  3742    17  2554  8037   672  1143  2081   287 17949  3239\n",
            "   178   132   588   426  3617   451    23  3079    93   179   359   206\n",
            "    92   730  8686   451    23  3079    93   730   634 17949  5465  1125\n",
            " 23415   451    23  3079    56   911   731  1826  3424  8686  1667  6274\n",
            "    25 18135   235  1613  1872   235  1129  3697 27273  9160   451    23\n",
            "  3079    56   104   451    23  3079    93  8686  1667  6274   426   239\n",
            "  3110  1166   352  1129  2598   115 10883   530  5953   150   371  6274\n",
            "   401   185  1795 10871   121  1772 15609   426 24255   952   945  1433\n",
            "   332  1251  1104   507   507  1337 16445  8686    56  2986  1939   185\n",
            "  1075  4549   928   494    65   185    23   252 17408  4549   793   185\n",
            "   494  1337   176  1000   977   451    23  3079    93  1667  6274   293\n",
            "  1194   331 20601   459   235  3704   197   178   588    17   775   129\n",
            "    87  2904   123 19764   913  2177   199 17949   426    17  9462  3079\n",
            "    56    17  2996 11381]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# try https://mccormickml.com/2019/09/19/XLNet-fine-tuning/\n",
        "from keras.utils import pad_sequences\n",
        "from nltk import tokenize\n",
        "xlnet_train_list = []\n",
        "xlnet_train_labels = []\n",
        "attention_masks_train = []\n",
        "num_embeddings = 160\n",
        "SEP = \"[SEP]\"\n",
        "CLS = \"[CLS]\"\n",
        "\n",
        "\n",
        "for count, txt in enumerate(train['full_text']):\n",
        "  \n",
        "  sentences = tokenize.sent_tokenize(txt)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    if i == len(sentences):\n",
        "      sentences[i] = sentence + SEP + \" \" + CLS\n",
        "    else:\n",
        "      sentences[i] = sentence + SEP\n",
        "  \n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "  # if count == 0:\n",
        "  #   print (\"Tokenize the first sentence:\")\n",
        "  #   print (tokenized_texts[0])\n",
        "  \n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\").squeeze()\n",
        "\n",
        "  # tokens = tokenizer.encode(txt, max_length=512, add_special_tokens=False)\n",
        "  # tokens = tokenizer.tokenize(txt)\n",
        "  # input_ids = [tokenizer.convert_tokens_to_ids(tokens)[:num_embeddings-2]]\n",
        "  # input_ids = [tokenizer.build_inputs_with_special_tokens(input_ids)]\n",
        "  # input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  attention_masks = []\n",
        "  # token_lens.append(len(tokens))\n",
        "  for seq in input_ids:\n",
        "    # seq_mask = [float(s>0) for s in seq]\n",
        "    attention_masks.append(float(seq>0))\n",
        "  \n",
        "  if count == 0:\n",
        "  #   print (\"Tokenize the first sentence:\")\n",
        "    print(input_ids.shape)\n",
        "    print(input_ids)\n",
        "    print(attention_masks)\n",
        "  \n",
        "  xlnet_train_list.append(input_ids)\n",
        "  xlnet_train_labels.append(float(train_labels[count] -2))\n",
        "  attention_masks_train.append((attention_masks))\n",
        "\n",
        "# print(xlnet_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dUOwk908Mag7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ef97f7-5873-45e0-f57c-4f791e9a8663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160,)\n",
            "[  781  3188   195   781 10202   300   466  1230  1190  7083  1705    92\n",
            "   467  1720   781  1304  1190  6439   781   673   679   466  2065 10202\n",
            "  6439   781   466   132   434  4418  1184   332 12247   332   770  1785\n",
            "  2032  4113   781  1304 10202   634   312   875   466  6439   781   319\n",
            "   679   312   875   466  6439   781   673   679  1190   175  1037  3309\n",
            "   978  1244   781   466  3894 10202   466   551  1627  1949  4642    23\n",
            "  1924   790   623   781  3097   312   875   466  6439   781   673   679\n",
            "   466   175  1037  3309   978  1244   781  1190   589  1437   781   426\n",
            "   312   875  1078   310  2550  3219  3659  1872   144   955 27927  2589\n",
            "  2737  1667   343   955   466  1250  3309   427   175   214  2383  1194\n",
            "  4277  1080  3431  1190   132  1578   133   383   920  4954  5940   790\n",
            "    41    60  8566  1767  1213  1556   920  1058   920  5476  3659   312\n",
            "  2141  1190    74   312   994  1190  6439   781   673   679  3272   175\n",
            "  5940   790  1244   781]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[4, 5, 4, 4, 4, 4, 4, 3, 6, 3, 4, 5, 4, 3, 6, 2, 5, 2, 5, 3, 4, 4, 5, 4, 4, 4, 3, 3, 4, 3, 6, 4, 2, 2, 5, 6, 5, 3, 3, 4, 3, 2, 5, 3, 3, 5, 2, 6, 4, 4, 5, 3, 4, 4, 4, 4, 2, 2, 3, 3, 4, 4, 6, 3, 3, 6, 7, 4, 2, 4, 6, 3, 3, 5, 4, 3, 5, 5, 2, 7, 3, 5, 5, 4, 4, 3, 1, 5, 5, 5, 4, 4, 3, 4, 2, 3, 6, 4, 3, 4, 4, 6, 7, 3, 5, 6, 6, 5, 5, 5, 3, 3, 7, 4, 5, 5, 2, 6, 6, 3, 4, 3, 3, 5, 6, 5, 4, 4, 3, 6, 0, 6, 4, 4, 3, 5, 5, 5, 4, 3, 6, 4, 4, 5, 4, 6, 4, 4, 4, 4, 3, 4, 5, 4, 6, 4, 4, 4, 4, 6, 3, 2, 5, 3, 4, 5, 3, 4, 5, 4, 4, 4, 5, 3, 3, 4, 2, 3, 7, 2, 2, 2, 5, 4, 4, 2, 6, 3, 5, 2, 4, 4, 3, 4, 3, 6, 4, 3, 3, 5, 3, 3, 3, 4, 5, 5, 2, 4, 3, 4, 3, 5, 4, 5, 6, 3, 3, 5, 5, 2, 4, 7, 7, 5, 4, 4, 4, 3, 6, 0, 4, 2, 5, 6, 2, 3, 5, 3, 4, 5, 4, 4, 3, 4, 2, 2, 5, 4, 4, 6, 6, 5, 2, 3, 7, 5, 5, 4, 4, 5, 4, 3, 2, 4, 5, 5, 3, 5, 3, 5, 3, 5, 3, 3, 4, 3, 4, 3, 6, 5, 2, 5, 4, 4, 3, 5, 2, 6, 3, 4, 6, 5, 5, 5, 6, 6, 3, 4, 3, 3, 5, 1, 7, 4, 4, 4, 2, 5, 4, 5, 4, 7, 3, 3, 5, 5, 5, 4, 3, 2, 3, 4, 4, 3, 5, 4, 5, 2, 3, 4, 5, 4, 5, 3, 5, 2, 6, 5, 4, 4, 4, 4, 4, 3, 6, 3, 6, 1, 6, 6, 4, 6, 5, 5, 5, 6, 2, 2, 4, 7, 6, 4, 5, 3, 3, 5, 4, 3, 4, 4, 3, 3, 6, 6, 3, 3, 5, 4, 5, 2, 3, 4, 3, 2, 4, 3, 4, 6, 5, 5, 7, 3, 2, 3, 4, 4, 5, 5, 5, 4, 3, 4, 2, 3, 5, 3, 7, 2, 3, 6, 4, 5, 2, 4, 3, 5, 5, 6, 4, 2, 3, 5, 2, 4, 4, 4, 3, 4, 3, 5, 5, 5, 5, 4, 2, 5, 6, 5, 4, 4, 4, 3, 5, 6, 4, 5, 2, 6, 4, 6, 3, 2, 3, 4, 4, 3, 2, 2, 4, 4, 3, 4, 2, 5, 2, 3, 4, 3, 3, 5, 5, 4, 2, 5, 4, 5, 4, 4, 5, 5, 3, 7, 7, 5, 3, 4, 4, 3, 2, 2, 4, 8, 4, 4, 2, 3, 3, 4, 4, 3, 3, 6, 4, 6, 7, 7, 3, 5, 5, 5, 3, 5, 4, 4, 5, 5, 6, 4, 5, 4, 3, 7, 5, 7, 4, 3, 5, 2, 6, 4, 4, 3, 5, 3, 6, 4, 4, 5, 4, 4, 2, 4, 4, 4, 4, 6, 4, 6, 5, 5, 6, 5, 5, 2, 5, 4, 4, 4, 3, 5, 4, 5, 6, 3, 4, 3, 4, 6, 4, 5, 5, 4, 5, 5, 4, 5, 4, 4, 6, 3, 3, 3, 4, 4, 3, 4, 4, 5, 4, 6, 2, 4, 5, 4, 4, 4, 4, 4, 4, 3, 2, 4, 4, 3, 6, 6, 5, 3, 3, 3, 6, 5, 1, 4, 4, 5, 1, 4, 3, 5, 3, 4, 6, 4, 4, 3, 4, 5, 4, 4, 3, 5, 3, 5, 4, 2, 4, 4, 4, 3, 5, 3, 3, 5, 4, 5, 5, 4, 2, 2, 3, 2, 4, 4, 2, 4, 3, 2, 5, 5, 5, 2, 4, 5, 6, 3, 2, 6, 5, 3, 4, 4, 3, 6, 5, 4, 3, 4, 2, 5, 4, 2, 7, 5, 6, 8, 4, 3, 4, 5, 6, 3, 3, 5, 3, 5, 4, 4, 4, 4, 6, 3, 3, 2, 5, 3, 5, 3, 4, 4, 4, 4, 5, 4, 4, 5, 6, 5, 3, 5, 4, 5, 5, 4, 4, 4, 3, 3, 3, 3, 6, 4, 4, 4, 2, 4, 2, 4, 4, 5, 2, 3, 2, 5, 2, 6, 5, 6, 5, 2, 3, 2, 3, 2, 2, 8, 5, 3, 4, 4, 3, 4, 4, 2, 4, 3, 6, 1, 2, 5, 6, 4, 5, 2, 5, 4, 4, 4, 5, 3, 5, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "# test on test set and get accuracy\n",
        "xlnet_test_list = []\n",
        "xlnet_test_labels = []\n",
        "attention_masks_test = []\n",
        "\n",
        "for count, txt in enumerate(test['full_text']):\n",
        "  \n",
        "  sentences = tokenize.sent_tokenize(txt)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    if i == len(sentences):\n",
        "      sentences[i] = sentence + SEP + \" \" + CLS\n",
        "    else:\n",
        "      sentences[i] = sentence + SEP\n",
        "  \n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "  \n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\").squeeze()\n",
        "\n",
        "  attention_masks = []\n",
        "  # token_lens.append(len(tokens))\n",
        "  for seq in input_ids:\n",
        "    # seq_mask = [float(s>0) for s in seq]\n",
        "    attention_masks.append(float(seq>0))\n",
        "\n",
        "  if count == 0:\n",
        "    print(input_ids.shape)\n",
        "    print(input_ids)\n",
        "    print(attention_masks)\n",
        "\n",
        "  xlnet_test_list.append(input_ids)\n",
        "  xlnet_test_labels.append(test_labels[count]-2)\n",
        "  attention_masks_test.append(attention_masks)\n",
        "print(xlnet_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "k921RQr5Mag4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f293bf7-6c4d-4ed5-8cc2-f81c782b47dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3128\n",
            "3128\n",
            "[5.0, 5.0, 3.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 6.0, 4.0, 2.0, 4.0, 3.0, 4.0, 4.0, 3.0, 6.0, 3.0, 4.0, 4.0, 5.0, 4.0, 3.0, 6.0, 3.0, 3.0, 5.0, 2.0, 3.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 5.0, 3.0, 4.0, 5.0, 2.0, 5.0, 3.0, 5.0, 6.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 2.0, 5.0, 4.0, 5.0, 5.0, 5.0, 6.0, 5.0, 4.0, 4.0, 7.0, 6.0, 5.0, 5.0, 6.0, 3.0, 5.0, 4.0, 3.0, 3.0, 4.0, 6.0, 5.0, 5.0, 4.0, 4.0, 5.0, 7.0, 6.0, 5.0, 5.0, 4.0, 5.0, 5.0, 2.0, 4.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 3.0, 3.0, 4.0, 2.0, 7.0, 4.0, 4.0, 5.0, 4.0, 3.0, 6.0, 3.0, 5.0, 2.0, 2.0, 6.0, 5.0, 4.0, 4.0, 5.0, 3.0, 5.0, 4.0, 4.0, 3.0, 5.0, 5.0, 4.0, 5.0, 2.0, 4.0, 4.0, 4.0, 5.0, 6.0, 2.0, 2.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 5.0, 5.0, 4.0, 5.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 3.0, 3.0, 5.0, 4.0, 5.0, 2.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 6.0, 4.0, 6.0, 3.0, 3.0, 5.0, 5.0, 4.0, 4.0, 6.0, 4.0, 5.0, 5.0, 6.0, 5.0, 4.0, 7.0, 4.0, 6.0, 4.0, 4.0, 5.0, 4.0, 4.0, 6.0, 3.0, 4.0, 2.0, 3.0, 3.0, 5.0, 4.0, 5.0, 2.0, 5.0, 4.0, 2.0, 5.0, 5.0, 5.0, 7.0, 3.0, 3.0, 7.0, 4.0, 2.0, 5.0, 4.0, 3.0, 4.0, 2.0, 2.0, 5.0, 5.0, 4.0, 2.0, 3.0, 3.0, 5.0, 4.0, 5.0, 6.0, 5.0, 4.0, 5.0, 3.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 4.0, 7.0, 3.0, 3.0, 6.0, 4.0, 4.0, 3.0, 5.0, 3.0, 5.0, 5.0, 6.0, 2.0, 5.0, 5.0, 4.0, 3.0, 4.0, 7.0, 5.0, 4.0, 5.0, 4.0, 3.0, 3.0, 4.0, 2.0, 2.0, 1.0, 4.0, 2.0, 6.0, 1.0, 6.0, 5.0, 4.0, 1.0, 4.0, 4.0, 2.0, 5.0, 4.0, 2.0, 2.0, 4.0, 2.0, 4.0, 4.0, 6.0, 5.0, 4.0, 3.0, 3.0, 4.0, 4.0, 2.0, 3.0, 3.0, 4.0, 5.0, 4.0, 5.0, 5.0, 6.0, 2.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 6.0, 4.0, 4.0, 5.0, 5.0, 3.0, 4.0, 5.0, 6.0, 2.0, 5.0, 5.0, 5.0, 6.0, 5.0, 6.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 4.0, 5.0, 6.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 4.0, 2.0, 3.0, 4.0, 3.0, 4.0, 2.0, 2.0, 4.0, 4.0, 0.0, 4.0, 4.0, 5.0, 5.0, 4.0, 4.0, 3.0, 5.0, 2.0, 3.0, 4.0, 4.0, 5.0, 5.0, 2.0, 3.0, 7.0, 4.0, 4.0, 4.0, 3.0, 3.0, 2.0, 4.0, 6.0, 4.0, 3.0, 3.0, 6.0, 4.0, 5.0, 6.0, 6.0, 3.0, 6.0, 3.0, 4.0, 5.0, 5.0, 3.0, 4.0, 5.0, 2.0, 3.0, 2.0, 5.0, 3.0, 6.0, 5.0, 3.0, 4.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 3.0, 3.0, 3.0, 4.0, 2.0, 5.0, 3.0, 7.0, 5.0, 4.0, 4.0, 4.0, 2.0, 1.0, 6.0, 4.0, 5.0, 3.0, 3.0, 6.0, 3.0, 3.0, 4.0, 2.0, 2.0, 6.0, 4.0, 4.0, 4.0, 7.0, 5.0, 3.0, 4.0, 2.0, 4.0, 2.0, 6.0, 3.0, 5.0, 3.0, 6.0, 4.0, 4.0, 2.0, 7.0, 3.0, 2.0, 4.0, 4.0, 3.0, 5.0, 3.0, 6.0, 2.0, 3.0, 3.0, 4.0, 4.0, 4.0, 5.0, 6.0, 3.0, 3.0, 5.0, 5.0, 3.0, 2.0, 4.0, 4.0, 7.0, 4.0, 5.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 6.0, 4.0, 4.0, 6.0, 2.0, 4.0, 3.0, 5.0, 4.0, 7.0, 4.0, 5.0, 5.0, 4.0, 5.0, 2.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, 7.0, 4.0, 3.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 6.0, 4.0, 2.0, 4.0, 4.0, 5.0, 6.0, 4.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 5.0, 4.0, 5.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 6.0, 5.0, 6.0, 4.0, 4.0, 5.0, 3.0, 4.0, 2.0, 5.0, 3.0, 1.0, 4.0, 5.0, 4.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 6.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 1.0, 5.0, 2.0, 3.0, 4.0, 6.0, 4.0, 3.0, 4.0, 4.0, 8.0, 5.0, 0.0, 4.0, 2.0, 6.0, 6.0, 4.0, 5.0, 4.0, 5.0, 6.0, 4.0, 3.0, 4.0, 3.0, 3.0, 5.0, 6.0, 3.0, 6.0, 5.0, 7.0, 4.0, 5.0, 2.0, 2.0, 5.0, 4.0, 6.0, 4.0, 2.0, 3.0, 4.0, 3.0, 2.0, 5.0, 2.0, 2.0, 5.0, 3.0, 7.0, 4.0, 5.0, 3.0, 3.0, 6.0, 4.0, 5.0, 5.0, 4.0, 3.0, 3.0, 2.0, 5.0, 2.0, 4.0, 3.0, 6.0, 4.0, 2.0, 5.0, 3.0, 3.0, 4.0, 3.0, 4.0, 5.0, 2.0, 4.0, 2.0, 6.0, 4.0, 4.0, 4.0, 2.0, 4.0, 6.0, 4.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 4.0, 2.0, 4.0, 3.0, 7.0, 6.0, 3.0, 4.0, 8.0, 7.0, 3.0, 1.0, 2.0, 5.0, 6.0, 4.0, 3.0, 4.0, 3.0, 4.0, 2.0, 2.0, 5.0, 4.0, 5.0, 2.0, 4.0, 3.0, 4.0, 2.0, 4.0, 5.0, 5.0, 6.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 5.0, 4.0, 4.0, 6.0, 4.0, 3.0, 4.0, 4.0, 5.0, 4.0, 2.0, 4.0, 6.0, 4.0, 5.0, 3.0, 6.0, 6.0, 3.0, 5.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 2.0, 4.0, 7.0, 6.0, 4.0, 5.0, 2.0, 3.0, 3.0, 5.0, 5.0, 4.0, 5.0, 2.0, 2.0, 3.0, 4.0, 2.0, 2.0, 5.0, 3.0, 3.0, 3.0, 5.0, 4.0, 4.0, 4.0, 6.0, 4.0, 3.0, 5.0, 4.0, 3.0, 7.0, 4.0, 4.0, 6.0, 3.0, 4.0, 5.0, 2.0, 3.0, 6.0, 5.0, 5.0, 7.0, 4.0, 3.0, 5.0, 4.0, 3.0, 2.0, 3.0, 6.0, 5.0, 4.0, 4.0, 6.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 6.0, 6.0, 4.0, 2.0, 4.0, 5.0, 6.0, 5.0, 4.0, 6.0, 5.0, 6.0, 5.0, 6.0, 5.0, 6.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 6.0, 4.0, 3.0, 4.0, 6.0, 4.0, 5.0, 5.0, 3.0, 4.0, 4.0, 4.0, 7.0, 1.0, 2.0, 5.0, 6.0, 4.0, 4.0, 5.0, 6.0, 2.0, 4.0, 4.0, 5.0, 3.0, 4.0, 4.0, 7.0, 3.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 2.0, 4.0, 2.0, 2.0, 2.0, 6.0, 2.0, 4.0, 6.0, 3.0, 0.0, 2.0, 2.0, 5.0, 5.0, 6.0, 2.0, 4.0, 4.0, 5.0, 3.0, 6.0, 3.0, 3.0, 2.0, 3.0, 4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 3.0, 3.0, 3.0, 2.0, 2.0, 5.0, 4.0, 3.0, 3.0, 5.0, 3.0, 6.0, 4.0, 3.0, 6.0, 5.0, 3.0, 4.0, 6.0, 4.0, 5.0, 6.0, 5.0, 4.0, 4.0, 4.0, 4.0, 2.0, 5.0, 2.0, 6.0, 5.0, 4.0, 4.0, 3.0, 4.0, 5.0, 2.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 6.0, 7.0, 5.0, 4.0, 3.0, 6.0, 3.0, 4.0, 5.0, 3.0, 6.0, 4.0, 5.0, 5.0, 6.0, 4.0, 3.0, 5.0, 3.0, 2.0, 5.0, 5.0, 4.0, 5.0, 3.0, 7.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 5.0, 4.0, 2.0, 4.0, 3.0, 2.0, 2.0, 4.0, 2.0, 4.0, 2.0, 4.0, 4.0, 6.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 2.0, 4.0, 4.0, 5.0, 4.0, 5.0, 7.0, 3.0, 6.0, 3.0, 4.0, 2.0, 4.0, 4.0, 4.0, 5.0, 4.0, 6.0, 2.0, 5.0, 6.0, 5.0, 6.0, 3.0, 4.0, 7.0, 5.0, 5.0, 3.0, 3.0, 4.0, 2.0, 6.0, 6.0, 6.0, 2.0, 6.0, 4.0, 4.0, 7.0, 4.0, 4.0, 5.0, 3.0, 3.0, 2.0, 5.0, 4.0, 5.0, 5.0, 4.0, 5.0, 4.0, 5.0, 5.0, 1.0, 4.0, 5.0, 5.0, 4.0, 4.0, 3.0, 2.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 4.0, 4.0, 5.0, 3.0, 5.0, 4.0, 6.0, 4.0, 3.0, 5.0, 3.0, 3.0, 3.0, 4.0, 4.0, 7.0, 2.0, 2.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 6.0, 4.0, 4.0, 4.0, 8.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 6.0, 4.0, 5.0, 4.0, 5.0, 4.0, 2.0, 3.0, 4.0, 4.0, 5.0, 4.0, 2.0, 3.0, 5.0, 5.0, 3.0, 2.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 6.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 3.0, 4.0, 6.0, 4.0, 5.0, 5.0, 2.0, 4.0, 4.0, 3.0, 2.0, 5.0, 4.0, 3.0, 8.0, 7.0, 2.0, 4.0, 4.0, 5.0, 3.0, 4.0, 6.0, 4.0, 3.0, 6.0, 5.0, 4.0, 3.0, 5.0, 6.0, 3.0, 2.0, 4.0, 4.0, 4.0, 3.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 7.0, 3.0, 4.0, 6.0, 5.0, 3.0, 6.0, 3.0, 4.0, 4.0, 3.0, 6.0, 4.0, 4.0, 3.0, 4.0, 3.0, 5.0, 2.0, 7.0, 5.0, 5.0, 3.0, 5.0, 3.0, 3.0, 5.0, 2.0, 2.0, 2.0, 5.0, 3.0, 2.0, 5.0, 5.0, 3.0, 3.0, 6.0, 5.0, 3.0, 4.0, 6.0, 4.0, 2.0, 4.0, 5.0, 3.0, 4.0, 4.0, 3.0, 6.0, 5.0, 2.0, 2.0, 4.0, 6.0, 3.0, 3.0, 5.0, 5.0, 3.0, 2.0, 4.0, 5.0, 4.0, 4.0, 6.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 5.0, 4.0, 6.0, 4.0, 3.0, 6.0, 5.0, 3.0, 2.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 5.0, 4.0, 5.0, 8.0, 5.0, 4.0, 2.0, 4.0, 3.0, 5.0, 6.0, 5.0, 2.0, 7.0, 4.0, 3.0, 4.0, 2.0, 3.0, 2.0, 3.0, 5.0, 4.0, 4.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 3.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 4.0, 5.0, 5.0, 3.0, 6.0, 6.0, 1.0, 4.0, 7.0, 5.0, 6.0, 4.0, 5.0, 5.0, 4.0, 4.0, 3.0, 2.0, 4.0, 2.0, 6.0, 4.0, 3.0, 3.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 6.0, 2.0, 4.0, 5.0, 6.0, 5.0, 6.0, 4.0, 5.0, 3.0, 2.0, 6.0, 4.0, 3.0, 8.0, 2.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 6.0, 4.0, 2.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 4.0, 3.0, 2.0, 2.0, 4.0, 4.0, 2.0, 4.0, 5.0, 5.0, 3.0, 6.0, 6.0, 2.0, 2.0, 6.0, 3.0, 6.0, 2.0, 6.0, 4.0, 5.0, 3.0, 5.0, 6.0, 4.0, 2.0, 3.0, 3.0, 3.0, 5.0, 3.0, 3.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 4.0, 6.0, 3.0, 5.0, 5.0, 3.0, 2.0, 3.0, 3.0, 6.0, 4.0, 4.0, 4.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 2.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 1.0, 4.0, 4.0, 3.0, 5.0, 3.0, 4.0, 6.0, 5.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 3.0, 7.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 6.0, 5.0, 3.0, 6.0, 4.0, 4.0, 4.0, 5.0, 4.0, 5.0, 4.0, 6.0, 6.0, 2.0, 4.0, 4.0, 4.0, 4.0, 4.0, 7.0, 2.0, 7.0, 5.0, 4.0, 4.0, 2.0, 6.0, 2.0, 4.0, 6.0, 4.0, 5.0, 5.0, 6.0, 3.0, 2.0, 2.0, 4.0, 4.0, 2.0, 4.0, 5.0, 2.0, 5.0, 6.0, 3.0, 4.0, 3.0, 3.0, 4.0, 6.0, 4.0, 5.0, 6.0, 4.0, 3.0, 3.0, 5.0, 4.0, 4.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 6.0, 2.0, 4.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 6.0, 4.0, 5.0, 3.0, 2.0, 2.0, 4.0, 6.0, 2.0, 3.0, 3.0, 4.0, 6.0, 5.0, 2.0, 5.0, 4.0, 7.0, 5.0, 2.0, 5.0, 5.0, 3.0, 7.0, 4.0, 6.0, 5.0, 2.0, 4.0, 2.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 6.0, 5.0, 2.0, 3.0, 5.0, 7.0, 2.0, 1.0, 4.0, 4.0, 2.0, 3.0, 5.0, 4.0, 4.0, 3.0, 6.0, 5.0, 6.0, 5.0, 4.0, 4.0, 4.0, 4.0, 2.0, 4.0, 4.0, 4.0, 4.0, 5.0, 4.0, 2.0, 3.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 3.0, 4.0, 3.0, 3.0, 4.0, 2.0, 5.0, 5.0, 4.0, 7.0, 4.0, 1.0, 3.0, 5.0, 4.0, 4.0, 6.0, 6.0, 3.0, 4.0, 5.0, 3.0, 4.0, 2.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0, 5.0, 4.0, 4.0, 2.0, 4.0, 3.0, 6.0, 3.0, 5.0, 5.0, 4.0, 3.0, 5.0, 6.0, 3.0, 3.0, 6.0, 3.0, 5.0, 4.0, 3.0, 5.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 5.0, 3.0, 2.0, 6.0, 5.0, 4.0, 4.0, 2.0, 4.0, 5.0, 4.0, 2.0, 7.0, 3.0, 4.0, 5.0, 4.0, 3.0, 2.0, 5.0, 3.0, 3.0, 4.0, 2.0, 6.0, 5.0, 3.0, 3.0, 5.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 4.0, 6.0, 7.0, 3.0, 4.0, 4.0, 4.0, 4.0, 6.0, 6.0, 3.0, 3.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 6.0, 7.0, 4.0, 4.0, 5.0, 4.0, 6.0, 6.0, 3.0, 4.0, 5.0, 3.0, 4.0, 4.0, 4.0, 5.0, 2.0, 3.0, 3.0, 5.0, 6.0, 4.0, 5.0, 3.0, 7.0, 5.0, 5.0, 4.0, 6.0, 3.0, 4.0, 3.0, 3.0, 5.0, 4.0, 4.0, 6.0, 4.0, 3.0, 5.0, 5.0, 3.0, 4.0, 5.0, 4.0, 4.0, 5.0, 2.0, 4.0, 2.0, 6.0, 3.0, 5.0, 7.0, 3.0, 7.0, 3.0, 6.0, 4.0, 6.0, 5.0, 6.0, 5.0, 5.0, 2.0, 5.0, 5.0, 5.0, 6.0, 2.0, 5.0, 5.0, 5.0, 6.0, 4.0, 4.0, 4.0, 4.0, 5.0, 6.0, 4.0, 2.0, 4.0, 2.0, 3.0, 3.0, 2.0, 2.0, 6.0, 2.0, 7.0, 4.0, 3.0, 4.0, 0.0, 2.0, 4.0, 2.0, 4.0, 5.0, 6.0, 7.0, 6.0, 4.0, 6.0, 5.0, 5.0, 2.0, 5.0, 3.0, 4.0, 4.0, 3.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 2.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 2.0, 3.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 5.0, 5.0, 2.0, 6.0, 5.0, 3.0, 2.0, 4.0, 2.0, 5.0, 4.0, 5.0, 5.0, 6.0, 5.0, 6.0, 3.0, 5.0, 6.0, 4.0, 4.0, 6.0, 5.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 6.0, 4.0, 4.0, 4.0, 1.0, 3.0, 5.0, 4.0, 7.0, 6.0, 3.0, 3.0, 4.0, 5.0, 2.0, 3.0, 3.0, 5.0, 5.0, 3.0, 4.0, 2.0, 5.0, 2.0, 5.0, 4.0, 4.0, 2.0, 4.0, 5.0, 3.0, 4.0, 7.0, 5.0, 3.0, 4.0, 2.0, 4.0, 5.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 2.0, 6.0, 3.0, 3.0, 4.0, 5.0, 6.0, 4.0, 5.0, 5.0, 2.0, 4.0, 7.0, 2.0, 5.0, 6.0, 5.0, 4.0, 4.0, 5.0, 4.0, 4.0, 7.0, 7.0, 5.0, 4.0, 2.0, 4.0, 4.0, 5.0, 3.0, 2.0, 4.0, 4.0, 3.0, 3.0, 4.0, 5.0, 5.0, 5.0, 4.0, 4.0, 3.0, 7.0, 2.0, 5.0, 8.0, 7.0, 1.0, 8.0, 3.0, 4.0, 4.0, 6.0, 4.0, 3.0, 8.0, 3.0, 4.0, 4.0, 5.0, 3.0, 3.0, 2.0, 4.0, 2.0, 6.0, 3.0, 3.0, 2.0, 4.0, 4.0, 2.0, 4.0, 5.0, 5.0, 3.0, 6.0, 3.0, 1.0, 3.0, 3.0, 6.0, 3.0, 3.0, 3.0, 4.0, 1.0, 5.0, 4.0, 5.0, 5.0, 5.0, 4.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 3.0, 2.0, 4.0, 2.0, 3.0, 5.0, 7.0, 5.0, 4.0, 2.0, 5.0, 2.0, 2.0, 3.0, 2.0, 5.0, 3.0, 3.0, 3.0, 3.0, 4.0, 6.0, 6.0, 4.0, 5.0, 4.0, 3.0, 6.0, 4.0, 3.0, 4.0, 2.0, 5.0, 5.0, 2.0, 2.0, 5.0, 4.0, 4.0, 5.0, 4.0, 2.0, 3.0, 4.0, 4.0, 3.0, 3.0, 5.0, 6.0, 4.0, 4.0, 2.0, 3.0, 5.0, 2.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 6.0, 6.0, 4.0, 4.0, 5.0, 8.0, 5.0, 7.0, 3.0, 4.0, 2.0, 3.0, 5.0, 6.0, 3.0, 4.0, 4.0, 4.0, 3.0, 1.0, 4.0, 3.0, 3.0, 2.0, 4.0, 4.0, 2.0, 3.0, 4.0, 3.0, 2.0, 5.0, 3.0, 3.0, 6.0, 3.0, 6.0, 5.0, 5.0, 3.0, 3.0, 6.0, 1.0, 3.0, 5.0, 2.0, 7.0, 5.0, 3.0, 6.0, 4.0, 3.0, 3.0, 7.0, 4.0, 4.0, 4.0, 3.0, 3.0, 5.0, 4.0, 3.0, 5.0, 5.0, 4.0, 6.0, 4.0, 2.0, 3.0, 4.0, 3.0, 6.0, 7.0, 3.0, 3.0, 3.0, 5.0, 3.0, 4.0, 4.0, 5.0, 6.0, 3.0, 4.0, 6.0, 4.0, 2.0, 6.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 1.0, 4.0, 3.0, 3.0, 6.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 5.0, 5.0, 3.0, 3.0, 5.0, 3.0, 3.0, 2.0, 4.0, 4.0, 4.0, 6.0, 4.0, 3.0, 6.0, 7.0, 4.0, 4.0, 4.0, 3.0, 4.0, 2.0, 5.0, 4.0, 4.0, 3.0, 3.0, 2.0, 2.0, 2.0, 5.0, 4.0, 5.0, 4.0, 6.0, 3.0, 6.0, 4.0, 5.0, 4.0, 4.0, 6.0, 4.0, 3.0, 5.0, 6.0, 2.0, 3.0, 3.0, 3.0, 4.0, 5.0, 6.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 3.0, 4.0, 6.0, 3.0, 6.0, 5.0, 4.0, 2.0, 3.0, 5.0, 4.0, 7.0, 5.0, 6.0, 2.0, 4.0, 4.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 2.0, 3.0, 3.0, 5.0, 5.0, 2.0, 5.0, 6.0, 3.0, 3.0, 3.0, 6.0, 5.0, 5.0, 4.0, 4.0, 7.0, 3.0, 6.0, 3.0, 5.0, 3.0, 3.0, 5.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 3.0, 4.0, 2.0, 4.0, 5.0, 5.0, 3.0, 5.0, 4.0, 6.0, 4.0, 4.0, 6.0, 4.0, 3.0, 5.0, 4.0, 3.0, 3.0, 1.0, 5.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 6.0, 3.0, 3.0, 4.0, 4.0, 4.0, 5.0, 4.0, 6.0, 5.0, 3.0, 2.0, 3.0, 4.0, 5.0, 4.0, 5.0, 5.0, 5.0, 3.0, 5.0, 5.0, 5.0, 2.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 3.0, 5.0, 4.0, 3.0, 3.0, 3.0, 4.0, 4.0, 5.0, 6.0, 3.0, 6.0, 6.0, 2.0, 3.0, 5.0, 3.0, 5.0, 5.0, 4.0, 4.0, 3.0, 4.0, 3.0, 2.0, 5.0, 4.0, 6.0, 4.0, 6.0, 0.0, 7.0, 7.0, 6.0, 3.0, 3.0, 4.0, 2.0, 3.0, 3.0, 5.0, 5.0, 5.0, 4.0, 3.0, 2.0, 5.0, 4.0, 7.0, 4.0, 4.0, 2.0, 4.0, 5.0, 3.0, 3.0, 5.0, 5.0, 3.0, 4.0, 4.0, 3.0, 5.0, 3.0, 5.0, 7.0, 2.0, 3.0, 3.0, 4.0, 4.0, 4.0, 2.0, 5.0, 2.0, 4.0, 3.0, 5.0, 6.0, 2.0, 3.0, 2.0, 4.0, 6.0, 2.0, 5.0, 3.0, 5.0, 1.0, 2.0, 4.0, 4.0, 4.0, 3.0, 5.0, 5.0, 6.0, 4.0, 3.0, 2.0, 4.0, 5.0, 6.0, 4.0, 2.0, 4.0, 4.0, 6.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 2.0, 6.0, 4.0, 3.0, 3.0, 5.0, 4.0, 2.0, 6.0, 3.0, 5.0, 6.0, 6.0, 4.0, 2.0, 3.0, 3.0, 4.0, 2.0, 5.0, 5.0, 3.0, 4.0, 3.0, 4.0, 4.0, 7.0, 4.0, 5.0, 5.0, 4.0, 5.0, 6.0, 3.0, 3.0, 5.0, 4.0, 4.0, 4.0, 3.0, 2.0, 4.0, 5.0, 3.0, 0.0, 4.0, 5.0, 3.0, 3.0, 4.0, 5.0, 5.0, 3.0, 3.0, 3.0, 5.0, 4.0, 7.0, 3.0, 6.0, 5.0, 3.0, 3.0, 2.0, 2.0, 4.0, 2.0, 5.0, 4.0, 6.0, 2.0, 5.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 5.0, 2.0, 3.0, 5.0, 3.0, 4.0, 5.0, 6.0, 5.0, 3.0, 3.0, 5.0, 3.0, 4.0, 5.0, 5.0, 4.0, 6.0, 5.0, 3.0, 3.0, 4.0, 4.0, 4.0, 5.0, 3.0, 2.0, 5.0, 4.0, 5.0, 6.0, 2.0, 5.0, 3.0, 2.0, 6.0, 4.0, 6.0, 4.0, 3.0, 6.0, 3.0, 5.0, 6.0, 6.0, 7.0, 5.0, 0.0, 5.0, 4.0, 2.0, 6.0, 4.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 2.0, 4.0, 5.0, 2.0, 4.0, 5.0, 4.0, 4.0, 4.0, 4.0, 2.0, 2.0, 3.0, 5.0, 4.0, 5.0, 3.0, 7.0, 4.0, 6.0, 4.0, 3.0, 5.0, 7.0, 4.0, 3.0, 4.0, 2.0, 4.0, 4.0, 4.0, 5.0, 4.0, 3.0, 4.0, 4.0, 2.0, 5.0, 3.0, 3.0, 3.0, 5.0, 5.0, 4.0, 4.0, 2.0, 3.0, 4.0, 6.0, 6.0, 4.0, 7.0, 3.0, 6.0, 6.0, 4.0, 5.0, 3.0, 2.0, 6.0, 5.0, 6.0, 2.0, 5.0, 4.0, 4.0, 6.0, 5.0, 6.0, 6.0, 3.0, 4.0, 4.0, 5.0, 6.0, 3.0, 6.0, 3.0, 3.0, 5.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 2.0, 3.0, 2.0, 5.0, 4.0, 4.0, 3.0, 6.0, 1.0, 3.0, 2.0, 4.0, 5.0, 3.0, 4.0, 6.0, 5.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 6.0, 4.0, 4.0, 6.0, 4.0, 3.0, 7.0, 6.0, 4.0, 5.0, 0.0, 5.0, 6.0, 4.0, 2.0, 5.0, 3.0, 2.0, 4.0, 4.0, 4.0, 5.0, 3.0, 4.0, 5.0, 6.0, 8.0, 4.0, 3.0, 2.0, 5.0, 3.0, 4.0, 5.0, 3.0, 3.0, 5.0, 3.0, 4.0, 4.0, 2.0, 5.0, 3.0, 4.0, 4.0, 4.0, 2.0, 3.0, 4.0, 2.0, 4.0, 5.0, 8.0, 2.0, 4.0, 5.0, 5.0, 6.0, 4.0, 4.0, 2.0, 4.0, 4.0, 4.0, 3.0, 6.0, 5.0, 4.0, 2.0, 4.0, 5.0, 2.0, 4.0, 3.0, 4.0, 3.0, 6.0, 4.0, 5.0, 4.0, 4.0, 3.0, 3.0, 5.0, 2.0, 5.0, 4.0, 6.0, 2.0, 3.0, 3.0, 4.0, 5.0, 6.0, 2.0, 4.0, 6.0, 4.0, 6.0, 5.0, 5.0, 3.0, 2.0, 3.0, 4.0, 2.0, 4.0, 3.0, 4.0, 5.0, 5.0, 4.0, 2.0, 3.0, 3.0, 5.0, 7.0, 6.0, 4.0, 4.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0, 6.0, 8.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 2.0, 3.0, 4.0, 3.0, 3.0, 4.0, 6.0, 5.0, 2.0, 4.0, 4.0, 5.0, 5.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 4.0, 6.0, 3.0, 6.0, 5.0, 4.0, 5.0, 4.0, 6.0, 5.0, 2.0, 4.0, 4.0, 4.0, 3.0, 4.0, 5.0, 7.0, 4.0, 7.0, 2.0, 2.0, 3.0, 2.0, 3.0, 5.0, 4.0, 2.0, 4.0, 3.0, 5.0, 5.0, 4.0, 3.0, 4.0, 5.0, 5.0, 3.0, 6.0, 3.0, 4.0, 5.0, 2.0, 4.0, 4.0, 2.0, 5.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 6.0, 5.0, 2.0, 4.0, 3.0, 4.0, 5.0, 6.0, 4.0, 4.0, 4.0, 6.0, 6.0, 3.0, 3.0, 0.0, 3.0, 8.0, 3.0, 3.0, 3.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 6.0, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 7.0, 4.0, 5.0, 4.0, 4.0, 4.0, 6.0, 4.0, 4.0, 2.0, 6.0, 5.0, 3.0, 2.0, 2.0, 4.0, 4.0, 5.0, 2.0, 3.0]\n",
            "783 783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ],
      "source": [
        "# loop through train_list and run text on tokenizer\n",
        "attention_masks_train = torch.tensor(attention_masks_train)\n",
        "print(len(xlnet_train_list))\n",
        "print(len(xlnet_train_labels))\n",
        "print(xlnet_train_labels)\n",
        "xlnet_tokenized_train = list(zip(xlnet_train_list, xlnet_train_labels, attention_masks_train))\n",
        "\n",
        "attention_masks_test = torch.tensor(attention_masks_test)\n",
        "xlnet_test_labels = torch.tensor(xlnet_test_labels)\n",
        "print(len(xlnet_test_list), len(xlnet_test_labels))\n",
        "xlnet_tokenized_test = list(zip(xlnet_test_list, xlnet_test_labels, attention_masks_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rlIykOJ6Mag6"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "batch_size = 17\n",
        "param_optimizer = list(model.named_parameters())\n",
        "criterion = nn.MSELoss().to(device)\n",
        "\n",
        "train_dataloader = DataLoader(xlnet_tokenized_train, sampler=RandomSampler(xlnet_tokenized_train), batch_size=batch_size)\n",
        "val_dataloader = torch.utils.data.DataLoader(xlnet_tokenized_test, batch_size=batch_size)\n",
        "# for data in train_dataloader:\n",
        "#   print(data)\n",
        "# model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "AkSUuNYOWgCo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OgcXIMicMag6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c20c16-a4d3-4056-a98c-247d9edc0f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 160, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1...... Step: 10/184....... Average Loss for Epoch: 1.8887580394744874,  0.1026498934497004\n",
            "Epoch 1...... Step: 20/184....... Average Loss for Epoch: 1.901164823770523,  0.20664835040983948\n",
            "Epoch 1...... Step: 30/184....... Average Loss for Epoch: 1.906394612789154,  0.3108252086069273\n",
            "Epoch 1...... Step: 40/184....... Average Loss for Epoch: 1.7919228821992874,  0.389548452652019\n",
            "Epoch 1...... Step: 50/184....... Average Loss for Epoch: 1.8108781254291535,  0.49208644712748734\n",
            "Epoch 1...... Step: 60/184....... Average Loss for Epoch: 1.7967339744170507,  0.5858915133968644\n",
            "Epoch 1...... Step: 70/184....... Average Loss for Epoch: 1.7686150644506726,  0.6728426875627559\n",
            "Epoch 1...... Step: 80/184....... Average Loss for Epoch: 1.7958178482949734,  0.7807903688239015\n",
            "Epoch 1...... Step: 90/184....... Average Loss for Epoch: 1.8118633508682251,  0.8862375085768492\n",
            "Epoch 1...... Step: 100/184....... Average Loss for Epoch: 1.810325837135315,  0.9838727375735408\n",
            "Epoch 1...... Step: 110/184....... Average Loss for Epoch: 1.82649305191907,  1.091925194082053\n",
            "Epoch 1...... Step: 120/184....... Average Loss for Epoch: 1.7928886532783508,  1.169275208659794\n",
            "Epoch 1...... Step: 130/184....... Average Loss for Epoch: 1.800621053347221,  1.2721779181257538\n",
            "Epoch 1...... Step: 140/184....... Average Loss for Epoch: 1.7980834009391922,  1.3681069354972113\n",
            "Epoch 1...... Step: 150/184....... Average Loss for Epoch: 1.797207092642784,  1.465114477697922\n",
            "Epoch 1...... Step: 160/184....... Average Loss for Epoch: 1.7869713613763452,  1.5538881403272566\n",
            "Epoch 1...... Step: 170/184....... Average Loss for Epoch: 1.7752008222481783,  1.6401311944684256\n",
            "Epoch 1...... Step: 180/184....... Average Loss for Epoch: 1.7610554713341924,  1.722771656739971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 160, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall for Epoch 1: Average training loss: 1.747471056068721 Average validation loss: 1.6913234704035394\n",
            "  Training epoch took: 0:02:11\n",
            "\n",
            "Epoch 2...... Step: 10/184....... Average Loss for Epoch: 1.972952628135681,  0.1072256863117218\n",
            "Epoch 2...... Step: 20/184....... Average Loss for Epoch: 1.892375648021698,  0.20569300521974979\n",
            "Epoch 2...... Step: 30/184....... Average Loss for Epoch: 1.8164030631383261,  0.29615267333777057\n",
            "Epoch 2...... Step: 40/184....... Average Loss for Epoch: 1.748169380426407,  0.3800368218318276\n",
            "Epoch 2...... Step: 50/184....... Average Loss for Epoch: 1.8083932995796204,  0.49141122271185333\n",
            "Epoch 2...... Step: 60/184....... Average Loss for Epoch: 1.8074173510074616,  0.589375223154607\n",
            "Epoch 2...... Step: 70/184....... Average Loss for Epoch: 1.7654887122767313,  0.6716533144531043\n",
            "Epoch 2...... Step: 80/184....... Average Loss for Epoch: 1.788706538826227,  0.7776984951418379\n",
            "Epoch 2...... Step: 90/184....... Average Loss for Epoch: 1.7972088674704234,  0.8790695547409679\n",
            "Epoch 2...... Step: 100/184....... Average Loss for Epoch: 1.786953358054161,  0.971170303290305\n",
            "Epoch 2...... Step: 110/184....... Average Loss for Epoch: 1.801795841888948,  1.0771605576510015\n",
            "Epoch 2...... Step: 120/184....... Average Loss for Epoch: 1.786100157847007,  1.1648479290306568\n",
            "Epoch 2...... Step: 130/184....... Average Loss for Epoch: 1.7732973522864854,  1.252873129332843\n",
            "Epoch 2...... Step: 140/184....... Average Loss for Epoch: 1.7691493913531304,  1.3460919282034687\n",
            "Epoch 2...... Step: 150/184....... Average Loss for Epoch: 1.7468246283133824,  1.4240418165598228\n",
            "Epoch 2...... Step: 160/184....... Average Loss for Epoch: 1.77027268987149,  1.5393675564099913\n",
            "Epoch 2...... Step: 170/184....... Average Loss for Epoch: 1.76523297096,  1.6309217666478262\n",
            "Epoch 2...... Step: 180/184....... Average Loss for Epoch: 1.744917649196254,  1.7069846568224223\n",
            "\n",
            "Overall for Epoch 2: Average training loss: 1.7298962248732215 Average validation loss: 1.655364102030054\n",
            "  Training epoch took: 0:02:12\n",
            "\n",
            "Epoch 3...... Step: 10/184....... Average Loss for Epoch: 1.9148819625377655,  0.10406967187705247\n",
            "Epoch 3...... Step: 20/184....... Average Loss for Epoch: 1.8189791291952133,  0.19771512273861014\n",
            "Epoch 3...... Step: 30/184....... Average Loss for Epoch: 1.637992078065872,  0.2670639257716096\n",
            "Epoch 3...... Step: 40/184....... Average Loss for Epoch: 1.6905570581555367,  0.3675124039468558\n",
            "Epoch 3...... Step: 50/184....... Average Loss for Epoch: 1.6964758408069611,  0.4609988697845003\n",
            "Epoch 3...... Step: 60/184....... Average Loss for Epoch: 1.7056211620569228,  0.556180813714214\n",
            "Epoch 3...... Step: 70/184....... Average Loss for Epoch: 1.6636758540357863,  0.6329201618614404\n",
            "Epoch 3...... Step: 80/184....... Average Loss for Epoch: 1.6978222846984863,  0.7381836020428202\n",
            "Epoch 3...... Step: 90/184....... Average Loss for Epoch: 1.7075486229525672,  0.8352140003572339\n",
            "Epoch 3...... Step: 100/184....... Average Loss for Epoch: 1.7341571211814881,  0.942476696294287\n",
            "Epoch 3...... Step: 110/184....... Average Loss for Epoch: 1.7094632116231052,  1.0219617026007695\n",
            "Epoch 3...... Step: 120/184....... Average Loss for Epoch: 1.7045782605806987,  1.11168147429176\n",
            "Epoch 3...... Step: 130/184....... Average Loss for Epoch: 1.696936436341359,  1.1989224821976994\n",
            "Epoch 3...... Step: 140/184....... Average Loss for Epoch: 1.703636116215161,  1.2962448710332746\n",
            "Epoch 3...... Step: 150/184....... Average Loss for Epoch: 1.7271140563488006,  1.4079734155017396\n",
            "Epoch 3...... Step: 160/184....... Average Loss for Epoch: 1.7147633384913206,  1.491098555209844\n",
            "Epoch 3...... Step: 170/184....... Average Loss for Epoch: 1.722064218450995,  1.5910375931340714\n",
            "Epoch 3...... Step: 180/184....... Average Loss for Epoch: 1.7201387657059564,  1.6827444447123485\n",
            "\n",
            "Overall for Epoch 3: Average training loss: 1.715435415506363 Average validation loss: 1.6336584578248414\n",
            "  Training epoch took: 0:02:12\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 3\n",
        "# print(len(train_dataloader))\n",
        "for i in range(epochs):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    training_time = format_time(t0)\n",
        "    for idx, (batch, target, mask) in enumerate(train_dataloader):\n",
        "        \n",
        "        input_ids_np = batch.to(device)\n",
        "        # print(input_ids_np.shape)\n",
        "        mask = mask.to(device)\n",
        "        # print(mask.shape)\n",
        "        target = target.to(device)\n",
        "        # print(target.shape)\n",
        "        # input_ids_np = torch.as_tensor(batch).unsqueeze(0)\n",
        "        # mask = torch.tensor(mask, dtype=torch.int)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # outputs = model(input_ids=input_ids_np, attention_mask=mask, labels=target)\n",
        "        output = model(input_ids_np, mask)\n",
        "        # loss = outputs[0]\n",
        "        # loss = criterion(outputs[0], target.float())\n",
        "        loss = criterion(output, target.float())\n",
        "\n",
        "        # print(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        # print(tr_loss, \" is tr_loss\")\n",
        "        # print()\n",
        "        # nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        optimizer.step()\n",
        "        if idx > 0 and (idx%10 == 0 or idx == len(train_dataloader)):\n",
        "            print(\"Epoch {}...... Step: {}/{}....... Average Loss for Epoch: {},  {}\".format(i+1, idx, len(train_dataloader), tr_loss/idx, tr_loss/len(train_dataloader)))\n",
        "    \n",
        "    total_loss_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, (val_input, val_label, val_mask) in enumerate(val_dataloader):\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_mask.to(device)\n",
        "            input_id = val_input.squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            loss = criterion(output, val_label)\n",
        "            total_loss_val += loss.item()\n",
        "\n",
        "            if idx > 0 and (idx%10 == 0 or idx == len(train_dataloader)):\n",
        "              print(\"Epoch {}...... Step: {}/{}....... Average Loss for Epoch: {},  {}\".format(i+1, idx, len(train_dataloader), tr_loss/idx, tr_loss/len(train_dataloader)))\n",
        "    \n",
        "    \n",
        "    epoch_train_loss = tr_loss/len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"Overall for Epoch {}: Average training loss: {} Average validation loss: {}\".format(i+1, epoch_train_loss, total_loss_val / len(val_dataloader)))\n",
        "    print(\"  Training epoch took: {}\".format(training_time))\n",
        "    print(\"\")\n",
        "    # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zKMwUJ2GMag7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def mcr_mse(y_trues, y_preds):\n",
        "    scores = []\n",
        "    idxes = y_trues.shape[1]\n",
        "    for i in range(idxes):\n",
        "        y_true = y_trues[:,i]\n",
        "        y_pred = y_preds[:,i]\n",
        "        score = mean_squared_error(y_true, y_pred, squared=False)\n",
        "        scores.append(score)\n",
        "    mcrmse_score = np.mean(scores)\n",
        "    return mcrmse_score, scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "attention_masks_test = torch.tensor(attention_masks_test)\n",
        "xlnet_test_labels = torch.tensor(xlnet_test_labels)\n",
        "print(len(xlnet_test_list), len(xlnet_test_labels))\n",
        "xlnet_tokenized_test = list(zip(xlnet_test_list, xlnet_test_labels, attention_masks_test))"
      ],
      "metadata": {
        "id": "XVSsZKV9ZXdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f15bad6-44c1-44c5-89ca-0f0ab303a7cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "783 783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YxH1FrjTMag7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9809bab-339d-4a49-b59c-fd26579988c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]],\n",
            "\n",
            "        [[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]],\n",
            "\n",
            "        [[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]],\n",
            "\n",
            "        [[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]],\n",
            "\n",
            "        [[3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         ...,\n",
            "         [3.5435],\n",
            "         [3.5435],\n",
            "         [3.5435]]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0a6672e55e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print(logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# mcrmse = mcr_mse(outputs[1].cpu(), target.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmcrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcr_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# logits = logits.sigmoid().detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4e0dd9637aa3>\u001b[0m in \u001b[0;36mmcr_mse\u001b[0;34m(y_trues, y_preds)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ],
      "source": [
        "\n",
        "test_dataloader = DataLoader(xlnet_tokenized_test, sampler=RandomSampler(xlnet_tokenized_test), batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for idx, (batch,target, mask) in enumerate(test_dataloader):\n",
        "  input_ids_np = batch.to(device)\n",
        "  target = target.to(device)\n",
        "  mask = mask.to(device)\n",
        "  with torch.no_grad():\n",
        "    # print(target)\n",
        "    # outputs = model(input_ids=input_ids_np, attention_mask=mask, labels=target)\n",
        "    output = model(input_ids_np, mask)\n",
        "    print(output)\n",
        "    # logits = outputs[1]\n",
        "    # print(logits)\n",
        "    # mcrmse = mcr_mse(outputs[1].cpu(), target.cpu())\n",
        "    mcrmse = mcr_mse(output, target)\n",
        "    print(mcrmse)\n",
        "  # logits = logits.sigmoid().detach().cpu().numpy()\n",
        "  # label_ids = target.cpu().numpy()\n",
        "  predictions.append(outputs)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIL1OolZMag7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48b0fd9e-296c-43d5-cf95-c217672a2e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLNetForSequenceClassificationOutput(loss=tensor(3.8802, device='cuda:0'), logits=tensor([[ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148],\n",
            "        [ 1.8462,  2.2846,  2.0155,  0.9988, -0.7767, -2.0933, -2.9940, -3.5153,\n",
            "         -1.3148]], device='cuda:0'), mems=(tensor([[[ 4.1071e-02, -1.8301e-03, -1.9870e-02,  ..., -4.5522e-02,\n",
            "           2.2274e-02, -3.2070e-02],\n",
            "         [-2.7732e-02,  2.3159e-03, -1.2619e-02,  ...,  1.4005e-02,\n",
            "           4.6191e-02,  1.8311e-02],\n",
            "         [ 1.4051e-02,  2.8089e-02, -3.1136e-02,  ..., -1.5162e-02,\n",
            "           2.8704e-02, -1.4939e-02],\n",
            "         ...,\n",
            "         [-3.7055e-02, -6.7944e-03, -2.1688e-03,  ...,  6.7240e-02,\n",
            "           1.5746e-02, -1.7377e-02],\n",
            "         [-3.7055e-02, -6.7944e-03, -2.1688e-03,  ...,  6.7240e-02,\n",
            "           1.5746e-02, -1.7377e-02],\n",
            "         [ 4.1032e-02,  3.2994e-02, -1.4208e-02,  ...,  4.1752e-02,\n",
            "          -2.0679e-02, -2.3683e-02]],\n",
            "\n",
            "        [[ 1.5663e-03,  2.3265e-02, -4.4774e-02,  ..., -4.8113e-02,\n",
            "           3.0738e-03, -4.1288e-02],\n",
            "         [ 1.5663e-03,  2.3265e-02, -4.4774e-02,  ..., -4.8113e-02,\n",
            "           3.0738e-03, -4.1288e-02],\n",
            "         [ 4.1032e-02,  3.2994e-02, -1.4208e-02,  ...,  4.1752e-02,\n",
            "          -2.0679e-02, -2.3683e-02],\n",
            "         ...,\n",
            "         [ 2.2155e-02,  5.0352e-02, -2.0030e-02,  ...,  6.3945e-03,\n",
            "           2.2483e-03,  2.6190e-03],\n",
            "         [ 2.2155e-02,  5.0352e-02, -2.0030e-02,  ...,  6.3945e-03,\n",
            "           2.2483e-03,  2.6190e-03],\n",
            "         [-2.7732e-02,  2.3159e-03, -1.2619e-02,  ...,  1.4005e-02,\n",
            "           4.6191e-02,  1.8311e-02]],\n",
            "\n",
            "        [[ 9.8740e-03,  4.6112e-02,  1.9616e-02,  ...,  4.6039e-02,\n",
            "           1.6875e-02, -3.4061e-02],\n",
            "         [ 6.6937e-03, -2.7244e-02,  2.1632e-02,  ..., -5.3790e-02,\n",
            "           4.1309e-02,  4.0973e-04],\n",
            "         [-2.4792e-02, -1.8907e-02, -1.3666e-02,  ..., -2.3663e-02,\n",
            "           2.9864e-02, -1.6236e-02],\n",
            "         ...,\n",
            "         [-4.0919e-02, -1.8522e-02, -8.3659e-02,  ..., -1.0395e-01,\n",
            "          -6.3384e-03, -2.4060e-02],\n",
            "         [ 7.2966e-02,  5.5271e-03,  1.2118e-02,  ..., -5.7334e-02,\n",
            "           1.1143e-02, -2.0586e-02],\n",
            "         [ 3.2228e-02,  1.9754e-02, -2.7412e-02,  ..., -4.8493e-02,\n",
            "          -1.5500e-02, -2.5713e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.5663e-03,  2.3265e-02, -4.4774e-02,  ..., -4.8113e-02,\n",
            "           3.0738e-03, -4.1288e-02],\n",
            "         [-2.4828e-02,  5.2138e-02, -8.7924e-03,  ..., -2.3036e-02,\n",
            "           3.6258e-02,  1.7483e-02],\n",
            "         [ 8.5664e-02, -4.5633e-02,  7.1449e-02,  ..., -8.4049e-02,\n",
            "          -9.5840e-02,  1.8475e-02],\n",
            "         ...,\n",
            "         [-2.4792e-02, -1.8907e-02, -1.3666e-02,  ..., -2.3663e-02,\n",
            "           2.9864e-02, -1.6236e-02],\n",
            "         [ 3.0745e-02, -3.8545e-03,  1.1666e-04,  ...,  5.3669e-02,\n",
            "          -2.3224e-02, -2.4806e-02],\n",
            "         [-2.4236e-03,  2.4701e-02, -5.0754e-02,  ...,  4.3356e-02,\n",
            "           1.4631e-02,  6.1057e-02]],\n",
            "\n",
            "        [[ 2.9584e-03, -8.4457e-03, -4.0567e-02,  ..., -3.6141e-02,\n",
            "          -1.9914e-02, -3.4678e-02],\n",
            "         [-3.2389e-02,  2.6677e-02,  4.3046e-02,  ..., -3.7474e-02,\n",
            "           2.2288e-02, -3.7110e-02],\n",
            "         [ 5.0673e-02, -3.2481e-03, -4.8342e-02,  ..., -3.1943e-02,\n",
            "          -3.7030e-02, -3.4617e-02],\n",
            "         ...,\n",
            "         [-1.1895e-02, -2.5693e-02,  3.0448e-03,  ..., -4.2308e-02,\n",
            "          -3.6064e-02, -3.0789e-02],\n",
            "         [-2.4792e-02, -1.8907e-02, -1.3666e-02,  ..., -2.3663e-02,\n",
            "           2.9864e-02, -1.6236e-02],\n",
            "         [-1.2202e-03, -1.2833e-02, -1.5126e-02,  ...,  1.6063e-02,\n",
            "          -8.4925e-02, -3.6383e-02]],\n",
            "\n",
            "        [[ 5.9669e-02, -1.6059e-02, -4.3693e-02,  ..., -2.9580e-02,\n",
            "           3.2770e-02, -1.8071e-02],\n",
            "         [ 9.5718e-03, -8.4506e-02, -5.1308e-03,  ..., -8.4361e-02,\n",
            "          -2.4895e-02,  1.0284e-01],\n",
            "         [ 3.2228e-02,  1.9754e-02, -2.7412e-02,  ..., -4.8493e-02,\n",
            "          -1.5500e-02, -2.5713e-02],\n",
            "         ...,\n",
            "         [ 8.2319e-02, -8.9906e-02, -4.5269e-02,  ...,  3.5411e-02,\n",
            "          -1.3602e-01,  1.3792e-02],\n",
            "         [-1.2202e-03, -1.2833e-02, -1.5126e-02,  ...,  1.6063e-02,\n",
            "          -8.4925e-02, -3.6383e-02],\n",
            "         [-5.7044e-02,  3.2769e-03, -4.0604e-02,  ..., -1.7402e-02,\n",
            "          -6.2945e-02, -8.1546e-02]]], device='cuda:0'), tensor([[[-1.5928e-01, -1.0007e+00, -5.2816e-01,  ..., -9.7333e-01,\n",
            "           9.6340e-02, -9.6204e-01],\n",
            "         [-1.9123e+00, -1.0123e+00, -1.0364e+00,  ...,  1.9641e-01,\n",
            "           1.4602e+00,  2.2518e-01],\n",
            "         [ 7.7384e-01, -7.2685e-01, -9.2409e-01,  ..., -7.7128e-02,\n",
            "           1.2741e+00, -8.4910e-02],\n",
            "         ...,\n",
            "         [-5.4651e-01, -7.7504e-01,  5.9634e-01,  ...,  6.8653e-01,\n",
            "          -1.9752e-02,  3.7037e-02],\n",
            "         [-4.0161e-01, -5.2711e-01,  4.9775e-01,  ...,  8.4120e-01,\n",
            "          -2.8963e-01,  6.5779e-02],\n",
            "         [ 3.6404e-01, -9.0845e-01, -6.7273e-01,  ...,  1.0636e+00,\n",
            "          -3.2711e-02, -5.7697e-01]],\n",
            "\n",
            "        [[-2.6529e-01, -8.0809e-01, -1.3674e+00,  ..., -4.9132e-01,\n",
            "           3.1244e-01, -1.0487e+00],\n",
            "         [-1.2657e-01, -7.9266e-01, -9.8793e-01,  ..., -5.4545e-01,\n",
            "           1.0339e+00, -7.2475e-01],\n",
            "         [ 8.4949e-01, -8.7899e-01, -7.0217e-01,  ...,  1.1711e+00,\n",
            "           2.8979e-02, -7.1994e-01],\n",
            "         ...,\n",
            "         [ 6.9485e-01, -3.6059e-01, -4.9445e-01,  ..., -2.8689e-01,\n",
            "          -7.2740e-01,  5.0420e-01],\n",
            "         [ 7.2221e-01, -2.7973e-01, -6.4260e-01,  ..., -7.6502e-02,\n",
            "          -9.9484e-01,  4.9986e-01],\n",
            "         [-1.9282e+00, -7.9233e-01, -1.4573e+00,  ..., -1.8891e-01,\n",
            "           1.0253e+00,  2.1753e-01]],\n",
            "\n",
            "        [[-7.1385e-01, -4.5503e-01, -7.2353e-01,  ...,  9.7920e-01,\n",
            "          -5.4958e-02, -7.7513e-01],\n",
            "         [ 4.8990e-02, -1.9286e+00, -4.7522e-02,  ..., -3.3043e-01,\n",
            "           1.0421e+00, -1.4585e-01],\n",
            "         [ 1.3956e-02, -4.2825e-01, -6.1729e-01,  ...,  5.6740e-01,\n",
            "          -5.9566e-01, -9.2602e-02],\n",
            "         ...,\n",
            "         [-8.5295e-01, -9.2746e-01, -1.9844e+00,  ..., -1.0530e+00,\n",
            "           6.9125e-01, -7.4781e-01],\n",
            "         [ 1.3646e+00, -2.9459e-01, -1.2470e-01,  ..., -5.3800e-01,\n",
            "           1.0562e+00, -2.8248e-01],\n",
            "         [ 6.2110e-01, -6.4926e-01, -9.4080e-01,  ..., -1.1542e+00,\n",
            "          -4.0827e-01,  3.0139e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.4536e-01, -7.1756e-01, -9.5456e-01,  ..., -1.1736e-01,\n",
            "           4.3083e-01, -6.5391e-01],\n",
            "         [-7.3129e-01,  4.5975e-05, -3.6993e-02,  ..., -4.6429e-01,\n",
            "           1.4852e-01, -5.4644e-01],\n",
            "         [ 1.0443e+00, -1.5659e+00,  9.2959e-01,  ..., -9.4785e-01,\n",
            "          -1.4488e+00,  1.8233e-01],\n",
            "         ...,\n",
            "         [ 3.3086e-01, -8.9206e-01,  1.5603e-01,  ..., -8.2044e-02,\n",
            "           2.5494e-03,  1.2009e-01],\n",
            "         [ 1.2007e-01, -1.3334e+00,  2.7332e-01,  ...,  7.1825e-01,\n",
            "          -2.9411e-01, -9.4802e-01],\n",
            "         [-1.1725e+00, -7.0392e-01, -1.0138e+00,  ...,  6.1200e-01,\n",
            "           6.8998e-01, -9.5635e-02]],\n",
            "\n",
            "        [[ 5.4591e-01, -8.7169e-01, -8.7778e-01,  ..., -2.2949e-01,\n",
            "           1.1384e-01, -3.1151e-02],\n",
            "         [-7.0246e-01, -5.6789e-01,  7.0245e-01,  ..., -2.4419e-02,\n",
            "          -9.7619e-02, -9.1323e-01],\n",
            "         [ 3.8134e-01, -5.1962e-01, -2.8375e-01,  ..., -2.9889e-01,\n",
            "          -2.8093e-01, -1.2218e+00],\n",
            "         ...,\n",
            "         [ 2.1599e-01, -1.2769e+00,  1.4437e-01,  ..., -9.9672e-01,\n",
            "          -8.9922e-01, -1.1026e+00],\n",
            "         [-1.7240e-01, -7.2230e-01,  7.0419e-02,  ...,  2.9378e-01,\n",
            "          -2.6974e-01, -5.1667e-01],\n",
            "         [-2.8229e-01, -1.1308e+00,  2.7970e-01,  ...,  2.0483e-01,\n",
            "          -1.2516e+00, -1.4411e+00]],\n",
            "\n",
            "        [[ 1.8971e+00, -1.3764e+00, -1.2351e+00,  ..., -7.5910e-01,\n",
            "           1.0158e+00, -3.9347e-01],\n",
            "         [ 5.9780e-02, -3.1808e+00, -9.1653e-01,  ..., -1.1426e+00,\n",
            "          -2.3609e-01,  1.5503e+00],\n",
            "         [ 2.6289e-01, -4.5808e-01, -4.4395e-01,  ..., -5.5822e-01,\n",
            "          -7.1392e-01, -6.4371e-01],\n",
            "         ...,\n",
            "         [ 1.3159e+00, -1.9319e+00, -4.8724e-01,  ...,  5.5267e-01,\n",
            "          -2.1150e+00, -5.1367e-01],\n",
            "         [ 1.5801e-02, -1.0214e+00,  2.6572e-01,  ...,  9.8360e-01,\n",
            "          -1.6549e+00, -1.5925e+00],\n",
            "         [-2.9037e-01, -6.9778e-01, -2.7873e-01,  ...,  1.3800e-01,\n",
            "          -7.4276e-04, -6.1821e-01]]], device='cuda:0'), tensor([[[-9.4658e-01, -1.5661e+00, -6.8359e-03,  ..., -1.9796e-01,\n",
            "          -5.8387e-01, -9.7325e-01],\n",
            "         [-1.5376e+00, -1.3955e+00, -1.5411e-03,  ..., -2.6161e-01,\n",
            "           8.9278e-02,  3.1105e-02],\n",
            "         [ 5.1978e-01, -1.1355e+00, -8.4358e-01,  ...,  2.4049e-01,\n",
            "           2.1806e-01,  5.2046e-01],\n",
            "         ...,\n",
            "         [-9.4260e-01, -1.0461e+00,  4.3157e-01,  ...,  6.0702e-01,\n",
            "          -9.1706e-01, -1.1992e+00],\n",
            "         [-6.9149e-01, -9.0473e-01,  5.6403e-01,  ...,  7.0626e-01,\n",
            "          -1.0216e+00, -1.1703e+00],\n",
            "         [-1.9825e-01, -1.6441e+00, -1.4860e-01,  ..., -5.3115e-02,\n",
            "          -7.5249e-01, -3.2788e-01]],\n",
            "\n",
            "        [[-1.2205e+00, -1.3955e+00, -2.1342e-01,  ...,  5.5658e-01,\n",
            "          -4.0382e-01, -6.7805e-01],\n",
            "         [-8.1022e-01, -1.5270e+00, -2.1385e-01,  ..., -1.7646e-01,\n",
            "           2.7248e-01, -1.6205e-01],\n",
            "         [ 6.8623e-01, -1.1086e+00, -4.5719e-01,  ...,  5.2724e-01,\n",
            "          -5.9991e-01, -3.8142e-01],\n",
            "         ...,\n",
            "         [ 3.3061e-02, -8.5975e-01, -4.5360e-01,  ...,  3.2807e-01,\n",
            "          -5.3488e-01, -3.6943e-01],\n",
            "         [-3.0911e-01, -1.1924e+00,  5.6956e-02,  ...,  4.7301e-01,\n",
            "          -6.3141e-01, -1.8134e-01],\n",
            "         [-1.7449e+00, -1.2658e+00, -1.2876e+00,  ..., -4.6716e-01,\n",
            "          -1.5662e-01, -3.7204e-01]],\n",
            "\n",
            "        [[-1.1507e+00, -1.3972e+00, -1.2302e-01,  ...,  8.4977e-01,\n",
            "          -3.7865e-01, -1.9679e-01],\n",
            "         [-2.6749e-01, -2.1184e+00, -8.3611e-02,  ..., -2.9470e-01,\n",
            "          -1.4545e-01,  1.2121e-01],\n",
            "         [-6.3026e-01, -1.7947e+00, -6.4380e-01,  ..., -4.7218e-01,\n",
            "          -8.9761e-01, -2.0197e-01],\n",
            "         ...,\n",
            "         [-1.1804e+00, -1.7833e+00, -5.7112e-01,  ..., -1.2644e-01,\n",
            "          -8.1899e-02, -1.5853e-01],\n",
            "         [-9.9374e-02, -1.4688e+00,  3.4484e-01,  ..., -1.5944e-01,\n",
            "          -2.3445e-01,  8.5082e-02],\n",
            "         [-5.4816e-01, -1.4664e+00,  3.2450e-02,  ..., -3.8258e-01,\n",
            "          -1.1087e+00,  1.4590e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-8.1954e-01, -1.5499e+00, -1.7824e-04,  ...,  6.7476e-01,\n",
            "           1.2819e-02, -6.7714e-01],\n",
            "         [-5.6603e-01, -1.3721e+00,  3.9324e-01,  ..., -4.8888e-01,\n",
            "          -7.7691e-01, -5.6297e-01],\n",
            "         [-7.0346e-01, -1.4807e+00,  1.4457e+00,  ..., -1.7907e-01,\n",
            "          -4.9956e-01, -1.5085e-01],\n",
            "         ...,\n",
            "         [-4.1002e-01, -1.7700e+00,  3.8992e-01,  ..., -1.1856e-01,\n",
            "          -2.8922e-01, -4.1849e-01],\n",
            "         [-9.8080e-01, -2.1110e+00,  4.7693e-01,  ...,  9.3134e-02,\n",
            "          -4.6664e-01, -7.4980e-01],\n",
            "         [-1.2461e+00, -2.2279e+00, -6.9243e-01,  ..., -9.9413e-01,\n",
            "          -7.7301e-01, -1.0766e-01]],\n",
            "\n",
            "        [[-4.6918e-01, -1.4810e+00,  3.8855e-03,  ...,  3.3591e-01,\n",
            "           1.7878e-01, -1.0336e-01],\n",
            "         [-4.6941e-01, -1.5373e+00,  5.1982e-01,  ...,  3.7806e-01,\n",
            "          -6.9329e-01, -2.1436e-01],\n",
            "         [-1.9717e-01, -1.7416e+00,  4.3893e-01,  ..., -2.6558e-01,\n",
            "          -3.0577e-01, -6.2950e-01],\n",
            "         ...,\n",
            "         [-9.9436e-01, -1.9066e+00,  2.7101e-01,  ..., -4.5888e-01,\n",
            "          -5.1413e-01, -3.7628e-01],\n",
            "         [-1.0630e+00, -1.9172e+00,  1.2919e+00,  ..., -6.5267e-01,\n",
            "          -5.3813e-01, -4.3416e-01],\n",
            "         [-7.4991e-01, -1.7703e+00,  7.6889e-01,  ..., -2.6618e-01,\n",
            "          -7.1349e-01, -8.0223e-01]],\n",
            "\n",
            "        [[-3.3839e-01, -1.5609e+00, -3.1726e-02,  ...,  3.4563e-01,\n",
            "           3.1126e-01, -1.2027e-01],\n",
            "         [-5.9382e-01, -2.4616e+00,  1.0888e-01,  ..., -9.1666e-01,\n",
            "          -5.1097e-01,  6.5070e-01],\n",
            "         [-5.8872e-01, -1.1641e+00,  3.4303e-01,  ..., -6.4198e-02,\n",
            "          -9.7665e-01, -6.7832e-01],\n",
            "         ...,\n",
            "         [-5.0589e-01, -1.7981e+00, -3.7305e-01,  ...,  1.7557e-01,\n",
            "          -1.1471e+00, -3.0495e-02],\n",
            "         [-7.6039e-01, -1.6712e+00,  5.5998e-01,  ..., -1.5999e-02,\n",
            "          -8.7431e-01, -5.9843e-01],\n",
            "         [-4.8947e-01, -1.4470e+00,  2.2402e-01,  ...,  4.3732e-01,\n",
            "          -2.6089e-01, -1.8084e-01]]], device='cuda:0'), tensor([[[-1.0432, -0.6695, -0.5524,  ..., -0.5781,  0.3005, -1.2740],\n",
            "         [-1.1152, -0.6083, -0.4860,  ..., -0.8687,  0.2178, -1.0941],\n",
            "         [-0.8012, -0.5716, -0.6305,  ..., -0.6536,  0.2412, -1.0280],\n",
            "         ...,\n",
            "         [-0.8632, -0.4507, -0.3642,  ..., -0.3139,  0.1891, -1.2353],\n",
            "         [-0.8642, -0.3808, -0.2446,  ..., -0.4872,  0.0937, -1.4371],\n",
            "         [-0.8817, -0.7019, -0.4871,  ..., -0.8135,  0.0176, -1.4033]],\n",
            "\n",
            "        [[-1.1060, -0.5633, -0.6379,  ..., -0.3297,  0.3673, -1.0346],\n",
            "         [-1.0173, -0.4968, -0.5398,  ..., -0.7814,  0.3825, -1.1038],\n",
            "         [-0.7511, -0.5898, -0.5766,  ..., -0.6033,  0.0543, -1.2879],\n",
            "         ...,\n",
            "         [-0.8265, -0.4570, -0.6049,  ..., -0.1747,  0.2249, -1.1412],\n",
            "         [-0.8787, -0.5239, -0.3527,  ..., -0.1891,  0.1385, -1.1649],\n",
            "         [-1.2620, -0.6183, -0.8843,  ..., -1.2042,  0.0872, -1.3262]],\n",
            "\n",
            "        [[-1.0841, -0.5295, -0.6258,  ..., -0.2330,  0.3552, -0.9913],\n",
            "         [-0.9183, -0.7217, -0.6026,  ..., -0.8793,  0.3243, -1.0936],\n",
            "         [-1.0156, -0.5859, -0.7077,  ..., -1.0509, -0.0216, -1.2779],\n",
            "         ...,\n",
            "         [-0.9025, -0.6253, -0.6143,  ..., -0.3000,  0.4257, -0.9340],\n",
            "         [-0.7948, -0.5966, -0.2737,  ..., -0.5138,  0.3194, -0.9421],\n",
            "         [-1.0967, -0.4794, -0.6604,  ..., -0.9900, -0.0928, -1.1590]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.9516, -0.6093, -0.5009,  ..., -0.3969,  0.4053, -0.8771],\n",
            "         [-1.0083, -0.5073, -0.2963,  ..., -0.8313,  0.2617, -1.2747],\n",
            "         [-0.9649, -0.6104, -0.0888,  ..., -1.1162,  0.0974, -1.2118],\n",
            "         ...,\n",
            "         [-0.9410, -0.5870, -0.4612,  ..., -0.5758,  0.2831, -1.0953],\n",
            "         [-1.0763, -0.6511, -0.4864,  ..., -0.8763,  0.1684, -1.2866],\n",
            "         [-1.0437, -1.0148, -0.6797,  ..., -1.4875, -0.1113, -1.7940]],\n",
            "\n",
            "        [[-0.9212, -0.6574, -0.6142,  ..., -0.4440,  0.4124, -0.7690],\n",
            "         [-1.0591, -0.4953, -0.3209,  ..., -0.5420,  0.1697, -1.0449],\n",
            "         [-0.8935, -0.7159, -0.4026,  ..., -1.0928,  0.1910, -1.3868],\n",
            "         ...,\n",
            "         [-1.0078, -0.6343, -0.4106,  ..., -0.6710,  0.1636, -1.0724],\n",
            "         [-1.0576, -0.6165, -0.1127,  ..., -1.1071,  0.1449, -1.3278],\n",
            "         [-0.8930, -0.7062, -0.3733,  ..., -1.0733,  0.0260, -1.8022]],\n",
            "\n",
            "        [[-0.8914, -0.6402, -0.6440,  ..., -0.4418,  0.3948, -0.7000],\n",
            "         [-1.0374, -0.7361, -0.4907,  ..., -0.9868,  0.1513, -0.9018],\n",
            "         [-1.1151, -0.5403, -0.4024,  ..., -0.9621,  0.0661, -1.1656],\n",
            "         ...,\n",
            "         [-0.9271, -0.6747, -0.5566,  ..., -0.6931,  0.1341, -1.0451],\n",
            "         [-1.0003, -0.6081, -0.3761,  ..., -0.9669, -0.0328, -1.3942],\n",
            "         [-0.8790, -0.6542, -0.4651,  ..., -0.9459,  0.2561, -1.3588]]],\n",
            "       device='cuda:0'), tensor([[[-0.4676, -0.1044, -0.2224,  ..., -0.1850, -0.4980,  0.2243],\n",
            "         [-0.4655, -0.1028, -0.2184,  ..., -0.1889, -0.4985,  0.2236],\n",
            "         [-0.4629, -0.1052, -0.2190,  ..., -0.1852, -0.4987,  0.2281],\n",
            "         ...,\n",
            "         [-0.4715, -0.1030, -0.2173,  ..., -0.1817, -0.4930,  0.2299],\n",
            "         [-0.4697, -0.1061, -0.2158,  ..., -0.1815, -0.4943,  0.2311],\n",
            "         [-0.4607, -0.1060, -0.2188,  ..., -0.1818, -0.5038,  0.2255]],\n",
            "\n",
            "        [[-0.4669, -0.1027, -0.2237,  ..., -0.1865, -0.4983,  0.2244],\n",
            "         [-0.4627, -0.1017, -0.2191,  ..., -0.1892, -0.4964,  0.2215],\n",
            "         [-0.4617, -0.1048, -0.2189,  ..., -0.1830, -0.5018,  0.2255],\n",
            "         ...,\n",
            "         [-0.4745, -0.1034, -0.2177,  ..., -0.1830, -0.4996,  0.2296],\n",
            "         [-0.4727, -0.1077, -0.2160,  ..., -0.1868, -0.5035,  0.2306],\n",
            "         [-0.4670, -0.1055, -0.2268,  ..., -0.1880, -0.5053,  0.2314]],\n",
            "\n",
            "        [[-0.4675, -0.1022, -0.2236,  ..., -0.1843, -0.4975,  0.2227],\n",
            "         [-0.4655, -0.1041, -0.2211,  ..., -0.1900, -0.4975,  0.2224],\n",
            "         [-0.4684, -0.1047, -0.2252,  ..., -0.1857, -0.4984,  0.2277],\n",
            "         ...,\n",
            "         [-0.4716, -0.1022, -0.2211,  ..., -0.1895, -0.4948,  0.2273],\n",
            "         [-0.4674, -0.1068, -0.2179,  ..., -0.1904, -0.4979,  0.2303],\n",
            "         [-0.4626, -0.1044, -0.2225,  ..., -0.1853, -0.5067,  0.2259]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.4653, -0.1039, -0.2189,  ..., -0.1864, -0.4962,  0.2231],\n",
            "         [-0.4701, -0.1006, -0.2187,  ..., -0.1874, -0.4959,  0.2219],\n",
            "         [-0.4685, -0.1062, -0.2165,  ..., -0.1888, -0.5011,  0.2296],\n",
            "         ...,\n",
            "         [-0.4676, -0.1001, -0.2187,  ..., -0.1844, -0.4992,  0.2197],\n",
            "         [-0.4657, -0.1035, -0.2174,  ..., -0.1863, -0.4944,  0.2235],\n",
            "         [-0.4641, -0.1084, -0.2191,  ..., -0.1873, -0.5029,  0.2259]],\n",
            "\n",
            "        [[-0.4657, -0.1044, -0.2202,  ..., -0.1853, -0.4956,  0.2236],\n",
            "         [-0.4705, -0.1007, -0.2157,  ..., -0.1891, -0.4973,  0.2259],\n",
            "         [-0.4674, -0.1077, -0.2161,  ..., -0.1919, -0.4949,  0.2326],\n",
            "         ...,\n",
            "         [-0.4665, -0.1013, -0.2187,  ..., -0.1840, -0.5006,  0.2216],\n",
            "         [-0.4694, -0.1029, -0.2148,  ..., -0.1870, -0.4966,  0.2278],\n",
            "         [-0.4654, -0.1064, -0.2198,  ..., -0.1870, -0.5005,  0.2277]],\n",
            "\n",
            "        [[-0.4656, -0.1040, -0.2229,  ..., -0.1847, -0.4953,  0.2230],\n",
            "         [-0.4692, -0.1034, -0.2194,  ..., -0.1889, -0.4936,  0.2243],\n",
            "         [-0.4717, -0.1074, -0.2190,  ..., -0.1926, -0.5020,  0.2319],\n",
            "         ...,\n",
            "         [-0.4659, -0.1034, -0.2196,  ..., -0.1832, -0.4986,  0.2225],\n",
            "         [-0.4679, -0.1050, -0.2204,  ..., -0.1896, -0.4994,  0.2299],\n",
            "         [-0.4684, -0.1093, -0.2196,  ..., -0.1885, -0.4991,  0.2287]]],\n",
            "       device='cuda:0'), tensor([[[-7.8693e-01, -3.2358e-01, -1.7708e+00,  ..., -1.4859e-03,\n",
            "          -1.5728e+00,  1.3903e+00],\n",
            "         [-7.8666e-01, -3.2359e-01, -1.7716e+00,  ..., -1.7358e-03,\n",
            "          -1.5735e+00,  1.3904e+00],\n",
            "         [-7.8551e-01, -3.2349e-01, -1.7714e+00,  ..., -1.5417e-03,\n",
            "          -1.5734e+00,  1.3896e+00],\n",
            "         ...,\n",
            "         [-7.8647e-01, -3.2350e-01, -1.7699e+00,  ..., -2.1727e-03,\n",
            "          -1.5724e+00,  1.3898e+00],\n",
            "         [-7.8559e-01, -3.2343e-01, -1.7702e+00,  ..., -2.1503e-03,\n",
            "          -1.5728e+00,  1.3897e+00],\n",
            "         [-7.8487e-01, -3.2314e-01, -1.7712e+00,  ..., -1.7741e-03,\n",
            "          -1.5735e+00,  1.3893e+00]],\n",
            "\n",
            "        [[-7.8721e-01, -3.2347e-01, -1.7715e+00,  ..., -1.3509e-03,\n",
            "          -1.5730e+00,  1.3905e+00],\n",
            "         [-7.8676e-01, -3.2354e-01, -1.7718e+00,  ..., -1.6034e-03,\n",
            "          -1.5738e+00,  1.3906e+00],\n",
            "         [-7.8527e-01, -3.2337e-01, -1.7708e+00,  ..., -1.4789e-03,\n",
            "          -1.5732e+00,  1.3892e+00],\n",
            "         ...,\n",
            "         [-7.8718e-01, -3.2371e-01, -1.7707e+00,  ..., -1.9286e-03,\n",
            "          -1.5727e+00,  1.3902e+00],\n",
            "         [-7.8680e-01, -3.2361e-01, -1.7715e+00,  ..., -1.7201e-03,\n",
            "          -1.5732e+00,  1.3903e+00],\n",
            "         [-7.8484e-01, -3.2318e-01, -1.7714e+00,  ..., -2.0984e-03,\n",
            "          -1.5735e+00,  1.3893e+00]],\n",
            "\n",
            "        [[-7.8686e-01, -3.2345e-01, -1.7713e+00,  ..., -1.2869e-03,\n",
            "          -1.5729e+00,  1.3903e+00],\n",
            "         [-7.8644e-01, -3.2353e-01, -1.7714e+00,  ..., -1.5780e-03,\n",
            "          -1.5733e+00,  1.3903e+00],\n",
            "         [-7.8522e-01, -3.2350e-01, -1.7711e+00,  ..., -1.7534e-03,\n",
            "          -1.5733e+00,  1.3893e+00],\n",
            "         ...,\n",
            "         [-7.8738e-01, -3.2354e-01, -1.7713e+00,  ..., -1.5234e-03,\n",
            "          -1.5729e+00,  1.3907e+00],\n",
            "         [-7.8678e-01, -3.2345e-01, -1.7718e+00,  ..., -1.4680e-03,\n",
            "          -1.5734e+00,  1.3908e+00],\n",
            "         [-7.8528e-01, -3.2321e-01, -1.7723e+00,  ..., -1.6653e-03,\n",
            "          -1.5739e+00,  1.3898e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-7.8718e-01, -3.2342e-01, -1.7710e+00,  ..., -1.4211e-03,\n",
            "          -1.5729e+00,  1.3903e+00],\n",
            "         [-7.8639e-01, -3.2342e-01, -1.7710e+00,  ..., -1.7254e-03,\n",
            "          -1.5735e+00,  1.3898e+00],\n",
            "         [-7.8536e-01, -3.2366e-01, -1.7717e+00,  ..., -1.6297e-03,\n",
            "          -1.5735e+00,  1.3897e+00],\n",
            "         ...,\n",
            "         [-7.8725e-01, -3.2337e-01, -1.7711e+00,  ..., -1.2651e-03,\n",
            "          -1.5733e+00,  1.3905e+00],\n",
            "         [-7.8586e-01, -3.2314e-01, -1.7711e+00,  ..., -1.4650e-03,\n",
            "          -1.5731e+00,  1.3896e+00],\n",
            "         [-7.8414e-01, -3.2309e-01, -1.7709e+00,  ..., -2.0647e-03,\n",
            "          -1.5734e+00,  1.3884e+00]],\n",
            "\n",
            "        [[-7.8701e-01, -3.2340e-01, -1.7707e+00,  ..., -1.3534e-03,\n",
            "          -1.5728e+00,  1.3902e+00],\n",
            "         [-7.8673e-01, -3.2348e-01, -1.7715e+00,  ..., -1.5442e-03,\n",
            "          -1.5736e+00,  1.3898e+00],\n",
            "         [-7.8503e-01, -3.2363e-01, -1.7707e+00,  ..., -1.7295e-03,\n",
            "          -1.5728e+00,  1.3892e+00],\n",
            "         ...,\n",
            "         [-7.8699e-01, -3.2327e-01, -1.7707e+00,  ..., -1.3686e-03,\n",
            "          -1.5733e+00,  1.3903e+00],\n",
            "         [-7.8580e-01, -3.2339e-01, -1.7709e+00,  ..., -1.6128e-03,\n",
            "          -1.5733e+00,  1.3896e+00],\n",
            "         [-7.8452e-01, -3.2326e-01, -1.7712e+00,  ..., -1.8042e-03,\n",
            "          -1.5733e+00,  1.3886e+00]],\n",
            "\n",
            "        [[-7.8696e-01, -3.2347e-01, -1.7707e+00,  ..., -1.3162e-03,\n",
            "          -1.5728e+00,  1.3903e+00],\n",
            "         [-7.8646e-01, -3.2361e-01, -1.7714e+00,  ..., -1.5131e-03,\n",
            "          -1.5735e+00,  1.3901e+00],\n",
            "         [-7.8578e-01, -3.2354e-01, -1.7716e+00,  ..., -1.8328e-03,\n",
            "          -1.5732e+00,  1.3898e+00],\n",
            "         ...,\n",
            "         [-7.8701e-01, -3.2333e-01, -1.7709e+00,  ..., -1.2438e-03,\n",
            "          -1.5734e+00,  1.3905e+00],\n",
            "         [-7.8557e-01, -3.2338e-01, -1.7708e+00,  ..., -1.5527e-03,\n",
            "          -1.5731e+00,  1.3896e+00],\n",
            "         [-7.8455e-01, -3.2332e-01, -1.7708e+00,  ..., -2.0842e-03,\n",
            "          -1.5731e+00,  1.3888e+00]]], device='cuda:0'), tensor([[[-0.7146, -0.2921, -0.8775,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8776,  ..., -0.0990, -0.4994,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989],\n",
            "         ...,\n",
            "         [-0.7146, -0.2922, -0.8774,  ..., -0.0990, -0.4994,  0.5989],\n",
            "         [-0.7145, -0.2922, -0.8774,  ..., -0.0989, -0.4995,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989]],\n",
            "\n",
            "        [[-0.7147, -0.2921, -0.8776,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7146, -0.2921, -0.8776,  ..., -0.0990, -0.4994,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989],\n",
            "         ...,\n",
            "         [-0.7146, -0.2921, -0.8775,  ..., -0.0990, -0.4994,  0.5989],\n",
            "         [-0.7145, -0.2922, -0.8775,  ..., -0.0989, -0.4995,  0.5988],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989]],\n",
            "\n",
            "        [[-0.7146, -0.2921, -0.8775,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8776,  ..., -0.0990, -0.4994,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989],\n",
            "         ...,\n",
            "         [-0.7146, -0.2921, -0.8775,  ..., -0.0989, -0.4994,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8775,  ..., -0.0988, -0.4995,  0.5989],\n",
            "         [-0.7144, -0.2921, -0.8777,  ..., -0.0988, -0.4996,  0.5988]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7147, -0.2920, -0.8775,  ..., -0.0991, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8776,  ..., -0.0990, -0.4995,  0.5989],\n",
            "         [-0.7145, -0.2922, -0.8776,  ..., -0.0989, -0.4995,  0.5989],\n",
            "         ...,\n",
            "         [-0.7146, -0.2920, -0.8775,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8775,  ..., -0.0989, -0.4994,  0.5989],\n",
            "         [-0.7143, -0.2922, -0.8776,  ..., -0.0989, -0.4996,  0.5989]],\n",
            "\n",
            "        [[-0.7147, -0.2921, -0.8775,  ..., -0.0991, -0.4992,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8776,  ..., -0.0989, -0.4995,  0.5988],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4995,  0.5990],\n",
            "         ...,\n",
            "         [-0.7146, -0.2920, -0.8775,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7146, -0.2921, -0.8775,  ..., -0.0989, -0.4995,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8776,  ..., -0.0989, -0.4997,  0.5989]],\n",
            "\n",
            "        [[-0.7147, -0.2921, -0.8775,  ..., -0.0991, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8776,  ..., -0.0989, -0.4995,  0.5989],\n",
            "         [-0.7144, -0.2922, -0.8777,  ..., -0.0989, -0.4996,  0.5989],\n",
            "         ...,\n",
            "         [-0.7146, -0.2920, -0.8775,  ..., -0.0990, -0.4993,  0.5989],\n",
            "         [-0.7145, -0.2921, -0.8775,  ..., -0.0990, -0.4995,  0.5989],\n",
            "         [-0.7144, -0.2923, -0.8776,  ..., -0.0989, -0.4997,  0.5989]]],\n",
            "       device='cuda:0'), tensor([[[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1037,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1823,  0.7386,  1.1188]],\n",
            "\n",
            "        [[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1823,  0.7386,  1.1188]],\n",
            "\n",
            "        [[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1037,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1188]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1037,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1823,  0.7386,  1.1188]],\n",
            "\n",
            "        [[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1037,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1823,  0.7385,  1.1188]],\n",
            "\n",
            "        [[-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1036,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1036,  ...,  0.1824,  0.7386,  1.1189],\n",
            "         ...,\n",
            "         [-0.1645,  0.1681,  0.1036,  ...,  0.1824,  0.7388,  1.1190],\n",
            "         [-0.1644,  0.1680,  0.1037,  ...,  0.1824,  0.7387,  1.1189],\n",
            "         [-0.1643,  0.1680,  0.1037,  ...,  0.1823,  0.7386,  1.1188]]],\n",
            "       device='cuda:0'), tensor([[[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4269,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]],\n",
            "\n",
            "        [[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4269,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]],\n",
            "\n",
            "        [[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5947,  ..., -0.4677,  1.3855,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4269,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]],\n",
            "\n",
            "        [[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5946,  ..., -0.4677,  1.3855,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4269,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]],\n",
            "\n",
            "        [[ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3856,  0.8154],\n",
            "         [ 0.4268,  0.2129, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5947,  ..., -0.4677,  1.3855,  0.8153],\n",
            "         ...,\n",
            "         [ 0.4267,  0.2129, -0.5947,  ..., -0.4676,  1.3855,  0.8154],\n",
            "         [ 0.4268,  0.2128, -0.5947,  ..., -0.4677,  1.3855,  0.8154],\n",
            "         [ 0.4269,  0.2128, -0.5946,  ..., -0.4677,  1.3854,  0.8153]]],\n",
            "       device='cuda:0'), tensor([[[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]],\n",
            "\n",
            "        [[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]],\n",
            "\n",
            "        [[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]],\n",
            "\n",
            "        [[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]],\n",
            "\n",
            "        [[ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         ...,\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405],\n",
            "         [ 0.1663, -0.0688, -0.1266,  ...,  0.1090,  0.1740,  0.1405]]],\n",
            "       device='cuda:0'), tensor([[[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]],\n",
            "\n",
            "        [[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]],\n",
            "\n",
            "        [[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]],\n",
            "\n",
            "        [[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]],\n",
            "\n",
            "        [[-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         ...,\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486],\n",
            "         [-1.6345, -0.1677,  0.3703,  ..., -0.2655, -0.5488, -0.7486]]],\n",
            "       device='cuda:0'), tensor([[[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]],\n",
            "\n",
            "        [[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]],\n",
            "\n",
            "        [[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]],\n",
            "\n",
            "        [[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]],\n",
            "\n",
            "        [[-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         ...,\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515],\n",
            "         [-1.4598,  0.0096,  0.4765,  ..., -0.3472,  0.0339, -0.0515]]],\n",
            "       device='cuda:0')), hidden_states=None, attentions=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f9ec68d8c082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmcr_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of XLNetForSequenceClassificationOutput"
          ]
        }
      ],
      "source": [
        "print(predictions[0])\n",
        "predictions = torch.tensor(predictions)\n",
        "true_labels = torch.tensor(true_labels)\n",
        "print(predictions, true_labels)\n",
        "mcr_mse(predictions, true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bB3cjwBMag7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  print(predictions[i], \" is prediction\")\n",
        "  print(true_labels)\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  print(matthews)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2wO2-dmMag8"
      },
      "outputs": [],
      "source": [
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-at-M52Mag8"
      },
      "outputs": [],
      "source": [
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEGiQ4oSMag8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aW2DYVw9Magw",
        "sVh3rSq-Magx",
        "LcBXE1kPMagz",
        "ttw1GRY5Mag1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "60fab8fe8636a998ca6cb0b1b1e73f8aa28fc993054ea5d496530e7813c285a1"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}