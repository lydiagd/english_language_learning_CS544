{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obiTALnwMHeQ",
        "outputId": "284bb72d-c915-40e4-a8dc-6df43bff70cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYP7vVdAMHeT",
        "outputId": "421b99e1-04f4-4d03-dcf0-bb21d6d05861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 # in case you don't have it installed\n",
        "\n",
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaU7ypb-MHeU"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Znq__eKMHeV",
        "outputId": "2b2f1d3e-6494-4023-e5f3-f0d6c575ab06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To run on Google Colab: \n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv', usecols=['full_text', 'cohesion', 'syntax', 'vocabulary',\t'phraseology', 'grammar',\t'conventions'], dtype={'full_text': 'object', 'cohesion': 'object', 'syntax': 'object', 'vocabulary': 'object',\t'phraseology': 'object', 'grammar': 'object','conventions': 'object'}\n",
        "                 )   \n"
      ],
      "metadata": {
        "id": "zNTFz9pSMs7h"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlvSd6SwMHeb"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eAXhUzq0MHec"
      },
      "outputs": [],
      "source": [
        "# Convert all reviews to lowercase\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].str.lower()\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].str.lower()\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].str.lower()\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].str.lower()\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].str.lower()\n",
        "\n",
        "df['full_text'] = df['full_text'].str.lower() \n",
        "\n",
        "\n",
        "# Check the output \n",
        "# print(random_reviews_5_stars['review_body'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBjYRjaEMHec"
      },
      "outputs": [],
      "source": [
        "# Remove the HTML and URLs from the reviews \n",
        "# def remove_HTML_URLS(sentence):\n",
        "#     sentence = str(sentence)\n",
        "#     soup = BeautifulSoup(sentence)\n",
        "#     clean_sentence = soup.get_text()\n",
        "#     return clean_sentence \n",
        "\n",
        "# # random_reviews_5_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_4_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_3_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_2_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_1_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# only_selected_datasets_combined['review_body'] = only_selected_datasets_combined['review_body'].apply(remove_HTML_URLS)\n",
        "\n",
        "\n",
        "# print(random_reviews_5_stars.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "N1FK9dUvMHec"
      },
      "outputs": [],
      "source": [
        "# Remove non-alphabetical characters\n",
        "\n",
        "import re \n",
        "def remove_non_alphabetical_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    removed_non_alphabetical_words_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_non_alphabetical_words_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_non_alphabetical_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F_MtpI-MHed"
      },
      "outputs": [],
      "source": [
        "# Remove extra spaces "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AQbPy3hUMHed"
      },
      "outputs": [],
      "source": [
        "def remove_extra_spaces(sentence):\n",
        "    sentence = str(sentence)\n",
        "    ' '.join(sentence.split())\n",
        "    removed_extra_spaces_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_extra_spaces_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_extra_spaces)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrTzLI1kMHed",
        "outputId": "45a35622-13ac-45fe-ea43-20c6ce1d3a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install the contractions library\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ymogAjWYMHed"
      },
      "outputs": [],
      "source": [
        "# Perform contractions on the dataframe\n",
        "import contractions\n",
        "\n",
        "def replace_contractions_with_full_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    replaced_contractions_sentence = contractions.fix(sentence)\n",
        "    return replaced_contractions_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(replace_contractions_with_full_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfLq37V4MHee"
      },
      "outputs": [],
      "source": [
        "# # Print the average length of the reviews in terms of character length in your dataset after cleaning\n",
        "# average_length_of_reviews_after_cleaning = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8KKheSSMHee"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_F6nVLQMHee"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset before preprocessing\n",
        "# average_length_of_reviews_before_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_before_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8sJ4uNwMHee"
      },
      "source": [
        "## remove the stop words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZ81Hb7MHee",
        "outputId": "9529f63f-6d47-4dd8-e48b-c0c07089ffa1",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Download the stopwords package. \n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Stop words are only in the review_body column, so remove stop words from this column. \n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stop_words(sentence):  \n",
        "    stopwords_to_remove = stopwords.words('english') # Set the stopwords that will be removed.\n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_stop_words = sentence.split()\n",
        "    sentence_list_without_stop_words = []\n",
        "    for word in sentence_list_with_stop_words:\n",
        "        if word not in stopwords_to_remove:\n",
        "            sentence_list_without_stop_words.append(word)\n",
        "    sentence_without_stop_words = ' '.join(sentence_list_without_stop_words)\n",
        "    return sentence_without_stop_words\n",
        "    \n",
        "# Apply this function along the review_body axis. \n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(remove_stop_words)\n",
        "df['full_text'] = df['full_text'].apply(remove_stop_words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GVMH09I5MHef"
      },
      "outputs": [],
      "source": [
        "# Check that output is correct.\n",
        "# print(only_selected_datasets_combined['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "76dDjtEIMXFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXAtqtl9MHef"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4AVHS8UMHef",
        "outputId": "3cc984f5-985e-418c-f6c6-ba454c24c6d9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize each word \n",
        "def lemmatize_each_word_in_the_sentence(sentence):  \n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_words_to_lemmatize = sentence.split() \n",
        "    sentence_list_with_lemmatized_words = []\n",
        "    for word in sentence_list_with_words_to_lemmatize:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word) \n",
        "        sentence_list_with_lemmatized_words.append(lemmatized_word)\n",
        "    sentence_with_lemmatized_words = ' '.join(sentence_list_with_lemmatized_words)\n",
        "    return sentence_with_lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lfMIWTq5MHef"
      },
      "outputs": [],
      "source": [
        "# Perform lemmatization.\n",
        "\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "df['full_text'] = df['full_text'].apply(lemmatize_each_word_in_the_sentence)\n",
        "\n",
        "\n",
        "# Check \n",
        "# print(random_reviews_5_stars['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YpxI2EqmVYgX"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset after preprocessing\n",
        "# average_length_of_reviews_after_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjw0f7e-MHeg"
      },
      "source": [
        "# TF-IDF Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "jogyKz7EMHeg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13162206-6e42-4ded-be81-3b468d88c6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "# Install sklearn\n",
        "!pip install sklearn\n",
        "\n",
        "\n",
        "# Import the module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.asarray(df[df.columns[1:]])\n"
      ],
      "metadata": {
        "id": "W7ukLSBa4fGi"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gScdr7YWD28D",
        "outputId": "0e740c22-00f4-44a9-d8d9-3b32e2920703"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',\n",
            "       'conventions'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "5bu867RkMHeg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the vectorizer\n",
        "vectorizer = TfidfVectorizer() \n",
        "train_features_star_rating = vectorizer.fit_transform(df['full_text'].apply(lambda x:np.str_(x))) # convert values to a string along the axis \n",
        "# significance of the words in the corpus \n",
        "# train_features_review_body = vectorizer.fit_transform(df['review_body'].values.astype('U')) # consumes too much memory \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6SYo48uMHeg"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5fXLNymMHeg"
      },
      "outputs": [],
      "source": [
        "# Train the Perceptron model on the training dataset using the sklearn built-in implementation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jo87ELnkMHeg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# X_labels = vectorizer.get_feature.names_out()\n",
        "# Y_values = df['conventions'].values\n",
        "# Y_values_2 = df['phraseology'].values\n",
        "\n",
        "\n",
        "X_training_data, X_testing_data, Y_training_data, Y_testing_data = train_test_split(train_features_star_rating, y, test_size=0.20)\n",
        "multi_output_perceptron = MultiOutputClassifier(Perceptron()).fit(X_training_data, Y_training_data)\n",
        "\n",
        "# perceptron = Perceptron(random_state=40) # choose whatever as long as good result \n",
        "# perceptron.fit(X_training_data, Y_training_data)\n",
        "# perceptron_score = perceptron.score(X_training_data, Y_training_data)\n",
        "\n",
        "# perceptron.fit(vectorizer.get_feature_names(), Y_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOgG3rgLtkhh"
      },
      "outputs": [],
      "source": [
        "# print(perceptron_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adidTozaMHeh",
        "outputId": "24fd59a3-6a25-4c02-9429-e3c6c36efc05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# X_values = df['full_text'].values\n",
        "# predictions_training = multi_output_perceptron.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "# predictions_test = multi_output_perceptron.predict(X_testing_data)\n",
        "# print(accuracy_score(predictions_training, Y_training_data))\n",
        "print(multi_output_perceptron.score(X_testing_data, Y_testing_data))\n",
        "\n",
        "# print(accuracy_score(predictions_train, Y_training_data))\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# Print the classification report \n",
        "# print(classification_report(Y_testing_data, predictions_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFW2pkZtVN7P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl_IKIy5MHei"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4hi2IVMHei",
        "outputId": "77ddd844-937c-4ee9-ef1f-9068008f8a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# svm_model = svm.SVR()\n",
        "# svm_model.fit(X_training_data, Y_training_data)\n",
        "\n",
        "multi_output_linear_svm = MultiOutputClassifier(svm.LinearSVC()).fit(X_training_data, Y_training_data)\n",
        "print(multi_output_linear_svm.score(X_testing_data, Y_testing_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfCS0hFMHei"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyhugkdGMHei",
        "outputId": "81dd6adf-f2fe-43e0-ced5-5c2515cebc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "multi_output_logistic_regression = MultiOutputClassifier(LogisticRegression(random_state=0, max_iter=1000)).fit(X_training_data, Y_training_data)\n",
        "print(multi_output_logistic_regression.score(X_testing_data, Y_testing_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCx2kJ2nMHej"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRWGiQ2QMHej",
        "outputId": "ac6994a0-bab7-403b-f198-3c6612b70943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010217113665389528\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "multi_output_multinomialNB = MultiOutputClassifier(MultinomialNB()).fit(X_training_data, Y_training_data)\n",
        "print(multi_output_multinomialNB.score(X_testing_data, Y_testing_data))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}