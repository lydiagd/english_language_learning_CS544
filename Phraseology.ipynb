{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obiTALnwMHeQ",
        "outputId": "6801ef4a-8873-4eda-bdc5-4b4b53959817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYP7vVdAMHeT",
        "outputId": "da7658e9-66c8-4ebc-ca0a-81459f05a287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 # in case you don't have it installed\n",
        "\n",
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaU7ypb-MHeU"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Znq__eKMHeV",
        "outputId": "8f39cc15-a8a9-4392-b8c6-012cbc3d2e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To run on Google Colab: \n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv', usecols=['full_text', 'phraseology'], dtype={'full_text': 'object', 'phraseology': 'object'}\n",
        "                 )   \n"
      ],
      "metadata": {
        "id": "zNTFz9pSMs7h"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['phraseology'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KvTvEKON56B",
        "outputId": "52b4a755-be0d-4cac-f313-ab3d35c11be7"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       3.0\n",
            "1       2.0\n",
            "2       3.0\n",
            "3       4.5\n",
            "4       3.0\n",
            "       ... \n",
            "3906    3.5\n",
            "3907    4.0\n",
            "3908    3.0\n",
            "3909    4.0\n",
            "3910    3.0\n",
            "Name: phraseology, Length: 3911, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "_87DuVOFfMo8"
      },
      "outputs": [],
      "source": [
        "# To run locally:\n",
        "# df = pd.read_csv('amazon_reviews_us_Jewelry_v1_00.tsv', sep='\\t', usecols=['star_rating', 'review_body'], dtype={'review_body': 'object', 'star_rating': 'object'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "EkTPZHNrMHeW"
      },
      "outputs": [],
      "source": [
        "# unique_star_rating_values = df['star_rating'].unique()\n",
        "# # print(unique_star_rating_values)\n",
        "# review_body = df['review_body'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "SdeyTf6-MHeW"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant (nan & 2012-12-21) values from the dataset \n",
        "# df_valid_star_ratings_only = df.loc[df['star_rating'].isin(['1', '2','3','4','5'])]\n",
        "\n",
        "# Drop null review bodies from the dataset.\n",
        "# df_valid_star_ratings_only = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['review_body'].notna()]\n",
        "# print(df_valid_star_ratings_only['review_body'].isnull().values.sum())\n",
        "# print(df['review_body'].isnull().values.sum())\n",
        "\n",
        "\n",
        "# unique_star_rating_values = df_valid_star_ratings_only['star_rating'].unique()\n",
        "# print(unique_star_rating_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "mQDQJ6DCMHeX"
      },
      "outputs": [],
      "source": [
        "# Check to see what df looks like with only valid star ratings \n",
        "# print(df_valid_star_ratings_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "nME53JYZMHeY"
      },
      "outputs": [],
      "source": [
        "# Check number of invalid star ratings to make sure number makes sense with df with valid star ratings. \n",
        "# df_invalid_star_ratings = df.loc[~df['star_rating'].isin(['1', '2','3','4','5'])]\n",
        "# unique_invalid_star_rating_values = df_invalid_star_ratings['star_rating'].unique()\n",
        "# print(unique_invalid_star_rating_values)\n",
        "# print(df_invalid_star_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA6ZbQFIMHea"
      },
      "source": [
        "## Keep Reviews and Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "dZ8_tqSoMHea"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pUDuUWMHea"
      },
      "source": [
        " ## We select 20000 reviews randomly from each rating class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "aJC9a6qoMHeb"
      },
      "outputs": [],
      "source": [
        "# Select 20,000 random reviews from each rating class. Create a balanced dataset to perform the required tasks on the\n",
        "# downsized data set.\n",
        "\n",
        "# random_reviews_5_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['5'])].sample(n = 20000)\n",
        "# random_reviews_4_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['4'])].sample(n = 20000)\n",
        "# random_reviews_3_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['3'])].sample(n = 20000)\n",
        "# random_reviews_2_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['2'])].sample(n = 20000)\n",
        "# random_reviews_1_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['1'])].sample(n = 20000)\n",
        "\n",
        "# print(random_reviews_5_stars)\n",
        "# print(random_reviews_4_stars)\n",
        "# print(random_reviews_3_stars)\n",
        "# print(random_reviews_2_stars)\n",
        "# print(random_reviews_1_stars)\n",
        "\n",
        "\n",
        "# only_selected_datasets_list = [\n",
        "#     random_reviews_5_stars,\n",
        "#     random_reviews_4_stars,\n",
        "#     random_reviews_3_stars,\n",
        "#     random_reviews_2_stars,\n",
        "#     random_reviews_1_stars\n",
        "#     ]\n",
        "\n",
        "# only_selected_datasets_combined = pd.concat(only_selected_datasets_list)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "bf1z2EWJMHeb"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset before cleaning\n",
        "# average_length_of_reviews_before_cleaning = only_selected_datasets_combined['review_body'].str.len().mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlvSd6SwMHeb"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "eAXhUzq0MHec"
      },
      "outputs": [],
      "source": [
        "# Convert all reviews to lowercase\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].str.lower()\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].str.lower()\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].str.lower()\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].str.lower()\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].str.lower()\n",
        "\n",
        "df['full_text'] = df['full_text'].str.lower() \n",
        "\n",
        "\n",
        "# Check the output \n",
        "# print(random_reviews_5_stars['review_body'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "hBjYRjaEMHec"
      },
      "outputs": [],
      "source": [
        "# Remove the HTML and URLs from the reviews \n",
        "# def remove_HTML_URLS(sentence):\n",
        "#     sentence = str(sentence)\n",
        "#     soup = BeautifulSoup(sentence)\n",
        "#     clean_sentence = soup.get_text()\n",
        "#     return clean_sentence \n",
        "\n",
        "# # random_reviews_5_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_4_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_3_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_2_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_1_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# only_selected_datasets_combined['review_body'] = only_selected_datasets_combined['review_body'].apply(remove_HTML_URLS)\n",
        "\n",
        "\n",
        "# print(random_reviews_5_stars.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "N1FK9dUvMHec"
      },
      "outputs": [],
      "source": [
        "# Remove non-alphabetical characters\n",
        "\n",
        "import re \n",
        "def remove_non_alphabetical_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    removed_non_alphabetical_words_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_non_alphabetical_words_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_non_alphabetical_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "8F_MtpI-MHed"
      },
      "outputs": [],
      "source": [
        "# Remove extra spaces "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "AQbPy3hUMHed"
      },
      "outputs": [],
      "source": [
        "def remove_extra_spaces(sentence):\n",
        "    sentence = str(sentence)\n",
        "    ' '.join(sentence.split())\n",
        "    removed_extra_spaces_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_extra_spaces_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_extra_spaces)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrTzLI1kMHed",
        "outputId": "628e4530-06de-46bc-b20e-fd0fd20ad36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install the contractions library\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "ymogAjWYMHed"
      },
      "outputs": [],
      "source": [
        "# Perform contractions on the dataframe\n",
        "import contractions\n",
        "\n",
        "def replace_contractions_with_full_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    replaced_contractions_sentence = contractions.fix(sentence)\n",
        "    return replaced_contractions_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(replace_contractions_with_full_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "cfLq37V4MHee"
      },
      "outputs": [],
      "source": [
        "# # Print the average length of the reviews in terms of character length in your dataset after cleaning\n",
        "# average_length_of_reviews_after_cleaning = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8KKheSSMHee"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "x_F6nVLQMHee"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset before preprocessing\n",
        "# average_length_of_reviews_before_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_before_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8sJ4uNwMHee"
      },
      "source": [
        "## remove the stop words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZ81Hb7MHee",
        "outputId": "506488e5-e54b-45ab-ad0a-d092305f7eb8",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Download the stopwords package. \n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Stop words are only in the review_body column, so remove stop words from this column. \n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stop_words(sentence):  \n",
        "    stopwords_to_remove = stopwords.words('english') # Set the stopwords that will be removed.\n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_stop_words = sentence.split()\n",
        "    sentence_list_without_stop_words = []\n",
        "    for word in sentence_list_with_stop_words:\n",
        "        if word not in stopwords_to_remove:\n",
        "            sentence_list_without_stop_words.append(word)\n",
        "    sentence_without_stop_words = ' '.join(sentence_list_without_stop_words)\n",
        "    return sentence_without_stop_words\n",
        "    \n",
        "# Apply this function along the review_body axis. \n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(remove_stop_words)\n",
        "df['full_text'] = df['full_text'].apply(remove_stop_words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "GVMH09I5MHef"
      },
      "outputs": [],
      "source": [
        "# Check that output is correct.\n",
        "# print(only_selected_datasets_combined['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "76dDjtEIMXFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXAtqtl9MHef"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4AVHS8UMHef",
        "outputId": "ac8af3c4-9559-4f09-925a-a481329d25e6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize each word \n",
        "def lemmatize_each_word_in_the_sentence(sentence):  \n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_words_to_lemmatize = sentence.split() \n",
        "    sentence_list_with_lemmatized_words = []\n",
        "    for word in sentence_list_with_words_to_lemmatize:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word) \n",
        "        sentence_list_with_lemmatized_words.append(lemmatized_word)\n",
        "    sentence_with_lemmatized_words = ' '.join(sentence_list_with_lemmatized_words)\n",
        "    return sentence_with_lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "lfMIWTq5MHef"
      },
      "outputs": [],
      "source": [
        "# Perform lemmatization.\n",
        "\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "df['full_text'] = df['full_text'].apply(lemmatize_each_word_in_the_sentence)\n",
        "\n",
        "\n",
        "# Check \n",
        "# print(random_reviews_5_stars['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "YpxI2EqmVYgX"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset after preprocessing\n",
        "# average_length_of_reviews_after_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjw0f7e-MHeg"
      },
      "source": [
        "# TF-IDF Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "jogyKz7EMHeg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0573de02-048d-405c-f687-3bc8dbacdd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "# Install sklearn\n",
        "!pip install sklearn\n",
        "\n",
        "\n",
        "# Import the module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "5bu867RkMHeg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the vectorizer\n",
        "vectorizer = TfidfVectorizer() \n",
        "train_features_star_rating = vectorizer.fit_transform(df['full_text'].apply(lambda x:np.str_(x))) # convert values to a string along the axis \n",
        "# significance of the words in the corpus \n",
        "# train_features_review_body = vectorizer.fit_transform(df['review_body'].values.astype('U')) # consumes too much memory \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6SYo48uMHeg"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "r5fXLNymMHeg"
      },
      "outputs": [],
      "source": [
        "# Train the Perceptron model on the training dataset using the sklearn built-in implementation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "jo87ELnkMHeg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score \n",
        "\n",
        "# X_labels = vectorizer.get_feature.names_out()\n",
        "Y_values = df['phraseology'].values\n",
        "\n",
        "X_training_data, X_testing_data, Y_training_data, Y_testing_data = train_test_split(train_features_star_rating, Y_values, test_size=0.20)\n",
        "\n",
        "\n",
        "perceptron = Perceptron(random_state=40) # choose whatever as long as good result \n",
        "perceptron.fit(X_training_data, Y_training_data)\n",
        "perceptron_score = perceptron.score(X_training_data, Y_training_data)\n",
        "\n",
        "# perceptron.fit(vectorizer.get_feature_names(), Y_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "VOgG3rgLtkhh"
      },
      "outputs": [],
      "source": [
        "# print(perceptron_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adidTozaMHeh",
        "outputId": "2063ec16-51a3-417f-ace1-211226471b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         2\n",
            "         2.0       0.23      0.20      0.21        70\n",
            "         2.5       0.31      0.25      0.27       160\n",
            "         3.0       0.32      0.33      0.32       227\n",
            "         3.5       0.27      0.21      0.24       193\n",
            "         4.0       0.22      0.30      0.25       108\n",
            "         4.5       0.17      0.06      0.08        18\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.26       783\n",
            "   macro avg       0.17      0.15      0.15       783\n",
            "weighted avg       0.28      0.26      0.26       783\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_values = df['full_text'].values\n",
        "predictions_training = perceptron.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test = perceptron.predict(X_testing_data)\n",
        "# print(accuracy_score(predictions_training, Y_training_data))\n",
        "# print(accuracy_score(predictions_test, Y_testing_data))\n",
        "\n",
        "# print(accuracy_score(predictions_train, Y_training_data))\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "VFW2pkZtVN7P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl_IKIy5MHei"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4hi2IVMHei",
        "outputId": "99d6314b-c217-44cf-d06b-263460a9455e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         2\n",
            "         2.0       0.21      0.09      0.12        70\n",
            "         2.5       0.31      0.25      0.27       160\n",
            "         3.0       0.30      0.41      0.34       227\n",
            "         3.5       0.30      0.32      0.31       193\n",
            "         4.0       0.24      0.23      0.23       108\n",
            "         4.5       0.00      0.00      0.00        18\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.29       783\n",
            "   macro avg       0.15      0.14      0.14       783\n",
            "weighted avg       0.27      0.29      0.27       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# svm_model = svm.SVR()\n",
        "# svm_model.fit(X_training_data, Y_training_data)\n",
        "linear_svm_model = svm.LinearSVC()\n",
        "linear_svm_model.fit(X_training_data, Y_training_data)\n",
        "\n",
        "# predictions_training_linear_svm = linear_svm_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_linear_svm = linear_svm_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_linear_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfCS0hFMHei"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyhugkdGMHei",
        "outputId": "bcc336d9-83b2-474c-87c1-e46f20c88d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         2\n",
            "         2.0       0.25      0.01      0.03        70\n",
            "         2.5       0.40      0.26      0.31       160\n",
            "         3.0       0.30      0.60      0.40       227\n",
            "         3.5       0.32      0.33      0.33       193\n",
            "         4.0       0.32      0.11      0.17       108\n",
            "         4.5       0.00      0.00      0.00        18\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.32       783\n",
            "   macro avg       0.18      0.15      0.14       783\n",
            "weighted avg       0.32      0.32      0.29       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression_model = LogisticRegression(random_state=0, max_iter=1000).fit(X_training_data, Y_training_data)\n",
        "predictions_training_logistic_regression = logistic_regression_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_logistic_regression = logistic_regression_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_logistic_regression))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCx2kJ2nMHej"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRWGiQ2QMHej",
        "outputId": "4daf7cf9-860f-4a7f-fb80-bb9c4f6d8786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         2\n",
            "         1.5       0.00      0.00      0.00         2\n",
            "         2.0       0.00      0.00      0.00        70\n",
            "         2.5       0.00      0.00      0.00       160\n",
            "         3.0       0.29      1.00      0.45       227\n",
            "         3.5       0.50      0.02      0.03       193\n",
            "         4.0       0.00      0.00      0.00       108\n",
            "         4.5       0.00      0.00      0.00        18\n",
            "         5.0       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.29       783\n",
            "   macro avg       0.09      0.11      0.05       783\n",
            "weighted avg       0.21      0.29      0.14       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create the Naive Bayes Model. \n",
        "naive_bayes_model = MultinomialNB()\n",
        "\n",
        "# Train the Naive Bayes Model. \n",
        "naive_bayes_model.fit(X_training_data, Y_training_data)\n",
        "\n",
        "predictions_training_naive_bayes= naive_bayes_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_naive_bayes = naive_bayes_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_naive_bayes))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['phraseology'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ViGSbQNf5T",
        "outputId": "c5884e73-ff7d-416f-b4ff-5a4631adb7c4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       3.0\n",
            "1       2.0\n",
            "2       3.0\n",
            "3       4.5\n",
            "4       3.0\n",
            "       ... \n",
            "3906    3.5\n",
            "3907    4.0\n",
            "3908    3.0\n",
            "3909    4.0\n",
            "3910    3.0\n",
            "Name: phraseology, Length: 3911, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "L8dEP8YQMHej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4298cb4-7bd9-4cd1-a9f0-271cf9ee4a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Generate classification reports for each model, using the predictions from the test data and the Y labels from the test data.\n",
        "# Make the classification report a dictionary with the argument output_dict = True. \n",
        "perceptron_report = classification_report(Y_testing_data, predictions_test, output_dict=True)\n",
        "svm_report = classification_report(Y_testing_data, predictions_test_linear_svm, output_dict=True)\n",
        "logistic_regression_report = classification_report(Y_testing_data, predictions_test_logistic_regression, output_dict=True)\n",
        "naive_bayes_report = classification_report(Y_testing_data, predictions_test_naive_bayes, output_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "ZWJ9Xfhgculo"
      },
      "outputs": [],
      "source": [
        "# print(average_length_of_reviews_before_cleaning, average_length_of_reviews_after_cleaning)\n",
        "# print(average_length_of_reviews_before_preprocessing, average_length_of_reviews_after_preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "yWB7qHEFMHej"
      },
      "outputs": [],
      "source": [
        "# print(perceptron_report['1']['precision'], perceptron_report['1']['recall'], perceptron_report['1']['f1-score'], sep=\",\")\n",
        "# print(perceptron_report['2']['precision'], perceptron_report['2']['recall'], perceptron_report['2']['f1-score'], sep=\",\")\n",
        "# print(perceptron_report['3']['precision'], perceptron_report['3']['recall'], perceptron_report['3']['f1-score'], sep=\",\")\n",
        "# print(perceptron_report['4']['precision'], perceptron_report['4']['recall'], perceptron_report['4']['f1-score'], sep=\",\")\n",
        "# print(perceptron_report['5']['precision'], perceptron_report['5']['recall'], perceptron_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# # Calculate the average precision, recall, and f1 scores for the Perceptron model using the arithmetic mean (as stated in Piazza post)\n",
        "# average_precision_perceptron = (perceptron_report['1']['precision'] + perceptron_report['2']['precision'] + perceptron_report['3']['precision'] + perceptron_report['4']['precision'] + perceptron_report['5']['precision']) / 5\n",
        "# average_recall_perceptron = (perceptron_report['1']['recall'] + perceptron_report['2']['recall'] + perceptron_report['3']['recall'] + perceptron_report['4']['recall'] + perceptron_report['5']['recall']) / 5 \n",
        "# average_f1_score_perceptron = (perceptron_report['1']['f1-score'] + perceptron_report['2']['f1-score'] + perceptron_report['3']['f1-score'] + perceptron_report['4']['f1-score'] + perceptron_report['5']['f1-score']) / 5 \n",
        "# print(average_precision_perceptron, average_recall_perceptron, average_f1_score_perceptron, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "TAtliqsCZlZg"
      },
      "outputs": [],
      "source": [
        "# print(svm_report['1']['precision'], svm_report['1']['recall'], svm_report['1']['f1-score'], sep=\",\")\n",
        "# print(svm_report['2']['precision'], svm_report['2']['recall'], svm_report['2']['f1-score'], sep=\",\")\n",
        "# print(svm_report['3']['precision'], svm_report['3']['recall'], svm_report['3']['f1-score'], sep=\",\")\n",
        "# print(svm_report['4']['precision'], svm_report['4']['recall'], svm_report['4']['f1-score'], sep=\",\")\n",
        "# print(svm_report['5']['precision'], svm_report['5']['recall'], svm_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# # Calculate the average precision, recall, and f1 scores for the SVM model using the arithmetic mean (as stated in Piazza post)\n",
        "# average_precision_svm = (svm_report['1']['precision'] + svm_report['2']['precision'] + svm_report['3']['precision'] + svm_report['4']['precision'] + svm_report['5']['precision']) / 5\n",
        "# average_recall_svm = (svm_report['1']['recall'] + svm_report['2']['recall'] + svm_report['3']['recall'] + svm_report['4']['recall'] + svm_report['5']['recall']) / 5 \n",
        "# average_f1_score_svm = (svm_report['1']['f1-score'] + svm_report['2']['f1-score'] + svm_report['3']['f1-score'] + svm_report['4']['f1-score'] + svm_report['5']['f1-score']) / 5 \n",
        "# print(average_precision_svm, average_recall_svm, average_f1_score_svm, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "7XRmpwGoZl9p"
      },
      "outputs": [],
      "source": [
        "# print(logistic_regression_report['1']['precision'], logistic_regression_report['1']['recall'], logistic_regression_report['1']['f1-score'], sep=\",\")\n",
        "# print(logistic_regression_report['2']['precision'], logistic_regression_report['2']['recall'], logistic_regression_report['2']['f1-score'], sep=\",\")\n",
        "# print(logistic_regression_report['3']['precision'], logistic_regression_report['3']['recall'], logistic_regression_report['3']['f1-score'], sep=\",\")\n",
        "# print(logistic_regression_report['4']['precision'], logistic_regression_report['4']['recall'], logistic_regression_report['4']['f1-score'], sep=\",\")\n",
        "# print(logistic_regression_report['5']['precision'], logistic_regression_report['5']['recall'], logistic_regression_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# # Calculate the average precision, recall, and f1 scores for the Logistic Regression model using the arithmetic mean (as stated in Piazza post)\n",
        "# average_precision_logistic_regression = (logistic_regression_report['1']['precision'] + logistic_regression_report['2']['precision'] + logistic_regression_report['3']['precision'] + logistic_regression_report['4']['precision'] + logistic_regression_report['5']['precision']) / 5\n",
        "# average_recall_logistic_regression = (logistic_regression_report['1']['recall'] + logistic_regression_report['2']['recall'] + logistic_regression_report['3']['recall'] + logistic_regression_report['4']['recall'] + logistic_regression_report['5']['recall']) / 5 \n",
        "# average_f1_score_logistic_regression = (logistic_regression_report['1']['f1-score'] + logistic_regression_report['2']['f1-score'] + logistic_regression_report['3']['f1-score'] + logistic_regression_report['4']['f1-score'] + logistic_regression_report['5']['f1-score']) / 5 \n",
        "# print(average_precision_logistic_regression, average_recall_logistic_regression, average_f1_score_logistic_regression, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Jvri7gTsZmXF"
      },
      "outputs": [],
      "source": [
        "# print(naive_bayes_report['1']['precision'], naive_bayes_report['1']['recall'], naive_bayes_report['1']['f1-score'], sep=\",\")\n",
        "# print(naive_bayes_report['2']['precision'], naive_bayes_report['2']['recall'], naive_bayes_report['2']['f1-score'], sep=\",\")\n",
        "# print(naive_bayes_report['3']['precision'], naive_bayes_report['3']['recall'], naive_bayes_report['3']['f1-score'], sep=\",\")\n",
        "# print(naive_bayes_report['4']['precision'], naive_bayes_report['4']['recall'], naive_bayes_report['4']['f1-score'], sep=\",\")\n",
        "# print(naive_bayes_report['5']['precision'], naive_bayes_report['5']['recall'], naive_bayes_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# # Calculate the average precision, recall, and f1 scores for the Naive Bayes model using the arithmetic mean (as stated in Piazza post)\n",
        "# average_precision_naive_bayes = (naive_bayes_report['1']['precision'] + naive_bayes_report['2']['precision'] + naive_bayes_report['3']['precision'] + naive_bayes_report['4']['precision'] + naive_bayes_report['5']['precision']) / 5\n",
        "# average_recall_naive_bayes = (naive_bayes_report['1']['recall'] + naive_bayes_report['2']['recall'] + naive_bayes_report['3']['recall'] + naive_bayes_report['4']['recall'] + naive_bayes_report['5']['recall']) / 5 \n",
        "# average_f1_score_naive_bayes = (naive_bayes_report['1']['f1-score'] + naive_bayes_report['2']['f1-score'] + naive_bayes_report['3']['f1-score'] + naive_bayes_report['4']['f1-score'] + naive_bayes_report['5']['f1-score']) / 5 \n",
        "# print(average_precision_naive_bayes, average_recall_naive_bayes, average_f1_score_naive_bayes, sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3\n",
        "\n",
        "```\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vW_8PLPdAGU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDrmmteO_0Lk",
        "outputId": "d40ecd31-6c97-402b-feae-baed0fdbe306"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.62)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "FFqp85ShNk4U"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'key': 23}\n"
      ],
      "metadata": {
        "id": "dDKJouKhNWb-"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the JSON file for GPT-3 \n",
        "def create_jsonl(dataset, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for row in dataset.itertuples():\n",
        "            file.write(json.dumps({\"text\": row.full_text,\n",
        "                                   'label': row.phraseology})+'\\n')\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "SMX_M4GB_zqB"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YSiawsc1MccN"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['full_text'], df['phraseology']\n",
        "\n",
        "create_jsonl(df, \"training.jsonl\")"
      ],
      "metadata": {
        "id": "h-ilY7RaKA1M"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "BIo56JV_Zmua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17f731c-9983-4a8f-dcf7-b5316fd27249"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<File file id=file-oev4QyTYMPBwr9GXtYoj1H2o at 0x7f21e7d8b290> JSON: {\n",
              "  \"bytes\": 5211623,\n",
              "  \"created_at\": 1669741984,\n",
              "  \"filename\": \"file\",\n",
              "  \"id\": \"file-oev4QyTYMPBwr9GXtYoj1H2o\",\n",
              "  \"object\": \"file\",\n",
              "  \"purpose\": \"classifications\",\n",
              "  \"status\": \"uploaded\",\n",
              "  \"status_details\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "# Set credentials\n",
        "secrete_key = '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'\n",
        "openai.api_key = secrete_key\n",
        "\n",
        "# Upload file\n",
        "openai.File.create(file=open(\"/content/drive/My Drive/Colab Notebooks/training.jsonl\"),\n",
        "                   purpose=\"classifications\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "VNMZ3GixT-rW"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "embedding = openai.Embedding.create(\n",
        "    input=\"Sample document text goes here\",\n",
        "    engine=\"text-similarity-davinci-001\"\n",
        ")[\"data\"][0][\"embedding\"]\n",
        "len(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa8KVZ7DAOFC",
        "outputId": "8dfa4ec5-66f7-4aa4-9a31-ffaea0790e8d"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12288"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def get_embedding(text: str, engine=\"text-similarity-davinci-001\"):\n",
        "\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
        "\n",
        "\n",
        "embedding = get_embedding(\"Sample query text goes here\", engine=\"text-search-ada-query-001\")\n",
        "print(len(embedding))"
      ],
      "metadata": {
        "id": "sWA474GhVZVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ae82d1-6985-4754-ed42-7008f7767c1c"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_1 = df.loc[df['phraseology'].isin(['1.0'])]\n",
        "scores_1_5 = df.loc[df['phraseology'].isin(['1.5'])]\n",
        "scores_2_5 = df.loc[df['phraseology'].isin(['2.5'])].sample(n=50)\n",
        "scores_3 = df.loc[df['phraseology'].isin(['3.0'])].sample(n=50)\n",
        "scores_3_5 = df.loc[df['phraseology'].isin(['3.5'])].sample(n=50)\n",
        "scores_4 = df.loc[df['phraseology'].isin(['4.0'])].sample(n=50)\n",
        "scores_5 = df.loc[df['phraseology'].isin(['5.0'])]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R1YDcK_Meoer"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_4_5 = df.loc[df['phraseology'].isin(['4.5'])].sample(n=30)"
      ],
      "metadata": {
        "id": "ZDPEH3mos4M4"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_2 = df.loc[df['phraseology'].isin(['2.0'])].sample(n=50)\n"
      ],
      "metadata": {
        "id": "Gwq5KUrkoaHw"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_1['babbage_similarity'] = scores_1.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uuVBdGOeq3X",
        "outputId": "ef4eb564-6baf-4f14-e9f9-b5a43c797cac"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_1_5['babbage_similarity'] = scores_1_5.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6aFxZRZem1R",
        "outputId": "4e3a16e5-0106-4f44-835f-0ef86a00ba13"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_2['babbage_similarity'] = scores_2.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "sUWQIMv4f94M"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_2_5['babbage_similarity'] = scores_2_5.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "udzuA6rGf-av"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_3['babbage_similarity'] = scores_3.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "4Ka8RnOhf_Ll"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_3_5['babbage_similarity'] = scores_3_5.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "E1aANk0IgD8A"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_4['babbage_similarity'] = scores_4.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "7Ku-wB2_gKOZ"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_4_5['babbage_similarity'] = scores_4_5.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "id": "2HHIMKGdgLjD"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_5['babbage_similarity'] = scores_5.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cByM42OdgLSL",
        "outputId": "3ed9d81f-4488-4c89-e6bc-75864ce4ed21"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df['babbage_similarity'] = df.full_text.apply(lambda x: get_embedding(x, engine='babbage-similarity'))\n",
        "# df.to_csv('output/embedded_newsgroups.csv')\n",
        "\n",
        "### Break up df by score to work with rate limit "
      ],
      "metadata": {
        "id": "Et8xP3FwYxab"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "only_selected_datasets_list = [\n",
        "    scores_1,\n",
        "    scores_1_5,\n",
        "    scores_2,\n",
        "    scores_2_5,\n",
        "    scores_3,\n",
        "    scores_3_5,\n",
        "    scores_4,\n",
        "    scores_4_5,\n",
        "    scores_5,\n",
        "    ]\n",
        "\n",
        "only_selected_datasets_combined = pd.concat(only_selected_datasets_list)"
      ],
      "metadata": {
        "id": "tvjtFlFOemV2"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(only_selected_datasets_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8Z9yA4lWNv",
        "outputId": "8c11da0c-f445-45fe-a91a-2a8ac16d14f3"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              full_text phraseology  \\\n",
            "378   allow student bring phone school wen student j...         1.0   \n",
            "514   student required take music drama art class im...         1.0   \n",
            "552   speak thomas jefferson need simtheing idle ask...         1.0   \n",
            "934   thi really select work wat iam interest wight ...         1.0   \n",
            "952   student offer distance learning option student...         1.0   \n",
            "...                                                 ...         ...   \n",
            "2812  student able graduate year early high school e...         5.0   \n",
            "2959  human survival depends technology live without...         5.0   \n",
            "2991  many people going say self even though want ce...         5.0   \n",
            "3075  agree statement positive attitude help cope ea...         5.0   \n",
            "3445  dear school boardwhy student education matter ...         5.0   \n",
            "\n",
            "                                     babbage_similarity  \n",
            "378   [-0.015627741813659668, 0.02473312057554722, 0...  \n",
            "514   [-0.026872124522924423, 0.01367106568068266, -...  \n",
            "552   [-0.019312791526317596, 0.020919041708111763, ...  \n",
            "934   [-0.004420934710651636, 0.015365226194262505, ...  \n",
            "952   [-0.00976379495114088, 0.03725634515285492, -0...  \n",
            "...                                                 ...  \n",
            "2812  [-0.03411556035280228, 0.03469042852520943, 0....  \n",
            "2959  [-0.00930184405297041, 0.02242688462138176, 0....  \n",
            "2991  [-0.0169890895485878, 0.02813408337533474, 0.0...  \n",
            "3075  [0.00468447245657444, 0.024763217195868492, -0...  \n",
            "3445  [-0.015377058647572994, 0.03684775531291962, 0...  \n",
            "\n",
            "[326 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# datafile_path = \"https://cdn.openai.com/API/examples/data/fine_food_reviews_with_embeddings_1k.csv\"  # for your convenience, we precomputed the embeddings\n",
        "# df = pd.read_csv(datafile_path)\n",
        "# df[\"babbage_similarity\"] = df.babbage_similarity.apply(eval).apply(np.array)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    list(only_selected_datasets_combined.babbage_similarity.values), only_selected_datasets_combined.phraseology, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_test)\n",
        "probas = clf.predict_proba(X_test)\n",
        "\n",
        "report = classification_report(y_test, preds)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mpm5X6AY84T",
        "outputId": "028e7ee5-e480-4d1c-d47d-8ff282227969"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         4\n",
            "         1.5       0.00      0.00      0.00         1\n",
            "         2.0       0.21      0.44      0.29         9\n",
            "         2.5       0.20      0.07      0.11        14\n",
            "         3.0       0.16      0.43      0.23         7\n",
            "         3.5       0.11      0.09      0.10        11\n",
            "         4.0       0.17      0.25      0.20         8\n",
            "         4.5       0.00      0.00      0.00         6\n",
            "         5.0       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.17        66\n",
            "   macro avg       0.09      0.14      0.10        66\n",
            "weighted avg       0.13      0.17      0.13        66\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}