{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obiTALnwMHeQ",
        "outputId": "84a091ff-3eea-4822-ba89-a49ae887e8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYP7vVdAMHeT",
        "outputId": "a16c29dc-18bc-4689-9fd1-fa6b19b87698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 # in case you don't have it installed\n",
        "\n",
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaU7ypb-MHeU"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Znq__eKMHeV",
        "outputId": "209b8f1c-e32b-4d18-c338-1eb885fdb56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To run on Google Colab: \n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv', usecols=['full_text', 'phraseology'], dtype={'full_text': 'object', 'phraseology': 'object'}\n",
        "                 )   \n"
      ],
      "metadata": {
        "id": "zNTFz9pSMs7h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['phraseology'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KvTvEKON56B",
        "outputId": "bbb00b6a-c178-428b-9c99-fe99c806b6f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       3.0\n",
            "1       2.0\n",
            "2       3.0\n",
            "3       4.5\n",
            "4       3.0\n",
            "       ... \n",
            "3906    3.5\n",
            "3907    4.0\n",
            "3908    3.0\n",
            "3909    4.0\n",
            "3910    3.0\n",
            "Name: phraseology, Length: 3911, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_87DuVOFfMo8"
      },
      "outputs": [],
      "source": [
        "# To run locally:\n",
        "# df = pd.read_csv('amazon_reviews_us_Jewelry_v1_00.tsv', sep='\\t', usecols=['star_rating', 'review_body'], dtype={'review_body': 'object', 'star_rating': 'object'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EkTPZHNrMHeW"
      },
      "outputs": [],
      "source": [
        "# unique_star_rating_values = df['star_rating'].unique()\n",
        "# # print(unique_star_rating_values)\n",
        "# review_body = df['review_body'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SdeyTf6-MHeW"
      },
      "outputs": [],
      "source": [
        "# Drop irrelevant (nan & 2012-12-21) values from the dataset \n",
        "# df_valid_star_ratings_only = df.loc[df['star_rating'].isin(['1', '2','3','4','5'])]\n",
        "\n",
        "# Drop null review bodies from the dataset.\n",
        "# df_valid_star_ratings_only = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['review_body'].notna()]\n",
        "# print(df_valid_star_ratings_only['review_body'].isnull().values.sum())\n",
        "# print(df['review_body'].isnull().values.sum())\n",
        "\n",
        "\n",
        "# unique_star_rating_values = df_valid_star_ratings_only['star_rating'].unique()\n",
        "# print(unique_star_rating_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mQDQJ6DCMHeX"
      },
      "outputs": [],
      "source": [
        "# Check to see what df looks like with only valid star ratings \n",
        "# print(df_valid_star_ratings_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nME53JYZMHeY"
      },
      "outputs": [],
      "source": [
        "# Check number of invalid star ratings to make sure number makes sense with df with valid star ratings. \n",
        "# df_invalid_star_ratings = df.loc[~df['star_rating'].isin(['1', '2','3','4','5'])]\n",
        "# unique_invalid_star_rating_values = df_invalid_star_ratings['star_rating'].unique()\n",
        "# print(unique_invalid_star_rating_values)\n",
        "# print(df_invalid_star_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA6ZbQFIMHea"
      },
      "source": [
        "## Keep Reviews and Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dZ8_tqSoMHea"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6pUDuUWMHea"
      },
      "source": [
        " ## We select 20000 reviews randomly from each rating class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aJC9a6qoMHeb"
      },
      "outputs": [],
      "source": [
        "# Select 20,000 random reviews from each rating class. Create a balanced dataset to perform the required tasks on the\n",
        "# downsized data set.\n",
        "\n",
        "# random_reviews_5_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['5'])].sample(n = 20000)\n",
        "# random_reviews_4_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['4'])].sample(n = 20000)\n",
        "# random_reviews_3_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['3'])].sample(n = 20000)\n",
        "# random_reviews_2_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['2'])].sample(n = 20000)\n",
        "# random_reviews_1_stars = df_valid_star_ratings_only.loc[df_valid_star_ratings_only['star_rating'].isin(['1'])].sample(n = 20000)\n",
        "\n",
        "# print(random_reviews_5_stars)\n",
        "# print(random_reviews_4_stars)\n",
        "# print(random_reviews_3_stars)\n",
        "# print(random_reviews_2_stars)\n",
        "# print(random_reviews_1_stars)\n",
        "\n",
        "\n",
        "# only_selected_datasets_list = [\n",
        "#     random_reviews_5_stars,\n",
        "#     random_reviews_4_stars,\n",
        "#     random_reviews_3_stars,\n",
        "#     random_reviews_2_stars,\n",
        "#     random_reviews_1_stars\n",
        "#     ]\n",
        "\n",
        "# only_selected_datasets_combined = pd.concat(only_selected_datasets_list)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bf1z2EWJMHeb"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset before cleaning\n",
        "# average_length_of_reviews_before_cleaning = only_selected_datasets_combined['review_body'].str.len().mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlvSd6SwMHeb"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eAXhUzq0MHec"
      },
      "outputs": [],
      "source": [
        "# Convert all reviews to lowercase\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].str.lower()\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].str.lower()\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].str.lower()\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].str.lower()\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].str.lower()\n",
        "\n",
        "df['full_text'] = df['full_text'].str.lower() \n",
        "\n",
        "\n",
        "# Check the output \n",
        "# print(random_reviews_5_stars['review_body'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hBjYRjaEMHec"
      },
      "outputs": [],
      "source": [
        "# Remove the HTML and URLs from the reviews \n",
        "# def remove_HTML_URLS(sentence):\n",
        "#     sentence = str(sentence)\n",
        "#     soup = BeautifulSoup(sentence)\n",
        "#     clean_sentence = soup.get_text()\n",
        "#     return clean_sentence \n",
        "\n",
        "# # random_reviews_5_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_4_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_3_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_2_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# # random_reviews_1_stars['review_body'].apply(remove_HTML_URLS)\n",
        "# only_selected_datasets_combined['review_body'] = only_selected_datasets_combined['review_body'].apply(remove_HTML_URLS)\n",
        "\n",
        "\n",
        "# print(random_reviews_5_stars.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "N1FK9dUvMHec"
      },
      "outputs": [],
      "source": [
        "# Remove non-alphabetical characters\n",
        "\n",
        "import re \n",
        "def remove_non_alphabetical_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    removed_non_alphabetical_words_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_non_alphabetical_words_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_non_alphabetical_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8F_MtpI-MHed"
      },
      "outputs": [],
      "source": [
        "# Remove extra spaces "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AQbPy3hUMHed"
      },
      "outputs": [],
      "source": [
        "def remove_extra_spaces(sentence):\n",
        "    sentence = str(sentence)\n",
        "    ' '.join(sentence.split())\n",
        "    removed_extra_spaces_sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
        "    return removed_extra_spaces_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(remove_extra_spaces)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrTzLI1kMHed",
        "outputId": "cd94f49c-452a-41a3-c889-7f804b20a8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install the contractions library\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ymogAjWYMHed"
      },
      "outputs": [],
      "source": [
        "# Perform contractions on the dataframe\n",
        "import contractions\n",
        "\n",
        "def replace_contractions_with_full_words(sentence):\n",
        "    sentence = str(sentence)\n",
        "    replaced_contractions_sentence = contractions.fix(sentence)\n",
        "    return replaced_contractions_sentence \n",
        "\n",
        "df['full_text'] = df['full_text'].apply(replace_contractions_with_full_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cfLq37V4MHee"
      },
      "outputs": [],
      "source": [
        "# # Print the average length of the reviews in terms of character length in your dataset after cleaning\n",
        "# average_length_of_reviews_after_cleaning = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8KKheSSMHee"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "x_F6nVLQMHee",
        "outputId": "a6359ed6-bfa0-468d-f4c3-ee4a37e91c91"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4fed6a6cc1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the average length of the reviews in terms of character length in your dataset before preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maverage_length_of_reviews_before_preprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monly_selected_datasets_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_length_of_reviews_before_preprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'only_selected_datasets_combined' is not defined"
          ]
        }
      ],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset before preprocessing\n",
        "# average_length_of_reviews_before_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_before_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8sJ4uNwMHee"
      },
      "source": [
        "## remove the stop words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZ81Hb7MHee",
        "outputId": "60c66cd3-bfa6-4e36-d45a-5b42623a4310",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Download the stopwords package. \n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Stop words are only in the review_body column, so remove stop words from this column. \n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stop_words(sentence):  \n",
        "    stopwords_to_remove = stopwords.words('english') # Set the stopwords that will be removed.\n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_stop_words = sentence.split()\n",
        "    sentence_list_without_stop_words = []\n",
        "    for word in sentence_list_with_stop_words:\n",
        "        if word not in stopwords_to_remove:\n",
        "            sentence_list_without_stop_words.append(word)\n",
        "    sentence_without_stop_words = ' '.join(sentence_list_without_stop_words)\n",
        "    return sentence_without_stop_words\n",
        "    \n",
        "# Apply this function along the review_body axis. \n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(remove_stop_words)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(remove_stop_words)\n",
        "df['full_text'] = df['full_text'].apply(remove_stop_words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVMH09I5MHef"
      },
      "outputs": [],
      "source": [
        "# Check that output is correct.\n",
        "# print(only_selected_datasets_combined['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76dDjtEIMXFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXAtqtl9MHef"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4AVHS8UMHef",
        "outputId": "68d13c38-b201-4d5c-e8e8-99e0c893906c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize each word \n",
        "def lemmatize_each_word_in_the_sentence(sentence):  \n",
        "    sentence = str(sentence)\n",
        "    sentence_list_with_words_to_lemmatize = sentence.split() \n",
        "    sentence_list_with_lemmatized_words = []\n",
        "    for word in sentence_list_with_words_to_lemmatize:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word) \n",
        "        sentence_list_with_lemmatized_words.append(lemmatized_word)\n",
        "    sentence_with_lemmatized_words = ' '.join(sentence_list_with_lemmatized_words)\n",
        "    return sentence_with_lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lfMIWTq5MHef"
      },
      "outputs": [],
      "source": [
        "# Perform lemmatization.\n",
        "\n",
        "# random_reviews_5_stars['review_body'] = random_reviews_5_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_4_stars['review_body'] = random_reviews_4_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_3_stars['review_body'] = random_reviews_3_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_2_stars['review_body'] = random_reviews_2_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "# random_reviews_1_stars['review_body'] = random_reviews_1_stars['review_body'].apply(lemmatize_each_word_in_the_sentence)\n",
        "df['full_text'] = df['full_text'].apply(lemmatize_each_word_in_the_sentence)\n",
        "\n",
        "\n",
        "# Check \n",
        "# print(random_reviews_5_stars['review_body'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpxI2EqmVYgX"
      },
      "outputs": [],
      "source": [
        "# Print the average length of the reviews in terms of character length in your dataset after preprocessing\n",
        "# average_length_of_reviews_after_preprocessing = only_selected_datasets_combined['review_body'].str.len().mean()\n",
        "# print(average_length_of_reviews_after_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjw0f7e-MHeg"
      },
      "source": [
        "# TF-IDF Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jogyKz7EMHeg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e0c951-f601-4504-c194-90a49693ddaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=41580bea63151898b5c4334fc692ac44db12ff4f65756ca4c26a2897aa65ab82\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "# Install sklearn\n",
        "!pip install sklearn\n",
        "\n",
        "\n",
        "# Import the module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5bu867RkMHeg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the vectorizer\n",
        "vectorizer = TfidfVectorizer() \n",
        "train_features_star_rating = vectorizer.fit_transform(df['full_text'].apply(lambda x:np.str_(x))) # convert values to a string along the axis \n",
        "# significance of the words in the corpus \n",
        "# train_features_review_body = vectorizer.fit_transform(df['review_body'].values.astype('U')) # consumes too much memory \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6SYo48uMHeg"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5fXLNymMHeg"
      },
      "outputs": [],
      "source": [
        "# Train the Perceptron model on the training dataset using the sklearn built-in implementation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jo87ELnkMHeg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score \n",
        "\n",
        "# X_labels = vectorizer.get_feature.names_out()\n",
        "Y_values = df['phraseology'].values\n",
        "\n",
        "X_training_data, X_testing_data, Y_training_data, Y_testing_data = train_test_split(train_features_star_rating, Y_values, test_size=0.20)\n",
        "\n",
        "\n",
        "perceptron = Perceptron(random_state=40) # choose whatever as long as good result \n",
        "perceptron.fit(X_training_data, Y_training_data)\n",
        "perceptron_score = perceptron.score(X_training_data, Y_training_data)\n",
        "\n",
        "# perceptron.fit(vectorizer.get_feature_names(), Y_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOgG3rgLtkhh"
      },
      "outputs": [],
      "source": [
        "# print(perceptron_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adidTozaMHeh",
        "outputId": "427b2c58-c17a-4924-8db5-d84765161903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         4\n",
            "         1.5       0.00      0.00      0.00         1\n",
            "         2.0       0.29      0.24      0.26        75\n",
            "         2.5       0.24      0.28      0.26       151\n",
            "         3.0       0.35      0.25      0.29       254\n",
            "         3.5       0.28      0.24      0.26       176\n",
            "         4.0       0.25      0.30      0.27        99\n",
            "         4.5       0.11      0.05      0.07        19\n",
            "         5.0       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.25       783\n",
            "   macro avg       0.17      0.15      0.16       783\n",
            "weighted avg       0.28      0.25      0.27       783\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_values = df['full_text'].values\n",
        "predictions_training = perceptron.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test = perceptron.predict(X_testing_data)\n",
        "# print(accuracy_score(predictions_training, Y_training_data))\n",
        "# print(accuracy_score(predictions_test, Y_testing_data))\n",
        "\n",
        "# print(accuracy_score(predictions_train, Y_training_data))\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFW2pkZtVN7P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl_IKIy5MHei"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4hi2IVMHei",
        "outputId": "b39952b6-845c-41b5-b6f0-7bd507ec1dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         4\n",
            "         1.5       0.00      0.00      0.00         1\n",
            "         2.0       0.30      0.15      0.20        75\n",
            "         2.5       0.24      0.28      0.26       151\n",
            "         3.0       0.35      0.39      0.37       254\n",
            "         3.5       0.28      0.32      0.30       176\n",
            "         4.0       0.26      0.21      0.23        99\n",
            "         4.5       0.00      0.00      0.00        19\n",
            "         5.0       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.30       783\n",
            "   macro avg       0.16      0.15      0.15       783\n",
            "weighted avg       0.28      0.30      0.29       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# svm_model = svm.SVR()\n",
        "# svm_model.fit(X_training_data, Y_training_data)\n",
        "linear_svm_model = svm.LinearSVC()\n",
        "linear_svm_model.fit(X_training_data, Y_training_data)\n",
        "\n",
        "# predictions_training_linear_svm = linear_svm_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_linear_svm = linear_svm_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_linear_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfCS0hFMHei"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyhugkdGMHei",
        "outputId": "a0276833-9a53-4399-eb8b-165894164860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         4\n",
            "         1.5       0.00      0.00      0.00         1\n",
            "         2.0       0.09      0.01      0.02        75\n",
            "         2.5       0.27      0.23      0.25       151\n",
            "         3.0       0.35      0.58      0.44       254\n",
            "         3.5       0.30      0.33      0.31       176\n",
            "         4.0       0.38      0.10      0.16        99\n",
            "         4.5       0.00      0.00      0.00        19\n",
            "         5.0       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.32       783\n",
            "   macro avg       0.15      0.14      0.13       783\n",
            "weighted avg       0.29      0.32      0.28       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression_model = LogisticRegression(random_state=0, max_iter=1000).fit(X_training_data, Y_training_data)\n",
        "predictions_training_logistic_regression = logistic_regression_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_logistic_regression = logistic_regression_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_logistic_regression))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCx2kJ2nMHej"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRWGiQ2QMHej",
        "outputId": "41c9a19e-c47e-40fb-8ebb-c7623b9914a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         4\n",
            "         1.5       0.00      0.00      0.00         1\n",
            "         2.0       0.00      0.00      0.00        75\n",
            "         2.5       0.00      0.00      0.00       151\n",
            "         3.0       0.33      0.98      0.49       254\n",
            "         3.5       0.38      0.05      0.09       176\n",
            "         4.0       0.00      0.00      0.00        99\n",
            "         4.5       0.00      0.00      0.00        19\n",
            "         5.0       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.33       783\n",
            "   macro avg       0.08      0.12      0.06       783\n",
            "weighted avg       0.19      0.33      0.18       783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create the Naive Bayes Model. \n",
        "naive_bayes_model = MultinomialNB()\n",
        "\n",
        "# Train the Naive Bayes Model. \n",
        "naive_bayes_model.fit(X_training_data, Y_training_data)\n",
        "\n",
        "predictions_training_naive_bayes= naive_bayes_model.predict(X_training_data) # must use sparse matrix here with the predict\n",
        "predictions_test_naive_bayes = naive_bayes_model.predict(X_testing_data)\n",
        "\n",
        "# Print the classification report \n",
        "print(classification_report(Y_testing_data, predictions_test_naive_bayes))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['phraseology'])"
      ],
      "metadata": {
        "id": "42ViGSbQNf5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8dEP8YQMHej"
      },
      "outputs": [],
      "source": [
        "# Generate classification reports for each model, using the predictions from the test data and the Y labels from the test data.\n",
        "# Make the classification report a dictionary with the argument output_dict = True. \n",
        "perceptron_report = classification_report(Y_testing_data, predictions_test, output_dict=True)\n",
        "svm_report = classification_report(Y_testing_data, predictions_test_linear_svm, output_dict=True)\n",
        "logistic_regression_report = classification_report(Y_testing_data, predictions_test_logistic_regression, output_dict=True)\n",
        "naive_bayes_report = classification_report(Y_testing_data, predictions_test_naive_bayes, output_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJ9Xfhgculo"
      },
      "outputs": [],
      "source": [
        "print(average_length_of_reviews_before_cleaning, average_length_of_reviews_after_cleaning)\n",
        "print(average_length_of_reviews_before_preprocessing, average_length_of_reviews_after_preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWB7qHEFMHej"
      },
      "outputs": [],
      "source": [
        "print(perceptron_report['1']['precision'], perceptron_report['1']['recall'], perceptron_report['1']['f1-score'], sep=\",\")\n",
        "print(perceptron_report['2']['precision'], perceptron_report['2']['recall'], perceptron_report['2']['f1-score'], sep=\",\")\n",
        "print(perceptron_report['3']['precision'], perceptron_report['3']['recall'], perceptron_report['3']['f1-score'], sep=\",\")\n",
        "print(perceptron_report['4']['precision'], perceptron_report['4']['recall'], perceptron_report['4']['f1-score'], sep=\",\")\n",
        "print(perceptron_report['5']['precision'], perceptron_report['5']['recall'], perceptron_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# Calculate the average precision, recall, and f1 scores for the Perceptron model using the arithmetic mean (as stated in Piazza post)\n",
        "average_precision_perceptron = (perceptron_report['1']['precision'] + perceptron_report['2']['precision'] + perceptron_report['3']['precision'] + perceptron_report['4']['precision'] + perceptron_report['5']['precision']) / 5\n",
        "average_recall_perceptron = (perceptron_report['1']['recall'] + perceptron_report['2']['recall'] + perceptron_report['3']['recall'] + perceptron_report['4']['recall'] + perceptron_report['5']['recall']) / 5 \n",
        "average_f1_score_perceptron = (perceptron_report['1']['f1-score'] + perceptron_report['2']['f1-score'] + perceptron_report['3']['f1-score'] + perceptron_report['4']['f1-score'] + perceptron_report['5']['f1-score']) / 5 \n",
        "print(average_precision_perceptron, average_recall_perceptron, average_f1_score_perceptron, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAtliqsCZlZg"
      },
      "outputs": [],
      "source": [
        "print(svm_report['1']['precision'], svm_report['1']['recall'], svm_report['1']['f1-score'], sep=\",\")\n",
        "print(svm_report['2']['precision'], svm_report['2']['recall'], svm_report['2']['f1-score'], sep=\",\")\n",
        "print(svm_report['3']['precision'], svm_report['3']['recall'], svm_report['3']['f1-score'], sep=\",\")\n",
        "print(svm_report['4']['precision'], svm_report['4']['recall'], svm_report['4']['f1-score'], sep=\",\")\n",
        "print(svm_report['5']['precision'], svm_report['5']['recall'], svm_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# Calculate the average precision, recall, and f1 scores for the SVM model using the arithmetic mean (as stated in Piazza post)\n",
        "average_precision_svm = (svm_report['1']['precision'] + svm_report['2']['precision'] + svm_report['3']['precision'] + svm_report['4']['precision'] + svm_report['5']['precision']) / 5\n",
        "average_recall_svm = (svm_report['1']['recall'] + svm_report['2']['recall'] + svm_report['3']['recall'] + svm_report['4']['recall'] + svm_report['5']['recall']) / 5 \n",
        "average_f1_score_svm = (svm_report['1']['f1-score'] + svm_report['2']['f1-score'] + svm_report['3']['f1-score'] + svm_report['4']['f1-score'] + svm_report['5']['f1-score']) / 5 \n",
        "print(average_precision_svm, average_recall_svm, average_f1_score_svm, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XRmpwGoZl9p"
      },
      "outputs": [],
      "source": [
        "print(logistic_regression_report['1']['precision'], logistic_regression_report['1']['recall'], logistic_regression_report['1']['f1-score'], sep=\",\")\n",
        "print(logistic_regression_report['2']['precision'], logistic_regression_report['2']['recall'], logistic_regression_report['2']['f1-score'], sep=\",\")\n",
        "print(logistic_regression_report['3']['precision'], logistic_regression_report['3']['recall'], logistic_regression_report['3']['f1-score'], sep=\",\")\n",
        "print(logistic_regression_report['4']['precision'], logistic_regression_report['4']['recall'], logistic_regression_report['4']['f1-score'], sep=\",\")\n",
        "print(logistic_regression_report['5']['precision'], logistic_regression_report['5']['recall'], logistic_regression_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# Calculate the average precision, recall, and f1 scores for the Logistic Regression model using the arithmetic mean (as stated in Piazza post)\n",
        "average_precision_logistic_regression = (logistic_regression_report['1']['precision'] + logistic_regression_report['2']['precision'] + logistic_regression_report['3']['precision'] + logistic_regression_report['4']['precision'] + logistic_regression_report['5']['precision']) / 5\n",
        "average_recall_logistic_regression = (logistic_regression_report['1']['recall'] + logistic_regression_report['2']['recall'] + logistic_regression_report['3']['recall'] + logistic_regression_report['4']['recall'] + logistic_regression_report['5']['recall']) / 5 \n",
        "average_f1_score_logistic_regression = (logistic_regression_report['1']['f1-score'] + logistic_regression_report['2']['f1-score'] + logistic_regression_report['3']['f1-score'] + logistic_regression_report['4']['f1-score'] + logistic_regression_report['5']['f1-score']) / 5 \n",
        "print(average_precision_logistic_regression, average_recall_logistic_regression, average_f1_score_logistic_regression, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvri7gTsZmXF"
      },
      "outputs": [],
      "source": [
        "print(naive_bayes_report['1']['precision'], naive_bayes_report['1']['recall'], naive_bayes_report['1']['f1-score'], sep=\",\")\n",
        "print(naive_bayes_report['2']['precision'], naive_bayes_report['2']['recall'], naive_bayes_report['2']['f1-score'], sep=\",\")\n",
        "print(naive_bayes_report['3']['precision'], naive_bayes_report['3']['recall'], naive_bayes_report['3']['f1-score'], sep=\",\")\n",
        "print(naive_bayes_report['4']['precision'], naive_bayes_report['4']['recall'], naive_bayes_report['4']['f1-score'], sep=\",\")\n",
        "print(naive_bayes_report['5']['precision'], naive_bayes_report['5']['recall'], naive_bayes_report['5']['f1-score'], sep=\",\")\n",
        "\n",
        "# Calculate the average precision, recall, and f1 scores for the Naive Bayes model using the arithmetic mean (as stated in Piazza post)\n",
        "average_precision_naive_bayes = (naive_bayes_report['1']['precision'] + naive_bayes_report['2']['precision'] + naive_bayes_report['3']['precision'] + naive_bayes_report['4']['precision'] + naive_bayes_report['5']['precision']) / 5\n",
        "average_recall_naive_bayes = (naive_bayes_report['1']['recall'] + naive_bayes_report['2']['recall'] + naive_bayes_report['3']['recall'] + naive_bayes_report['4']['recall'] + naive_bayes_report['5']['recall']) / 5 \n",
        "average_f1_score_naive_bayes = (naive_bayes_report['1']['f1-score'] + naive_bayes_report['2']['f1-score'] + naive_bayes_report['3']['f1-score'] + naive_bayes_report['4']['f1-score'] + naive_bayes_report['5']['f1-score']) / 5 \n",
        "print(average_precision_naive_bayes, average_recall_naive_bayes, average_f1_score_naive_bayes, sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "vW_8PLPdAGU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logi_clf = LogisticRegression(random_state=0).fit(X_training_data, Y_training_data)\n",
        "logi_pred_y = logi_clf.predict(X_testing_data)"
      ],
      "metadata": {
        "id": "zDrmmteO_0Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(Y_testing_data, logi_pred_y)"
      ],
      "metadata": {
        "id": "SMX_M4GB_zqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIo56JV_Zmua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Y_testing_data, logi_pred_y))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Aa8KVZ7DAOFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}